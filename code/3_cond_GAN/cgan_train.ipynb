{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cosmogan\n",
    "Nov 19, 2020\n",
    "\n",
    "Borrowing pieces of code from : \n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/11569e0db3599ac214b03e01956c2971b02c64ce/beginner_source/dcgan_faces_tutorial.py\n",
    "- https://github.com/exalearn/epiCorvid/tree/master/cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_load_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    return config\n",
    "\n",
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4.) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Generator Code\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_model(model_name,gdict):\n",
    "    ''' Module to define Generator and Discriminator'''\n",
    "    print(\"Model name\",model_name)\n",
    "    if model_name==1: ## With embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.label_embedding=nn.Embedding(num_classes,num_classes)\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+num_classes,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                labels=labels.unsqueeze(-1).long()\n",
    "                gen_input=torch.cat((self.label_embedding(labels),noise),-1)\n",
    "                img=self.main(gen_input)\n",
    "        #         print(type(img),img.size())\n",
    "        #         img=img.view(128,nc,128,128))\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "                self.label_embedding=nn.Embedding(num_classes,num_classes)\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                labels=labels.unsqueeze(-1).long()\n",
    "                img_size=gdict['image_size']\n",
    "                a=self.label_embedding(labels)\n",
    "                x=a.view(a.size(0),-1)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4))\n",
    "                x=x.view(a.size(0),1,img_size,img_size)\n",
    "        #         print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "        #         d_input=torch.cat((img,self.label_embedding(labels)),-1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==2: #### Model 2: without embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+1,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).float()\n",
    "                gen_input=torch.cat((noise,x),-1)\n",
    "                img=self.main(gen_input)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                img_size=gdict['image_size']\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,4).float() # get to size (128,1,1,4)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4)) # get to size (128,1, 128, 128)\n",
    "                x=x.view(labels.size(0),1,img_size,img_size)\n",
    "#                 print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==3:#### Model 3: with ConditionalInstanceNorm2d\n",
    "        class ConditionalInstanceNorm2d(nn.Module):\n",
    "            def __init__(self, num_features, num_params):\n",
    "                super().__init__()\n",
    "                self.num_features = num_features\n",
    "                self.InstNorm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "                self.affine = nn.Linear(num_params, num_features * 2)\n",
    "                self.affine.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "                self.affine.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                out = self.InstNorm(x)\n",
    "                gamma, beta = self.affine(y).chunk(2, 1)\n",
    "                out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "                return out\n",
    "\n",
    "        class ConditionalSequential(nn.Sequential):\n",
    "            def __init__(self,*args):\n",
    "                super(ConditionalSequential, self).__init__(*args)\n",
    "\n",
    "            def forward(self, inputs, labels):\n",
    "                for module in self:\n",
    "                    if module.__class__ is ConditionalInstanceNorm2d:\n",
    "                        inputs = module(inputs, labels.float())\n",
    "                    else:\n",
    "                        inputs = module(inputs)\n",
    "\n",
    "                return inputs\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*4,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*2,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                img=self.main(noise,labels)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "        #             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*2,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*4,1),\n",
    "\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*8,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):   \n",
    "                pred=self.main(img,labels)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==4: #### Model 4: without embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+1,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).float()\n",
    "                gen_input=torch.cat((noise,x),-1)\n",
    "                img=self.main(gen_input)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                img_size=gdict['image_size']\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,4).float() # get to size (128,1,1,4)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4)) # get to size (128,1, 128, 128)\n",
    "                x=x.view(labels.size(0),1,img_size,img_size)\n",
    "#                 print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==5:#### Model 5: with ConditionalInstanceNorm2d\n",
    "        class ConditionalInstanceNorm2d(nn.Module):\n",
    "            def __init__(self, num_features, num_params):\n",
    "                super().__init__()\n",
    "                self.num_features = num_features\n",
    "                self.InstNorm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "                self.affine = nn.Linear(num_params, num_features * 2)\n",
    "                self.affine.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "                self.affine.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                out = self.InstNorm(x)\n",
    "                gamma, beta = self.affine(y).chunk(2, 1)\n",
    "                out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "                return out\n",
    "\n",
    "        class ConditionalSequential(nn.Sequential):\n",
    "            def __init__(self,*args):\n",
    "                super(ConditionalSequential, self).__init__(*args)\n",
    "\n",
    "            def forward(self, inputs, labels):\n",
    "                for module in self:\n",
    "                    if module.__class__ is ConditionalInstanceNorm2d:\n",
    "                        inputs = module(inputs, labels.float())\n",
    "                    else:\n",
    "                        inputs = module(inputs)\n",
    "\n",
    "                return inputs\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*4,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*2,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                img=self.main(noise,labels)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "        #             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*2,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*4,1),\n",
    "\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*8,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):   \n",
    "                pred=self.main(img,labels)\n",
    "                return pred\n",
    "\n",
    "    return Generator, Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_sigma(ip_categories,gdict):\n",
    "    sigma_list=gdict['sigma_list']\n",
    "    return torch.tensor([sigma_list[i] for i in ip_categories.long()],device=gdict['device']).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def f_gen_images(gdict,netG,optimizerG,label,ip_fname,op_loc,op_strg='inf_img_',op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: gdict, netG, optimizerG, \n",
    "                 label : class label\n",
    "                 ip_fname: name of input file\n",
    "                op_strg: [string name for output file]\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    nz,device=gdict['nz'],gdict['device']\n",
    "\n",
    "    try:\n",
    "        if torch.cuda.is_available(): checkpoint=torch.load(ip_fname)\n",
    "        else: checkpoint=torch.load(ip_fname,map_location=torch.device('cpu'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "    \n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, nz, device=device)\n",
    "    tnsr_categories=(torch.ones(op_size,device=device)*label).view(op_size,1)\n",
    "    if gdict['model']>3: tnsr_categories=f_get_sigma(tnsr_categories,gdict)\n",
    "\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    gen = netG(noise,tnsr_categories)\n",
    "    gen_images=gen.detach().cpu().numpy()[:,0,:,:]\n",
    "    print(gen_images.shape)\n",
    "\n",
    "    op_fname='%s_epoch-%s_step-%s_label-%s.npy'%(op_strg,epoch,iters,label)\n",
    "    np.save(op_loc+op_fname,gen_images)\n",
    "    \n",
    "    print(\"Image saved in \",op_fname)\n",
    "    \n",
    "def f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc):\n",
    "    ''' Checkpoint model '''\n",
    "    \n",
    "    if gdict['multi-gpu']: ## Dataparallel\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.module.state_dict(),'D_state':netD.module.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc) \n",
    "    else :\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.state_dict(),'D_state':netD.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc)\n",
    "\n",
    "\n",
    "def f_load_checkpoint(ip_fname,netG,netD,optimizerG,optimizerD,gdict):\n",
    "    ''' Load saved checkpoint\n",
    "    Also loads step, epoch, best_chi1, best_chi2'''\n",
    "    \n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        raise SystemError\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "        netD.module.load_state_dict(checkpoint['D_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "        netD.load_state_dict(checkpoint['D_state'])\n",
    "    \n",
    "    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    best_chi1=checkpoint['best_chi1']\n",
    "    best_chi2=checkpoint['best_chi2']\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    return iters,epoch,best_chi1,best_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_torch_radial_profile(img, center=(None,None)):\n",
    "    ''' Module to compute radial profile of a 2D image \n",
    "    Bincount causes issues with backprop, so not using this code\n",
    "    '''\n",
    "    \n",
    "    y,x=torch.meshgrid(torch.arange(0,img.shape[0]),torch.arange(0,img.shape[1])) # Get a grid of x and y values\n",
    "    if center[0]==None and center[1]==None:\n",
    "        center = torch.Tensor([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0]) # compute centers\n",
    "\n",
    "    # get radial values of every pair of points\n",
    "    r = torch.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    r= r.int()\n",
    "    \n",
    "#     print(r.shape,img.shape)\n",
    "    # Compute histogram of r values\n",
    "    tbin=torch.bincount(torch.reshape(r,(-1,)),weights=torch.reshape(img,(-1,)).type(torch.DoubleTensor))\n",
    "    nr = torch.bincount(torch.reshape(r,(-1,)))\n",
    "    radialprofile = tbin / nr\n",
    "    \n",
    "    return radialprofile[1:-1]\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage_with_batch(image, center=None): ### Not used in this code.\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile. Only use if you need to combine batches\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, channel, height, width = image.shape\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (batch, channel,-1)))\n",
    "    r_sorted = torch.gather(torch.reshape(r, (batch, channel, -1,)),2, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, (batch, channel, -1,)),2, ind)\n",
    "\n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[:,:,1:] - r_int[:,:,:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[2], (batch, -1))    # location of changes in radius\n",
    "    rind=torch.unsqueeze(rind,1)\n",
    "    nr = (rind[:,:,1:] - rind[:,:,:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "\n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "#     print(csum.shape,rind.shape,nr.shape)\n",
    "\n",
    "    tbin = torch.gather(csum, 2, rind[:,:,1:]) - torch.gather(csum, 2, rind[:,:,:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "\n",
    "def f_get_rad(img):\n",
    "    ''' Get the radial tensor for use in f_torch_get_azimuthalAverage '''\n",
    "    \n",
    "    height,width=img.shape[-2:]\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "    \n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "    \n",
    "    return r.detach(),ind.detach()\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image,r,ind):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "#     height, width = image.shape\n",
    "#     # Create a grid of points with x and y coordinates\n",
    "#     y, x = np.indices([height,width])\n",
    "\n",
    "#     if not center:\n",
    "#         center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "#     # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "#     r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "#     # Get sorted radii\n",
    "#     ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    y1=torch.rfft(arr,signal_ndim=2,onesided=False)\n",
    "    real,imag=f_torch_fftshift(y1[:,:,0],y1[:,:,1])    ## last index is real/imag part\n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    \n",
    "#     print(y2.shape)\n",
    "    z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    \n",
    "    return z1\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr,r,ind):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i,r,ind) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels,r,ind):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    \n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    sdev=[[] for i in range(num_channels)]    \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:]\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr,r,ind)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "#         sdev[i]=torch.std(batch_pk,axis=0)/np.sqrt(batch_pk.shape[0])\n",
    "#         sdev[i]=torch.std(batch_pk,axis=0)\n",
    "        sdev[i]=torch.var(batch_pk,axis=0)\n",
    "    \n",
    "    mean=torch.stack(mean)\n",
    "    sdev=torch.stack(sdev)\n",
    "        \n",
    "    return mean,sdev\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    \n",
    "    try: \n",
    "        hist_data=torch.histc(data,bins=bins)\n",
    "        ## A kind of normalization of histograms: divide by total sum\n",
    "        hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hist_data=torch.zeros(bins)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_std,spec_std_ref,image_size,lambda1):\n",
    "    ''' Loss function for the spectrum : mean + variance \n",
    "    Log(sum( batch value - expect value) ^ 2 )) '''\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "        \n",
    "    spec_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    spec_sdev=torch.log(torch.mean(torch.pow(spec_std[:,:idx]-spec_std_ref[:,:idx],2)))\n",
    "    \n",
    "    lambda1=lambda1;\n",
    "    lambda2=lambda1;\n",
    "    ans=lambda1*spec_mean+lambda2*spec_sdev\n",
    "    \n",
    "    if torch.isnan(spec_sdev).any():    print(\"spec loss with nan\",ans)\n",
    "    \n",
    "    return ans\n",
    "    \n",
    "\n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return lambda1*torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def f_get_hist_cond(img_tensor,categories,bins,gdict,hist_val_tnsr):\n",
    "#     ''' Module to compute pixel intensity histogram loss for conditional GAN '''\n",
    "#     num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "#     loss_hist_tensor=torch.zeros(num_classes,device=device)\n",
    "#     for i in np.arange(num_classes):    \n",
    "#         idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "#     #     print(i,idxs.size(0))\n",
    "#         if idxs.size(0)>1: \n",
    "#             img=img_tensor[idxs]\n",
    "#             loss_hist_tensor[i]=loss_hist(f_compute_hist(img,bins),hist_val_tnsr[i])\n",
    "#     hist_loss=loss_hist_tensor.sum()\n",
    "    \n",
    "#     return hist_loss\n",
    "\n",
    "# def f_get_spec_cond(img_tensor,categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind):\n",
    "#     ''' Module to compute spectral loss for conditional GAN '''\n",
    "#     num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "#     loss_spec_tensor=torch.zeros(num_classes,device=device)\n",
    "#     for i in np.arange(num_classes):    \n",
    "#         idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "#     #     print(i,idxs.size(0))\n",
    "#         if idxs.size(0)>1: \n",
    "#             img=img_tensor[idxs]\n",
    "#             mean,sdev=f_torch_image_spectrum(f_invtransform(img),1,r,ind)\n",
    "#             loss_spec_tensor[i]=loss_spectrum(mean,spec_mean_tnsr[i],sdev,spec_sdev_tnsr[i],gdict['image_size'],gdict['lambda1'])\n",
    "#     spec_loss=loss_spec_tensor.sum()\n",
    "#     return spec_loss\n",
    "\n",
    "\n",
    "\n",
    "def f_get_hist_cond(img_tensor,categories,bins,gdict,hist_val_tnsr):\n",
    "    ''' Module to compute pixel intensity histogram loss for conditional GAN '''\n",
    "    num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "    loss_hist_tensor=torch.zeros(num_classes,device=device)\n",
    "    for count,i in enumerate(gdict['sigma_list']):    \n",
    "        idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "        if idxs.size(0)>1: \n",
    "            num_frac=idxs.size(0)/img_tensor.shape[0] ## Fraction of points in the category\n",
    "            img=img_tensor[idxs]\n",
    "            loss_hist_tensor[count]=loss_hist(f_compute_hist(img,bins),hist_val_tnsr[count])*num_frac\n",
    "    hist_loss=loss_hist_tensor.sum()\n",
    "    \n",
    "    return hist_loss\n",
    "\n",
    "def f_get_spec_cond(img_tensor,categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind):\n",
    "    ''' Module to compute spectral loss for conditional GAN '''\n",
    "    num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "    loss_spec_tensor=torch.zeros(num_classes,device=device)\n",
    "    for count,i in enumerate(gdict['sigma_list']):    \n",
    "        idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "        if idxs.size(0)>1: \n",
    "            num_frac=idxs.size(0)/img_tensor.shape[0] ## Fraction of points in the category\n",
    "            img=img_tensor[idxs]\n",
    "            mean,sdev=f_torch_image_spectrum(f_invtransform(img),1,r,ind)\n",
    "            loss_spec_tensor[count]=loss_spectrum(mean,spec_mean_tnsr[count],sdev,spec_sdev_tnsr[count],gdict['image_size'],gdict['lambda1'])*num_frac\n",
    "    spec_loss=loss_spec_tensor.sum()\n",
    "    return spec_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_size(ip):\n",
    "#     p=2;s=2\n",
    "# #     return (ip + 2 * 0 - 1 * (p-1) -1 )/ s + 1\n",
    "\n",
    "#     return (ip-1)*s - 2 * p + 1 *(5-1)+ 1 + 1\n",
    "\n",
    "# f_size(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(filename=save_dir+'/log.log',filemode='w',format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_train_loop(dataloader,metrics_df,gdict):\n",
    "    ''' Train single epoch '''\n",
    "    \n",
    "    ## Define new variables from dict\n",
    "    keys=['start_epoch','epochs','iters','best_chi1','best_chi2','save_dir','device','flip_prob','nz','num_classes','batch_size','bns']\n",
    "    start_epoch,epochs,iters,best_chi1,best_chi2,save_dir,device,flip_prob,nz,num_classes,batch_size,bns=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        t_epoch_start=time.time()\n",
    "        for count, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            ####### Train GAN ########\n",
    "            netG.train(); netD.train();  ### Need to add these after inference and before training\n",
    "\n",
    "            tme1=time.time()\n",
    "            ### Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            netD.zero_grad()\n",
    "\n",
    "            real_cpu = data[0].to(device)\n",
    "            real_categories=data[1].to(device)\n",
    "#             real_categories=real_categories.squeeze(-1)\n",
    "            \n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), 1, device=device)\n",
    "            fake_label = torch.full((b_size,), 0, device=device)\n",
    "            g_label = torch.full((b_size,), 1, device=device) # No flipping for Generator labels\n",
    "            # Flip labels with probability flip_prob for Discriminator\n",
    "            for idx in np.random.choice(np.arange(b_size),size=int(np.ceil(b_size*flip_prob))):\n",
    "                real_label[idx]=0; fake_label[idx]=1\n",
    "            \n",
    "            # Generate fake image batch with G\n",
    "            noise = torch.randn(b_size, 1, 1, nz, device=device)\n",
    "            fake_categories=torch.randint(gdict['num_classes'],(gdict['batch_size'],1),device=gdict['device'])\n",
    "            if gdict['model']>3: fake_categories=f_get_sigma(fake_categories,gdict)\n",
    "            fake = netG(noise,fake_categories)  \n",
    "            \n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_cpu,real_categories).view(-1)\n",
    "            errD_real = criterion(output, real_label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "            \n",
    "            # Forward pass fake batch through D\n",
    "            output = netD(fake.detach(),fake_categories).view(-1)\n",
    "            errD_fake = criterion(output, fake_label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "            \n",
    "            ###Update G network: maximize log(D(G(z)))\n",
    "            netG.zero_grad()\n",
    "            output = netD(fake,fake_categories).view(-1)\n",
    "            errG_adv = criterion(output, g_label)\n",
    "            # Histogram pixel intensity loss\n",
    "            \n",
    "#             hist_gen=f_compute_hist(fake,bins=bns)\n",
    "#             hist_loss=loss_hist(hist_gen,hist_val.to(device))\n",
    "            hist_loss=f_get_hist_cond(fake,fake_categories,bns,gdict,hist_val_tnsr)\n",
    "            \n",
    "            # Add spectral loss\n",
    "#             mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1,r.to(device),ind.to(device))\n",
    "#             spec_loss=loss_spectrum(mean,mean_spec_val.to(device),sdev,sdev_spec_val.to(device),image_size)\n",
    "            spec_loss=f_get_spec_cond(fake,fake_categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind)\n",
    "\n",
    "            if gdict['spec_loss_flag']: errG=errG_adv+spec_loss\n",
    "            else: errG=errG_adv\n",
    "#             errG=errG_adv\n",
    "            if torch.isnan(errG).any():\n",
    "                print(errG)\n",
    "                raise SystemError\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "            \n",
    "            tme2=time.time()\n",
    "            \n",
    "            ####### Store metrics ########\n",
    "            # Output training stats\n",
    "            if count % gdict['checkpoint_size'] == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_adv: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, epochs, count, len(dataloader), errD.item(), errG_adv.item(),errG.item(), D_x, D_G_z1, D_G_z2)),\n",
    "                print(\"Spec loss: %s,\\t hist loss: %s\"%(spec_loss.item(),hist_loss.item())),\n",
    "                print(\"Training time for step %s : %s\"%(iters, tme2-tme1))\n",
    "\n",
    "            # Save metrics\n",
    "            cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','D(x)','D_G_z1','D_G_z2','time']\n",
    "            vals=[iters,epoch,errD_real.item(),errD_fake.item(),errD.item(),errG_adv.item(),errG.item(),spec_loss.item(),hist_loss.item(),D_x,D_G_z1,D_G_z2,tme2-tme1]\n",
    "            for col,val in zip(cols,vals):  metrics_df.loc[iters,col]=val\n",
    "\n",
    "            ### Checkpoint the best model\n",
    "            checkpoint=True\n",
    "            iters += 1  ### Model has been updated, so update iters before saving metrics and model.\n",
    "\n",
    "            ### Compute validation metrics for updated model\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise,fixed_categories)\n",
    "#                 hist_gen=f_compute_hist(fake,bins=bns)\n",
    "#                 hist_chi=loss_hist(hist_gen,hist_val.to(device))\n",
    "#                 mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1,r.to(device),ind.to(device))\n",
    "#                 spec_chi=loss_spectrum(mean,mean_spec_val.to(device),sdev,sdev_spec_val.to(device),image_size)      \n",
    "                hist_chi=f_get_hist_cond(fake,fixed_categories,bns,gdict,hist_val_tnsr)\n",
    "                spec_chi=f_get_spec_cond(fake,fixed_categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind)\n",
    "\n",
    "            # Storing chi for next step\n",
    "            for col,val in zip(['spec_chi','hist_chi'],[spec_chi.item(),hist_chi.item()]):  metrics_df.loc[iters,col]=val            \n",
    "\n",
    "            # Checkpoint model for continuing run\n",
    "            if count == len(dataloader)-1: ## Check point at last step of epoch\n",
    "                f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_last.tar')  \n",
    "\n",
    "            if (checkpoint and (epoch > 1)): # Choose best models by metric\n",
    "                if hist_chi< best_chi1:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_hist.tar')\n",
    "                    best_chi1=hist_chi.item()\n",
    "                    print(\"Saving best hist model at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                if  spec_chi< best_chi2:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_spec.tar')\n",
    "                    best_chi2=spec_chi.item()\n",
    "                    print(\"Saving best spec model at epoch %s, step %s\"%(epoch,iters))\n",
    "                    \n",
    "                if iters in gdict['save_steps_list']:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_{0}.tar'.format(iters))\n",
    "                    print(\"Saving given-step at epoch %s, step %s.\"%(epoch,iters))\n",
    "                    \n",
    "            # Save G's output on fixed_noise\n",
    "            if ((iters % gdict['checkpoint_size'] == 0) or ((epoch == epochs-1) and (count == len(dataloader)-1))):\n",
    "                netG.eval()\n",
    "                with torch.no_grad():\n",
    "                    for category in range(num_classes):\n",
    "#                         cat_tensor=torch.LongTensor(np.ones(batch_size)*category).view(batch_size,1,1)\n",
    "                        tnsr_categories=(torch.ones(batch_size,device=device)*category).view(batch_size,1)\n",
    "                        if gdict['model']>3: tnsr_categories=f_get_sigma(tnsr_categories,gdict)\n",
    "                        fake = netG(fixed_noise,tnsr_categories).detach().cpu()\n",
    "                        img_arr=np.array(fake[:,0,:,:])\n",
    "                        fname='gen_img_label-%s_epoch-%s_step-%s'%(category,epoch,iters)\n",
    "                        np.save(save_dir+'/images/'+fname,img_arr)\n",
    "        \n",
    "        t_epoch_end=time.time()\n",
    "        print(\"Time taken for epoch %s: %s\"%(epoch,t_epoch_end-t_epoch_start))\n",
    "        # Save Metrics to file after each epoch\n",
    "        metrics_df.to_pickle(save_dir+'/df_metrics.pkle')\n",
    "        \n",
    "    print(\"best chis: {0}, {1}\".format(best_chi1,best_chi2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_init_gdict(gdict,config_dict):\n",
    "    ''' Initialize the global dictionary gdict with values in config file'''\n",
    "    keys1=['workers','nc','nz','ngf','ndf','beta1','kernel_size','stride','g_padding','d_padding','flip_prob']\n",
    "    keys2=['image_size','checkpoint_size','num_imgs','ip_fname','op_loc']\n",
    "    for key in keys1: gdict[key]=config_dict['training'][key]\n",
    "    for key in keys2: gdict[key]=config_dict['data'][key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workers': 2, 'nc': 1, 'nz': 64, 'ngf': 64, 'ndf': 64, 'beta1': 0.5, 'kernel_size': 5, 'stride': 2, 'g_padding': 2, 'd_padding': 2, 'flip_prob': 0.01, 'image_size': 128, 'checkpoint_size': 10, 'num_imgs': 2000, 'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_5_4univ_cgan', 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/', 'sigma_list': [0.5, 0.65, 0.8, 1.1], 'ngpu': 1, 'batch_size': 128, 'mode': 'fresh', 'spec_loss_flag': False, 'epochs': 5, 'learn_rate': 0.0002, 'lambda1': 0.1, 'save_steps_list': [5, 10], 'model': 4, 'save_dir': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201214_184914_nb_test', 'bns': 50, 'num_classes': 4, 'device': device(type='cuda'), 'multi-gpu': False}\n",
      "Seed:234373\n",
      "Device:cuda\n",
      "torch.Size([8000, 1]), torch.Size([8000, 1, 128, 128])\n",
      "Model name 4\n",
      "Building GAN networks\n",
      "Number of GPUs used 1\n",
      "{'workers': 2, 'nc': 1, 'nz': 64, 'ngf': 64, 'ndf': 64, 'beta1': 0.5, 'kernel_size': 5, 'stride': 2, 'g_padding': 2, 'd_padding': 2, 'flip_prob': 0.01, 'image_size': 128, 'checkpoint_size': 10, 'num_imgs': 2000, 'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_5_4univ_cgan', 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/', 'sigma_list': [0.5, 0.65, 0.8, 1.1], 'ngpu': 1, 'batch_size': 128, 'mode': 'fresh', 'spec_loss_flag': False, 'epochs': 5, 'learn_rate': 0.0002, 'lambda1': 0.1, 'save_steps_list': [5, 10], 'model': 4, 'save_dir': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201214_184914_nb_test', 'bns': 50, 'num_classes': 4, 'device': device(type='cuda'), 'multi-gpu': False, 'best_chi1': 10000000000.0, 'best_chi2': 10000000000.0, 'iters': 0, 'start_epoch': 0}\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "#     torch.backends.cudnn.deterministic=True\n",
    "    \n",
    "    t0=time.time()\n",
    "    #################################\n",
    "#     args=f_parse_args()\n",
    "    # Manually add args ( different for jupyter notebook)\n",
    "    args=argparse.Namespace()\n",
    "    args.config='1_main_code/config_128.yaml'\n",
    "    args.ngpu=1\n",
    "    args.batch_size=128\n",
    "    args.spec_loss_flag=False\n",
    "    args.checkpoint_size=100\n",
    "    args.epochs=5\n",
    "    args.learn_rate=0.0002\n",
    "    args.mode='fresh'\n",
    "#     args.mode='continue'\n",
    "#     args.ip_fldr='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201211_093818_nb_test/'    args.run_suffix='_nb_test'\n",
    "    args.run_suffix='nb_test'\n",
    "    args.deterministic=False\n",
    "    args.seed='234373'\n",
    "    args.lambda1=0.1\n",
    "    args.model=4\n",
    "    args.save_steps_list=[5,10]\n",
    "    \n",
    "    ### Set up ###\n",
    "    config_file=args.config\n",
    "    config_dict=f_load_config(config_file)\n",
    "\n",
    "    # Initilize variables    \n",
    "    gdict={}\n",
    "    f_init_gdict(gdict,config_dict)\n",
    "    \n",
    "    ## Add args variables to gdict\n",
    "    gdict['sigma_list']=[0.5,0.65,0.8,1.1]\n",
    "    for key in ['ngpu','batch_size','mode','spec_loss_flag','epochs','learn_rate','lambda1','save_steps_list','model']:\n",
    "        gdict[key]=vars(args)[key]\n",
    "       \n",
    "    ###### Set up directories #######\n",
    "    if gdict['mode']=='fresh':\n",
    "        # Create prefix for foldername        \n",
    "        fldr_name=datetime.now().strftime('%Y%m%d_%H%M%S') ## time format\n",
    "        gdict['save_dir']=gdict['op_loc']+fldr_name+'_'+args.run_suffix\n",
    "        \n",
    "        if not os.path.exists(gdict['save_dir']):\n",
    "            os.makedirs(gdict['save_dir']+'/models')\n",
    "            os.makedirs(gdict['save_dir']+'/images')\n",
    "        \n",
    "    elif args.mode=='continue': ## For checkpointed runs\n",
    "        gdict['save_dir']=args.ip_fldr\n",
    "        ### Read loss data\n",
    "        with open (gdict['save_dir']+'df_metrics.pkle','rb') as f:\n",
    "            metrics_dict=pickle.load(f) \n",
    "\n",
    "#     ### Write all print statements to stdout and log file (different for jpt notebooks)\n",
    "#     logfile=gdict['save_dir']+'/log.log'\n",
    "#     logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "    \n",
    "#     Lg = logging.getLogger()\n",
    "#     Lg.setLevel(logging.DEBUG)\n",
    "#     lg_handler_file = logging.FileHandler(logfile)\n",
    "#     lg_handler_stdout = logging.StreamHandler(sys.stdout)\n",
    "#     Lg.addHandler(lg_handler_file)\n",
    "#     Lg.addHandler(lg_handler_stdout)\n",
    "    \n",
    "#     print('Args: {0}'.format(args))\n",
    "#     print(config_dict)\n",
    "#     print('Start: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "#     if gdict['spec_loss_flag']: print(\"Using Spectral loss\")\n",
    "\n",
    "    ### Override (different for jpt notebooks)\n",
    "    gdict['num_imgs']=2000\n",
    "#     gdict['learn_rate']=0.0008\n",
    "\n",
    "    ## Special declarations\n",
    "    gdict['bns']=50\n",
    "    gdict['num_classes']=4\n",
    "    gdict['device']=torch.device(\"cuda\" if (torch.cuda.is_available() and gdict['ngpu'] > 0) else \"cpu\")\n",
    "    gdict['ngpu']=torch.cuda.device_count()\n",
    "    \n",
    "    gdict['multi-gpu']=True if (gdict['device'].type == 'cuda') and (gdict['ngpu'] > 1) else False \n",
    "    print(gdict)\n",
    "    \n",
    "    ### Initialize random seed\n",
    "    if args.seed=='random': manualSeed = np.random.randint(1, 10000)\n",
    "    else: manualSeed=int(args.seed)\n",
    "    print(\"Seed:{0}\".format(manualSeed))\n",
    "    random.seed(manualSeed)\n",
    "    np.random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    print('Device:{0}'.format(gdict['device']))\n",
    "\n",
    "    if args.deterministic: \n",
    "        print(\"Running with deterministic sequence. Performance will be slower\")\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "#         torch.backends.cudnn.enabled = False\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    #################################   \n",
    "    ### Read input data from different files\n",
    "    for count,sigma in enumerate(gdict['sigma_list']):\n",
    "        fname=gdict['ip_fname']+'/norm_1_sig_%s_train_val.npy'%(sigma)\n",
    "        x=np.load(fname,mmap_mode='r')[:gdict['num_imgs']].transpose(0,1,2,3)\n",
    "        size=x.shape[0]\n",
    "        y=count*np.ones(size)\n",
    "\n",
    "        if count==0:\n",
    "            img=x[:]\n",
    "            lab=y[:]\n",
    "        else: \n",
    "            img=np.vstack([img,x]) # Store images\n",
    "            lab=np.hstack([lab,y]) # Store class labels\n",
    "\n",
    "    t_img=torch.from_numpy(img)\n",
    "    labels=torch.LongTensor(lab).view(size*4,1)\n",
    "    print(\"%s, %s\"%(labels.shape,t_img.shape))\n",
    "    \n",
    "    dataset=TensorDataset(t_img,labels)\n",
    "    dataloader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)\n",
    "\n",
    "    # Precompute metrics with validation data for computing losses\n",
    "    with torch.no_grad():\n",
    "        spec_mean_list=[];spec_sdev_list=[];hist_val_list=[]\n",
    "        \n",
    "        for count,sigma in enumerate(gdict['sigma_list']):\n",
    "            ip_fname=gdict['ip_fname']+'/norm_1_sig_%s_train_val.npy'%(sigma)\n",
    "            val_img=np.load(ip_fname,mmap_mode='r')[-3000:].transpose(0,1,2,3)\n",
    "            t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "\n",
    "            # Precompute radial coordinates\n",
    "            if count==0: \n",
    "                r,ind=f_get_rad(img)\n",
    "                r=r.to(gdict['device']); ind=ind.to(gdict['device'])\n",
    "            # Stored mean and std of spectrum for full input data once\n",
    "            mean_spec_val,sdev_spec_val=f_torch_image_spectrum(f_invtransform(t_val_img),1,r,ind)\n",
    "            hist_val=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "            \n",
    "            spec_mean_list.append(mean_spec_val)\n",
    "            spec_sdev_list.append(sdev_spec_val)\n",
    "            hist_val_list.append(hist_val)\n",
    "        spec_mean_tnsr=torch.stack(spec_mean_list)\n",
    "        spec_sdev_tnsr=torch.stack(spec_sdev_list)\n",
    "        hist_val_tnsr=torch.stack(hist_val_list)\n",
    "        \n",
    "        del val_img; del t_val_img; del img; del t_img; del spec_mean_list; del spec_sdev_list; del hist_val_list\n",
    "    \n",
    "    #################################\n",
    "    ###### Build Networks ###\n",
    "    # Define Models\n",
    "    Generator, Discriminator=f_get_model(gdict['model'],gdict)\n",
    "    print(\"Building GAN networks\")\n",
    "    # Create Generator\n",
    "    netG = Generator(gdict).to(gdict['device'])\n",
    "    netG.apply(weights_init)\n",
    "#     print(netG)\n",
    "#     summary(netG,(1,1,64))\n",
    "    # Create Discriminator\n",
    "    netD = Discriminator(gdict).to(gdict['device'])\n",
    "    netD.apply(weights_init)\n",
    "#     print(netD)\n",
    "#     summary(netD,(1,128,128))\n",
    "    \n",
    "    print(\"Number of GPUs used %s\"%(gdict['ngpu']))\n",
    "    if (gdict['multi-gpu']):\n",
    "        netG = nn.DataParallel(netG, list(range(gdict['ngpu'])))\n",
    "        netD = nn.DataParallel(netD, list(range(gdict['ngpu'])))\n",
    "    \n",
    "    #### Initialize networks ####\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if gdict['mode']=='fresh':\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=gdict['learn_rate'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        optimizerG = optim.Adam(netG.parameters(), lr=gdict['learn_rate'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        ### Initialize variables      \n",
    "        iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10    \n",
    "    \n",
    "    ### Load network weights for continuing run\n",
    "    elif gdict['mode']=='continue':\n",
    "        iters,start_epoch,best_chi1,best_chi2=f_load_checkpoint(gdict['save_dir']+'/models/checkpoint_last.tar',netG,netD,optimizerG,optimizerD,gdict) \n",
    "        print(\"Continuing existing run. Loading checkpoint with epoch {0} and step {1}\".format(start_epoch,iters))\n",
    "        start_epoch+=1  ## Start with the next epoch  \n",
    "\n",
    "    ## Add to gdict\n",
    "    for key,val in zip(['best_chi1','best_chi2','iters','start_epoch'],[best_chi1,best_chi2,iters,start_epoch]): gdict[key]=val\n",
    "    print(gdict)\n",
    "    \n",
    "    fixed_noise = torch.randn(gdict['batch_size'], 1, 1, gdict['nz'], device=gdict['device']) #Latent vectors to view G progress\n",
    "    fixed_categories=torch.randint(gdict['num_classes'],(gdict['batch_size'],1),device=gdict['device'])\n",
    "    if gdict['model']>3: fixed_categories=f_get_sigma(fixed_categories,gdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/62]\tLoss_D: 1.3360\tLoss_adv: 2.1226\tLoss_G: 2.1226\tD(x): -0.1150\tD(G(z)): -0.2790 / -1.9769\n",
      "Spec loss: 8.47372817993164,\t hist loss: 1.7015306949615479\n",
      "Training time for step 0 : 0.3444478511810303\n",
      "[0/5][10/62]\tLoss_D: 0.3116\tLoss_adv: 3.3980\tLoss_G: 3.3980\tD(x): 1.6894\tD(G(z)): -3.4901 / -3.3620\n",
      "Spec loss: 8.6365327835083,\t hist loss: 1.6095385551452637\n",
      "Training time for step 10 : 0.3219606876373291\n",
      "[0/5][20/62]\tLoss_D: 0.1948\tLoss_adv: 0.8379\tLoss_G: 0.8379\tD(x): 5.5186\tD(G(z)): -2.5910 / -0.1358\n",
      "Spec loss: 8.612759590148926,\t hist loss: 1.4996614456176758\n",
      "Training time for step 20 : 0.3218653202056885\n",
      "[0/5][30/62]\tLoss_D: 0.2266\tLoss_adv: 3.8646\tLoss_G: 3.8646\tD(x): 3.9871\tD(G(z)): -2.9224 / -3.8374\n",
      "Spec loss: 8.577536582946777,\t hist loss: 1.256093978881836\n",
      "Training time for step 30 : 0.3233358860015869\n",
      "[0/5][40/62]\tLoss_D: 0.1805\tLoss_adv: 6.4732\tLoss_G: 6.4732\tD(x): 5.9008\tD(G(z)): -5.1730 / -6.4715\n",
      "Spec loss: 8.540903091430664,\t hist loss: 1.0806989669799805\n",
      "Training time for step 40 : 0.32247352600097656\n",
      "[0/5][50/62]\tLoss_D: 0.2163\tLoss_adv: 6.2220\tLoss_G: 6.2220\tD(x): 3.3022\tD(G(z)): -6.7423 / -6.2194\n",
      "Spec loss: 8.635350227355957,\t hist loss: 1.0070499181747437\n",
      "Training time for step 50 : 0.3229525089263916\n",
      "[0/5][60/62]\tLoss_D: 0.1549\tLoss_adv: 5.1488\tLoss_G: 5.1488\tD(x): 4.5303\tD(G(z)): -3.3486 / -5.1422\n",
      "Spec loss: 8.56649398803711,\t hist loss: 1.0052053928375244\n",
      "Training time for step 60 : 0.3221454620361328\n",
      "Time taken for epoch 0: 26.887635469436646\n",
      "[1/5][0/62]\tLoss_D: 0.2430\tLoss_adv: 3.0240\tLoss_G: 3.0240\tD(x): 2.2151\tD(G(z)): -4.3378 / -2.9717\n",
      "Spec loss: 8.509841918945312,\t hist loss: 0.9618697762489319\n",
      "Training time for step 62 : 0.32537198066711426\n",
      "[1/5][10/62]\tLoss_D: 0.6341\tLoss_adv: 7.7328\tLoss_G: 7.7328\tD(x): 9.0769\tD(G(z)): -0.7545 / -7.7322\n",
      "Spec loss: 8.592649459838867,\t hist loss: 1.3535265922546387\n",
      "Training time for step 72 : 0.32313084602355957\n",
      "[1/5][20/62]\tLoss_D: 0.1766\tLoss_adv: 5.0043\tLoss_G: 5.0043\tD(x): 3.4238\tD(G(z)): -4.7422 / -4.9966\n",
      "Spec loss: 8.659116744995117,\t hist loss: 1.1036819219589233\n",
      "Training time for step 82 : 0.31928014755249023\n",
      "[1/5][30/62]\tLoss_D: 0.1606\tLoss_adv: 4.4777\tLoss_G: 4.4777\tD(x): 4.1954\tD(G(z)): -3.7088 / -4.4650\n",
      "Spec loss: 10.433812141418457,\t hist loss: 0.7888448238372803\n",
      "Training time for step 92 : 0.32271575927734375\n",
      "[1/5][40/62]\tLoss_D: 0.1634\tLoss_adv: 4.2921\tLoss_G: 4.2921\tD(x): 3.8829\tD(G(z)): -3.7662 / -4.2774\n",
      "Spec loss: 11.047794342041016,\t hist loss: 0.7252751588821411\n",
      "Training time for step 102 : 0.3210270404815674\n",
      "[1/5][50/62]\tLoss_D: 0.1591\tLoss_adv: 4.3522\tLoss_G: 4.3522\tD(x): 4.4197\tD(G(z)): -4.3001 / -4.3378\n",
      "Spec loss: 11.027803421020508,\t hist loss: 0.622450053691864\n",
      "Training time for step 112 : 0.31996822357177734\n",
      "[1/5][60/62]\tLoss_D: 0.1686\tLoss_adv: 4.0433\tLoss_G: 4.0433\tD(x): 3.9122\tD(G(z)): -3.6496 / -4.0243\n",
      "Spec loss: 11.00191879272461,\t hist loss: 0.6627709865570068\n",
      "Training time for step 122 : 0.32353925704956055\n",
      "Time taken for epoch 1: 26.82518720626831\n",
      "[2/5][0/62]\tLoss_D: 0.1610\tLoss_adv: 4.2985\tLoss_G: 4.2985\tD(x): 4.0041\tD(G(z)): -4.0564 / -4.2824\n",
      "Spec loss: 10.841623306274414,\t hist loss: 0.7129151821136475\n",
      "Training time for step 124 : 0.32791662216186523\n",
      "Saving best hist model at epoch 2, step 125.\n",
      "Saving best spec model at epoch 2, step 125\n",
      "Saving best spec model at epoch 2, step 126\n",
      "Saving best spec model at epoch 2, step 127\n",
      "Saving best spec model at epoch 2, step 128\n",
      "Saving best spec model at epoch 2, step 129\n",
      "Saving best spec model at epoch 2, step 130\n",
      "Saving best spec model at epoch 2, step 131\n",
      "[2/5][10/62]\tLoss_D: 0.1579\tLoss_adv: 4.4769\tLoss_G: 4.4769\tD(x): 3.9497\tD(G(z)): -4.3842 / -4.4647\n",
      "Spec loss: 10.380925178527832,\t hist loss: 0.8423210978507996\n",
      "Training time for step 134 : 0.3179357051849365\n",
      "Saving best spec model at epoch 2, step 135\n",
      "Saving best spec model at epoch 2, step 140\n",
      "Saving best spec model at epoch 2, step 142\n",
      "[2/5][20/62]\tLoss_D: 0.1667\tLoss_adv: 4.1009\tLoss_G: 4.1009\tD(x): 3.8807\tD(G(z)): -3.6493 / -4.0836\n",
      "Spec loss: 10.267770767211914,\t hist loss: 1.0368574857711792\n",
      "Training time for step 144 : 0.32441091537475586\n",
      "[2/5][30/62]\tLoss_D: 0.1521\tLoss_adv: 4.3520\tLoss_G: 4.3520\tD(x): 4.6438\tD(G(z)): -4.1800 / -4.3381\n",
      "Spec loss: 11.067730903625488,\t hist loss: 0.9259448051452637\n",
      "Training time for step 154 : 0.32117748260498047\n",
      "[2/5][40/62]\tLoss_D: 0.1757\tLoss_adv: 4.7085\tLoss_G: 4.7085\tD(x): 4.3282\tD(G(z)): -3.9268 / -4.6984\n",
      "Spec loss: 10.91238784790039,\t hist loss: 0.9273744225502014\n",
      "Training time for step 164 : 0.32074594497680664\n",
      "Saving best spec model at epoch 2, step 169\n",
      "[2/5][50/62]\tLoss_D: 0.1826\tLoss_adv: 5.0546\tLoss_G: 5.0546\tD(x): 3.6487\tD(G(z)): -6.0340 / -5.0478\n",
      "Spec loss: 11.120962142944336,\t hist loss: 0.9816933274269104\n",
      "Training time for step 174 : 0.3311023712158203\n",
      "[2/5][60/62]\tLoss_D: 0.1700\tLoss_adv: 3.8022\tLoss_G: 3.8022\tD(x): 3.2029\tD(G(z)): -4.2474 / -3.7750\n",
      "Spec loss: 11.173084259033203,\t hist loss: 1.0555875301361084\n",
      "Training time for step 184 : 0.3201417922973633\n",
      "Time taken for epoch 2: 28.470996618270874\n",
      "[3/5][0/62]\tLoss_D: 0.1649\tLoss_adv: 4.6558\tLoss_G: 4.6558\tD(x): 4.3631\tD(G(z)): -4.2276 / -4.6461\n",
      "Spec loss: 11.128373146057129,\t hist loss: 1.069422960281372\n",
      "Training time for step 186 : 0.32752180099487305\n",
      "[3/5][10/62]\tLoss_D: 0.1666\tLoss_adv: 4.4029\tLoss_G: 4.4029\tD(x): 4.5214\tD(G(z)): -4.5878 / -4.3897\n",
      "Spec loss: 10.713441848754883,\t hist loss: 1.1187505722045898\n",
      "Training time for step 196 : 0.3335542678833008\n",
      "[3/5][20/62]\tLoss_D: 0.1623\tLoss_adv: 3.4345\tLoss_G: 3.4345\tD(x): 3.4360\tD(G(z)): -3.8817 / -3.4004\n",
      "Spec loss: 10.655891418457031,\t hist loss: 1.103501558303833\n",
      "Training time for step 206 : 0.32035017013549805\n",
      "[3/5][30/62]\tLoss_D: 0.1613\tLoss_adv: 4.3112\tLoss_G: 4.3112\tD(x): 4.3650\tD(G(z)): -4.1126 / -4.2957\n",
      "Spec loss: 10.674375534057617,\t hist loss: 1.0161612033843994\n",
      "Training time for step 216 : 0.3205556869506836\n",
      "[3/5][40/62]\tLoss_D: 0.1798\tLoss_adv: 4.1032\tLoss_G: 4.1032\tD(x): 4.4274\tD(G(z)): -3.7113 / -4.0864\n",
      "Spec loss: 10.623939514160156,\t hist loss: 0.9712284207344055\n",
      "Training time for step 226 : 0.32569313049316406\n",
      "[3/5][50/62]\tLoss_D: 0.1696\tLoss_adv: 4.3067\tLoss_G: 4.3067\tD(x): 4.4085\tD(G(z)): -4.1369 / -4.2922\n",
      "Spec loss: 10.691112518310547,\t hist loss: 0.9293786883354187\n",
      "Training time for step 236 : 0.3233950138092041\n",
      "[3/5][60/62]\tLoss_D: 0.1620\tLoss_adv: 3.8654\tLoss_G: 3.8654\tD(x): 4.8246\tD(G(z)): -3.8549 / -3.8434\n",
      "Spec loss: 10.888294219970703,\t hist loss: 0.9168545007705688\n",
      "Training time for step 246 : 0.3197212219238281\n",
      "Time taken for epoch 3: 27.026255130767822\n",
      "[4/5][0/62]\tLoss_D: 0.1718\tLoss_adv: 4.5900\tLoss_G: 4.5900\tD(x): 4.7773\tD(G(z)): -4.2424 / -4.5794\n",
      "Spec loss: 10.937239646911621,\t hist loss: 0.9464759230613708\n",
      "Training time for step 248 : 0.3273584842681885\n",
      "[4/5][10/62]\tLoss_D: 0.1608\tLoss_adv: 4.6362\tLoss_G: 4.6362\tD(x): 4.7155\tD(G(z)): -4.1394 / -4.6263\n",
      "Spec loss: 10.934293746948242,\t hist loss: 1.0990732908248901\n",
      "Training time for step 258 : 0.3221321105957031\n",
      "Saving best hist model at epoch 4, step 268.\n",
      "[4/5][20/62]\tLoss_D: 0.2011\tLoss_adv: 6.7467\tLoss_G: 6.7467\tD(x): 3.4139\tD(G(z)): -7.6951 / -6.7453\n",
      "Spec loss: inf,\t hist loss: 0.7590750455856323\n",
      "Training time for step 268 : 0.3239274024963379\n",
      "Saving best hist model at epoch 4, step 272.\n",
      "Saving best hist model at epoch 4, step 273.\n",
      "Saving best hist model at epoch 4, step 274.\n",
      "Saving best hist model at epoch 4, step 275.\n",
      "Saving best hist model at epoch 4, step 276.\n",
      "Saving best hist model at epoch 4, step 278.\n",
      "[4/5][30/62]\tLoss_D: 0.1698\tLoss_adv: 4.7353\tLoss_G: 4.7353\tD(x): 4.3675\tD(G(z)): -3.3107 / -4.7264\n",
      "Spec loss: inf,\t hist loss: 0.5749012231826782\n",
      "Training time for step 278 : 0.3257172107696533\n",
      "Saving best hist model at epoch 4, step 279.\n",
      "Saving best hist model at epoch 4, step 285.\n",
      "Saving best hist model at epoch 4, step 287.\n",
      "[4/5][40/62]\tLoss_D: 0.1646\tLoss_adv: 3.8875\tLoss_G: 3.8875\tD(x): 3.9253\tD(G(z)): -4.4263 / -3.8665\n",
      "Spec loss: inf,\t hist loss: 0.5368653535842896\n",
      "Training time for step 288 : 0.3221299648284912\n",
      "[4/5][50/62]\tLoss_D: 0.1589\tLoss_adv: 4.2495\tLoss_G: 4.2495\tD(x): 4.0479\tD(G(z)): -4.3914 / -4.2346\n",
      "Spec loss: inf,\t hist loss: 0.5795868635177612\n",
      "Training time for step 298 : 0.3182799816131592\n",
      "[4/5][60/62]\tLoss_D: 0.1586\tLoss_adv: 4.2057\tLoss_G: 4.2057\tD(x): 4.3149\tD(G(z)): -3.7027 / -4.1904\n",
      "Spec loss: inf,\t hist loss: 0.6334698796272278\n",
      "Training time for step 308 : 0.3200829029083252\n",
      "Time taken for epoch 4: 28.484992265701294\n",
      "best chis: 0.4871026277542114, 10.059586524963379\n",
      "(200, 128, 128)\n",
      "Image saved in  best_spec_epoch-2_step-169_label-0.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_hist_epoch-4_step-287_label-0.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_spec_epoch-2_step-169_label-1.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_hist_epoch-4_step-287_label-1.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_spec_epoch-2_step-169_label-2.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_hist_epoch-4_step-287_label-2.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_spec_epoch-2_step-169_label-3.npy\n",
      "(200, 128, 128)\n",
      "Image saved in  best_hist_epoch-4_step-287_label-3.npy\n",
      "Total time 147.84139370918274\n",
      "End: 2020-12-14  18:51:42\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #################################       \n",
    "    ### Set up metrics dataframe\n",
    "    cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','spec_chi','hist_chi','D(x)','D_G_z1','D_G_z2','time']\n",
    "    # size=int(len(dataloader) * epochs)+1\n",
    "    metrics_df=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    #################################\n",
    "    ########## Train loop and save metrics and images ######\n",
    "    print(\"Starting Training Loop...\")\n",
    "    f_train_loop(dataloader,metrics_df,gdict)\n",
    "    \n",
    "    ### Generate images for best saved models ######\n",
    "    for cl in np.arange(gdict['num_classes']):\n",
    "        op_loc=gdict['save_dir']+'/images/'\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_spec.tar'\n",
    "        f_gen_images(gdict,netG,optimizerG,cl,ip_fname,op_loc,op_strg='best_spec',op_size=200)\n",
    "\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_hist.tar'\n",
    "        f_gen_images(gdict,netG,optimizerG,cl,ip_fname,op_loc,op_strg='best_hist',op_size=200)\n",
    "\n",
    "    tf=time.time()\n",
    "    print(\"Total time %s\"%(tf-t0))\n",
    "    print('End: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c939a2f75a24e36a4b702926554647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='step'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.plot(x='step',y=['hist_loss','spec_chi'],kind='line')\n",
    "# metrics_df.plot(x='step',y=['hist_loss','hist_chi'],kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_get_hist_cond(img_tensor,categories,bins,gdict,hist_val_tnsr):\n",
    "    ''' Module to compute pixel intensity histogram loss for conditional GAN '''\n",
    "    num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "    loss_hist_tensor=torch.zeros(num_classes,device=device)\n",
    "    for count,i in enumerate(gdict['sigma_list']):    \n",
    "        idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "        if idxs.size(0)>1: \n",
    "            num_frac=idxs.size(0)/img_tensor.shape[0] ## Fraction of points in the category\n",
    "            img=img_tensor[idxs]\n",
    "            loss_hist_tensor[count]=loss_hist(f_compute_hist(img,bins),hist_val_tnsr[count])*num_frac\n",
    "    hist_loss=loss_hist_tensor.sum()\n",
    "    \n",
    "    return hist_loss\n",
    "\n",
    "def f_get_spec_cond(img_tensor,categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind):\n",
    "    ''' Module to compute spectral loss for conditional GAN '''\n",
    "    num_classes=gdict['num_classes'];device=gdict['device']\n",
    "    \n",
    "    loss_spec_tensor=torch.zeros(num_classes,device=device)\n",
    "    for count,i in enumerate(gdict['sigma_list']):    \n",
    "        idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "        if idxs.size(0)>1: \n",
    "            num_frac=idxs.size(0)/img_tensor.shape[0] ## Fraction of points in the category\n",
    "            img=img_tensor[idxs]\n",
    "            mean,sdev=f_torch_image_spectrum(f_invtransform(img),1,r,ind)\n",
    "            loss_spec_tensor[count]=loss_spectrum(mean,spec_mean_tnsr[count],sdev,spec_sdev_tnsr[count],gdict['image_size'],gdict['lambda1'])*num_frac\n",
    "    spec_loss=loss_spec_tensor.sum()\n",
    "    return spec_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_categories=torch.randint(gdict['num_classes'],(gdict['batch_size'],1),device=gdict['device'])\n",
    "# fixed_categories=(torch.ones(gdict['batch_size'],device=gdict['device'])*3).view(gdict['batch_size'],1)\n",
    "if gdict['model']>3: fixed_categories=f_get_sigma(fixed_categories,gdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_get_hist_cond(img_tensor,categories,bins,gdict,hist_val_tnsr):\n",
    "#     ''' Module to compute pixel intensity histogram loss for conditional GAN '''\n",
    "#     num_classes=gdict['num_classes'];device=gdict['device']\n",
    "#     print(img_tensor.shape[0])\n",
    "#     loss_hist_tensor=torch.zeros(num_classes,device=device)\n",
    "#     for count,i in enumerate(gdict['sigma_list']):    \n",
    "#         idxs=torch.where(categories==i)[0] ## Get indices for that category\n",
    "# #         print(categories.shape,idxs)\n",
    "# #         print(i,idxs.size(0))\n",
    "#         if idxs.size(0)>1: \n",
    "#             num_frac=idxs.size(0)/img_tensor.shape[0]\n",
    "#             print(idxs.shape,count,i,num_frac)\n",
    "#             img=img_tensor[idxs]\n",
    "#             loss_hist_tensor[count]=loss_hist(f_compute_hist(img,bins),hist_val_tnsr[count])*num_frac\n",
    "# #             print(f_compute_hist(img,bins),hist_val_tnsr[count])\n",
    "# #             print(torch.log(torch.mean(torch.pow(f_compute_hist(img,bins)-hist_val_tnsr[count],2))))\n",
    "# #             print(loss_hist_tensor)\n",
    "#     hist_loss=loss_hist_tensor.sum()\n",
    "    \n",
    "#     return hist_loss\n",
    "\n",
    "hist_chi=f_get_hist_cond(t_val_img[:128],fixed_categories,50,gdict,hist_val_tnsr)\n",
    "print(hist_chi)\n",
    "spec_chi=f_get_spec_cond(t_val_img[:128],fixed_categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind)\n",
    "print(spec_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_categories\n",
    "# idxs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_chi=f_get_hist_cond(fake,fixed_categories,50,gdict,hist_val_tnsr)\n",
    "print(hist_chi)\n",
    "spec_chi=f_get_spec_cond(fake,fixed_categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_val_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_compute_hist(t_val_img,50),hist_val_tnsr[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def f_get_model(model_name,gdict):\n",
    "    ''' Module to define Generator and Discriminator'''\n",
    "    print(\"Model name\",model_name)\n",
    "    if model_name==1: ## With embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.label_embedding=nn.Embedding(num_classes,num_classes)\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+num_classes,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                labels=labels.unsqueeze(-1).long()\n",
    "                gen_input=torch.cat((self.label_embedding(labels),noise),-1)\n",
    "                img=self.main(gen_input)\n",
    "        #         print(type(img),img.size())\n",
    "        #         img=img.view(128,nc,128,128))\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "                self.label_embedding=nn.Embedding(num_classes,num_classes)\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                labels=labels.unsqueeze(-1).long()\n",
    "                img_size=gdict['image_size']\n",
    "                a=self.label_embedding(labels)\n",
    "                x=a.view(a.size(0),-1)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4))\n",
    "                x=x.view(a.size(0),1,img_size,img_size)\n",
    "        #         print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "        #         d_input=torch.cat((img,self.label_embedding(labels)),-1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==2: #### Model 2: without embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+1,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).float()\n",
    "                print(x.shape)\n",
    "                gen_input=torch.cat((noise,x),-1)\n",
    "                img=self.main(gen_input)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                img_size=gdict['image_size']\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,4).float() # get to size (128,1,1,4)\n",
    "                print(x.shape)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4)) # get to size (128,1, 128, 128)\n",
    "                x=x.view(labels.size(0),1,img_size,img_size)\n",
    "#                 print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==3:#### Model 3: with ConditionalInstanceNorm2d\n",
    "        class ConditionalInstanceNorm2d(nn.Module):\n",
    "            def __init__(self, num_features, num_params):\n",
    "                super().__init__()\n",
    "                self.num_features = num_features\n",
    "                self.InstNorm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "                self.affine = nn.Linear(num_params, num_features * 2)\n",
    "                self.affine.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "                self.affine.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                out = self.InstNorm(x)\n",
    "                gamma, beta = self.affine(y).chunk(2, 1)\n",
    "                out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "                return out\n",
    "\n",
    "        class ConditionalSequential(nn.Sequential):\n",
    "            def __init__(self,*args):\n",
    "                super(ConditionalSequential, self).__init__(*args)\n",
    "\n",
    "            def forward(self, inputs, labels):\n",
    "                for module in self:\n",
    "                    if module.__class__ is ConditionalInstanceNorm2d:\n",
    "                        inputs = module(inputs, labels.float())\n",
    "                    else:\n",
    "                        inputs = module(inputs)\n",
    "\n",
    "                return inputs\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*4,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*2,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                img=self.main(noise,labels)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "        #             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*2,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*4,1),\n",
    "\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*8,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):   \n",
    "                pred=self.main(img,labels)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==4: #### Model 4: without embeddings\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz+1,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).float()\n",
    "                print(x.shape)\n",
    "                gen_input=torch.cat((noise,x),-1)\n",
    "                img=self.main(gen_input)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.linear_transf=nn.Linear(4,4)\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):\n",
    "                img_size=gdict['image_size']\n",
    "                x=labels.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,4).float() # get to size (128,1,1,4)\n",
    "                print(x.shape)\n",
    "                x=self.linear_transf(x)\n",
    "                x=torch.repeat_interleave(x,int((img_size*img_size)/4)) # get to size (128,1, 128, 128)\n",
    "                x=x.view(labels.size(0),1,img_size,img_size)\n",
    "#                 print(x.size())\n",
    "                d_input=torch.cat((img,x),axis=1)\n",
    "                pred=self.main(d_input)\n",
    "                return pred\n",
    "\n",
    "    elif model_name==5:#### Model 5: with ConditionalInstanceNorm2d\n",
    "        class ConditionalInstanceNorm2d(nn.Module):\n",
    "            def __init__(self, num_features, num_params):\n",
    "                super().__init__()\n",
    "                self.num_features = num_features\n",
    "                self.InstNorm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "                self.affine = nn.Linear(num_params, num_features * 2)\n",
    "                self.affine.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "                self.affine.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                out = self.InstNorm(x)\n",
    "                gamma, beta = self.affine(y).chunk(2, 1)\n",
    "                out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "                return out\n",
    "\n",
    "        class ConditionalSequential(nn.Sequential):\n",
    "            def __init__(self,*args):\n",
    "                super(ConditionalSequential, self).__init__(*args)\n",
    "\n",
    "            def forward(self, inputs, labels):\n",
    "                for module in self:\n",
    "                    if module.__class__ is ConditionalInstanceNorm2d:\n",
    "                        inputs = module(inputs, labels.float())\n",
    "                    else:\n",
    "                        inputs = module(inputs)\n",
    "\n",
    "                return inputs\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                num_classes, ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "                    nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8]),\n",
    "                    nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*4,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf*2,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "        #             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ngf,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, noise,labels):\n",
    "                img=self.main(noise,labels)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['num_classes','ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                num_classes, ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "        #             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*2,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*4,1),\n",
    "\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "        #             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*8,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, img,labels):   \n",
    "                pred=self.main(img,labels)\n",
    "                return pred\n",
    "\n",
    "    return Generator, Discriminator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator, Discriminator=f_get_model(4,gdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(gdict).to(gdict['device'])\n",
    "netG.apply(weights_init)\n",
    "netD = Discriminator(gdict).to(gdict['device'])\n",
    "netD.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(gdict['batch_size'], 1, 1, gdict['nz'], device=gdict['device'])\n",
    "fake_categories=torch.randint(gdict['num_classes'],(gdict['batch_size'],1),device=gdict['device'])\n",
    "fake_categories=f_get_sigma(fake_categories,gdict)\n",
    "fake = netG(noise,fake_categories)    \n",
    "\n",
    "print(noise.shape,fake_categories.shape,fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=netD(fake,fake_categories)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total number of parameters\n",
    "sum(p.numel() for p in netD.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(netG,[(1,1,64),(1,1)])\n",
    "# summary(netD,[(1,1,64),(1,1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_get_spec_cond(fake,fake_categories,gdict,spec_mean_tnsr,spec_sdev_tnsr,r,ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_list["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fake_categories[:10]:\n",
    "#     print(i.shape)\n",
    "    print(sigma_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdict['model']>3: fake_categories=f_get_sigma(fake_categories,gdict,sigma_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
