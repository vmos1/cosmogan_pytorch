{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cosmogan\n",
    "Aug 25, 2020\n",
    "\n",
    "Borrowing pieces of code from : \n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/11569e0db3599ac214b03e01956c2971b02c64ce/beginner_source/dcgan_faces_tutorial.py\n",
    "- https://github.com/exalearn/epiCorvid/tree/master/cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "# from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "#from IPython.display import HTML\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections\n",
    "\n",
    "import shutil\n",
    "\n",
    "# # Import modules from other files\n",
    "# from utils import *\n",
    "# from spec_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4.) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Generator Code\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "#         ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "#             nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             View(shape=[-1,ngf*8,8,8]),\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, ip):\n",
    "#         return self.main(ip)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "#         ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "#             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(nc*ndf*8*8*8, 1)\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, ip):\n",
    "#         return self.main(ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, gdict):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        ## Define new variables from dict\n",
    "        keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "        ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "            nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            View(shape=[-1,ngf*8,8,8]),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, ip):\n",
    "        return self.main(ip)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, gdict):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        ## Define new variables from dict\n",
    "        keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "        ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "            nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc*ndf*8*8*8, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, ip):\n",
    "#         print(ip.shape)\n",
    "        results=[ip]\n",
    "        lst_idx=[]\n",
    "        for i,submodel in enumerate(self.main.children()):\n",
    "            mid_output=submodel(results[-1])\n",
    "            results.append(mid_output)\n",
    "            ## Select indices in list corresponding to output of Conv layers\n",
    "            if submodel.__class__.__name__.startswith('Conv'):\n",
    "#                 print(submodel.__class__.__name__)\n",
    "#                 print(mid_output.shape)\n",
    "                lst_idx.append(i)\n",
    "\n",
    "        FMloss=True\n",
    "        if FMloss:\n",
    "            ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "        else :\n",
    "            ans=results[-1]\n",
    "        return ans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gen_images(gdict,netG,optimizerG,ip_fname,op_loc,op_strg='inf_img_',op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: gdict, netG, optimizerG, \n",
    "                 ip_fname: name of input file\n",
    "                op_strg: [string name for output file]\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    nz,device=gdict['nz'],gdict['device']\n",
    "\n",
    "    try:\n",
    "        if torch.cuda.is_available(): checkpoint=torch.load(ip_fname)\n",
    "        else: checkpoint=torch.load(ip_fname,map_location=torch.device('cpu'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "    \n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, nz, device=device)\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    gen = netG(noise)\n",
    "    gen_images=gen.detach().cpu().numpy()[:,:,:,:]\n",
    "    print(gen_images.shape)\n",
    "    \n",
    "    op_fname='%s_epoch-%s_step-%s.npy'%(op_strg,epoch,iters)\n",
    "\n",
    "    np.save(op_loc+op_fname,gen_images)\n",
    "\n",
    "    print(\"Image saved in \",op_fname)\n",
    "    \n",
    "def f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc):\n",
    "    ''' Checkpoint model '''\n",
    "    \n",
    "    if gdict['multi-gpu']: ## Dataparallel\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.module.state_dict(),'D_state':netD.module.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc) \n",
    "    else :\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.state_dict(),'D_state':netD.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc)\n",
    "    \n",
    "def f_load_checkpoint(ip_fname,netG,netD,optimizerG,optimizerD,gdict):\n",
    "    ''' Load saved checkpoint\n",
    "    Also loads step, epoch, best_chi1, best_chi2'''\n",
    "    \n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        raise SystemError\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "        netD.module.load_state_dict(checkpoint['D_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "        netD.load_state_dict(checkpoint['D_state'])\n",
    "    \n",
    "    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    best_chi1=checkpoint['best_chi1']\n",
    "    best_chi2=checkpoint['best_chi2']\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    return iters,epoch,best_chi1,best_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_torch_radial_profile(img, center=(None,None)):\n",
    "    ''' Module to compute radial profile of a 2D image \n",
    "    Bincount causes issues with backprop, so not using this code\n",
    "    '''\n",
    "    \n",
    "    y,x=torch.meshgrid(torch.arange(0,img.shape[0]),torch.arange(0,img.shape[1])) # Get a grid of x and y values\n",
    "    if center[0]==None and center[1]==None:\n",
    "        center = torch.Tensor([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0]) # compute centers\n",
    "\n",
    "    # get radial values of every pair of points\n",
    "    r = torch.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    r= r.int()\n",
    "    \n",
    "#     print(r.shape,img.shape)\n",
    "    # Compute histogram of r values\n",
    "    tbin=torch.bincount(torch.reshape(r,(-1,)),weights=torch.reshape(img,(-1,)).type(torch.DoubleTensor))\n",
    "    nr = torch.bincount(torch.reshape(r,(-1,)))\n",
    "    radialprofile = tbin / nr\n",
    "    \n",
    "    return radialprofile[1:-1]\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage_with_batch(image, center=None): ### Not used in this code.\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile. Only use if you need to combine batches\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, channel, height, width = image.shape\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (batch, channel,-1)))\n",
    "    r_sorted = torch.gather(torch.reshape(r, (batch, channel, -1,)),2, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, (batch, channel, -1,)),2, ind)\n",
    "\n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[:,:,1:] - r_int[:,:,:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[2], (batch, -1))    # location of changes in radius\n",
    "    rind=torch.unsqueeze(rind,1)\n",
    "    nr = (rind[:,:,1:] - rind[:,:,:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "\n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "#     print(csum.shape,rind.shape,nr.shape)\n",
    "\n",
    "    tbin = torch.gather(csum, 2, rind[:,:,1:]) - torch.gather(csum, 2, rind[:,:,:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "\n",
    "def f_get_rad(img):\n",
    "    ''' Get the radial tensor for use in f_torch_get_azimuthalAverage '''\n",
    "    \n",
    "    height,width=img.shape[-2:]\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "    \n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "    \n",
    "    return r.detach(),ind.detach()\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image,r,ind):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "#     height, width = image.shape\n",
    "#     # Create a grid of points with x and y coordinates\n",
    "#     y, x = np.indices([height,width])\n",
    "\n",
    "#     if not center:\n",
    "#         center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "#     # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "#     r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "#     # Get sorted radii\n",
    "#     ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "\n",
    "def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    y1=torch.rfft(arr,signal_ndim=2,onesided=False)\n",
    "    real,imag=f_torch_fftshift(y1[:,:,0],y1[:,:,1])    ## last index is real/imag part\n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    \n",
    "#     print(y2.shape)\n",
    "    z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    \n",
    "    return z1\n",
    "\n",
    "## Method for pytorch 1.8 \n",
    "# import torch.fft\n",
    "# def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "#     GLOBAL_MEAN=1.0\n",
    "#     arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "\n",
    "#     y1=torch.fft.fftn(arr,dim=(-2,-1))\n",
    "#     real,imag=f_torch_fftshift(y1.real,y1.imag)\n",
    "    \n",
    "#     y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    \n",
    "# #     print(y2.shape)\n",
    "#     z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    \n",
    "#     return z1\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr,r,ind):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i,r,ind) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels,r,ind):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    sdev=[[] for i in range(num_channels)]    \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:]\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr,r,ind)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "#         sdev[i]=torch.std(batch_pk,axis=0)/np.sqrt(batch_pk.shape[0])\n",
    "#         sdev[i]=torch.std(batch_pk,axis=0)\n",
    "        sdev[i]=torch.var(batch_pk,axis=0)\n",
    "    \n",
    "    mean=torch.stack(mean)\n",
    "    sdev=torch.stack(sdev)\n",
    "        \n",
    "    return mean,sdev\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    \n",
    "    try: \n",
    "        hist_data=torch.histc(data,bins=bins)\n",
    "        ## A kind of normalization of histograms: divide by total sum\n",
    "        hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hist_data=torch.zeros(bins)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_std,spec_std_ref,image_size,lambda_spec_mean,lambda_spec_var):\n",
    "    ''' Loss function for the spectrum : mean + variance \n",
    "    Log(sum( batch value - expect value) ^ 2 )) '''\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "        \n",
    "    spec_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    spec_sdev=torch.log(torch.mean(torch.pow(spec_std[:,:idx]-spec_std_ref[:,:idx],2)))\n",
    "    \n",
    "    lambda1=lambda_spec_mean;\n",
    "    lambda2=lambda_spec_var;\n",
    "    ans=lambda1*spec_mean+lambda2*spec_sdev\n",
    "    \n",
    "    if torch.isnan(spec_sdev).any():    print(\"spec loss with nan\",ans)\n",
    "    \n",
    "    return ans\n",
    "    \n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return lambda1*torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n",
    "\n",
    "def f_FM_loss(real_output,fake_output,lambda_fm,gdict):\n",
    "    '''\n",
    "    Module to implement Feature-Matching loss. Reads all but last elements of Discriminator ouput\n",
    "    '''\n",
    "    FM=torch.Tensor([0.0]).to(gdict['device'])\n",
    "    for i,j in zip(real_output[:-1][0],fake_output[:-1][0]):\n",
    "        real_mean=torch.mean(i)\n",
    "        fake_mean=torch.mean(j)\n",
    "        FM=FM.clone()+torch.sum(torch.square(real_mean-fake_mean))\n",
    "    return lambda_fm*FM\n",
    "\n",
    "def f_gp_loss(grads,l=1.0):\n",
    "    '''\n",
    "    Module to implement gradient penalty loss.\n",
    "    '''\n",
    "    loss=torch.mean(torch.sum(torch.square(grads),dim=[1,2,3]))\n",
    "    return l*loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_size(ip):\n",
    "#     p=2;s=2\n",
    "# #     return (ip + 2 * 0 - 1 * (p-1) -1 )/ s + 1\n",
    "\n",
    "#     return (ip-1)*s - 2 * p + 1 *(5-1)+ 1 + 1\n",
    "\n",
    "# f_size(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(filename=save_dir+'/log.log',filemode='w',format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_train_loop(dataloader,metrics_df,gdict,fixed_noise,mean_spec_val,sdev_spec_val,hist_val,r,ind):\n",
    "    ''' Train single epoch '''\n",
    "    \n",
    "    ## Define new variables from dict\n",
    "    keys=['image_size','start_epoch','epochs','iters','best_chi1','best_chi2','save_dir','device','flip_prob','nz','batch_size','bns']\n",
    "    image_size,start_epoch,epochs,iters,best_chi1,best_chi2,save_dir,device,flip_prob,nz,batchsize,bns=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        t_epoch_start=time.time()\n",
    "        for count, data in enumerate(dataloader, 0):\n",
    "            print(count,len(data),data[0].shape)\n",
    "            ####### Train GAN ########\n",
    "            netG.train(); netD.train();  ### Need to add these after inference and before training\n",
    "\n",
    "            tme1=time.time()\n",
    "            ### Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            netD.zero_grad()\n",
    "\n",
    "            real_cpu = data[0].to(device)\n",
    "            real_cpu.requires_grad=True\n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), 1, device=device)\n",
    "            fake_label = torch.full((b_size,), 0, device=device)\n",
    "            g_label = torch.full((b_size,), 1, device=device) ## No flipping for Generator labels\n",
    "            # Flip labels with probability flip_prob\n",
    "            for idx in np.random.choice(np.arange(b_size),size=int(np.ceil(b_size*flip_prob))):\n",
    "                real_label[idx]=0; fake_label[idx]=1\n",
    "\n",
    "            # Generate fake image batch with G\n",
    "            noise = torch.randn(b_size, 1, 1, nz, device=device)\n",
    "            fake = netG(noise)            \n",
    "\n",
    "            # Forward pass real batch through D\n",
    "            real_output = netD(real_cpu)\n",
    "#             print(\"Real output\",torch.max(real_output[-1]))\n",
    "            errD_real = criterion(real_output[-1].view(-1), real_label.float())\n",
    "            print(errD_real.item())\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            D_x = real_output[-1].mean().item()\n",
    "\n",
    "            # Forward pass fake batch through D\n",
    "            fake_output = netD(fake.detach())   # The detach is important\n",
    "#             print(\"output for Dfake\",torch.max(fake_output[-1]))\n",
    "            errD_fake = criterion(fake_output[-1].view(-1), fake_label.float())\n",
    "            print(errD_fake.item())\n",
    "            errD_fake.backward(retain_graph=True)\n",
    "            D_G_z1 = fake_output[-1].mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake \n",
    "\n",
    "            if gdict['lambda_gp']: ## Add gradient - penalty loss\n",
    "                grads=torch.autograd.grad(outputs=real_output[-1],inputs=real_cpu,grad_outputs=torch.ones_like(real_output[-1]),allow_unused=False,create_graph=True)[0]\n",
    "                gp_loss=f_gp_loss(grads,gdict['lambda_gp'])\n",
    "                errD = errD + gp_loss\n",
    "            else:\n",
    "                gp_loss=torch.Tensor([np.nan])\n",
    "\n",
    "            optimizerD.step()\n",
    "            \n",
    "            ###Update G network: maximize log(D(G(z)))\n",
    "            netG.zero_grad()\n",
    "            output = netD(fake)\n",
    "#             print(\"op for G\",torch.max(output[-1]))\n",
    "            errG_adv = criterion(output[-1].view(-1), g_label.float())\n",
    "            print(errG_adv.item())\n",
    "            # Histogram pixel intensity loss\n",
    "            hist_gen=f_compute_hist(fake,bins=bns)\n",
    "            hist_loss=loss_hist(hist_gen,hist_val.to(device))\n",
    "\n",
    "            # Add spectral loss\n",
    "            mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1,r.to(device),ind.to(device))\n",
    "            spec_loss=loss_spectrum(mean,mean_spec_val.to(device),sdev,sdev_spec_val.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])\n",
    "            \n",
    "            errG=errG_adv\n",
    "            if gdict['lambda_spec_mean']: errG=errG + spec_loss \n",
    "            if gdict['lambda_fm']:## Add feature matching loss\n",
    "                fm_loss=f_FM_loss(real_output,fake_output,gdict['lambda_fm'],gdict)\n",
    "                errG= errG + fm_loss\n",
    "            else: \n",
    "                fm_loss=torch.Tensor([np.nan])\n",
    "\n",
    "            if torch.isnan(errG).any():\n",
    "                logging.info(errG)\n",
    "                raise SystemError\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            errG.backward(retain_graph=True)\n",
    "            D_G_z2 = output[-1].mean().item()\n",
    "            \n",
    "            ### Implement Gradient clipping\n",
    "            if gdict['grad_clip']:\n",
    "                nn.utils.clip_grad_norm_(netG.parameters(),gdict['grad_clip'])\n",
    "                nn.utils.clip_grad_norm_(netD.parameters(),gdict['grad_clip'])\n",
    "                \n",
    "            optimizerG.step()\n",
    "#             optimizerD.step()\n",
    "\n",
    "            tme2=time.time()\n",
    "\n",
    "            ####### Store metrics ########\n",
    "            # Output training stats\n",
    "            if count % gdict['checkpoint_size'] == 0:\n",
    "                logging.info('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_adv: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, epochs, count, len(dataloader), errD.item(), errG_adv.item(),errG.item(), D_x, D_G_z1, D_G_z2)),\n",
    "                logging.info(\"Spec loss: %s,\\t hist loss: %s\"%(spec_loss.item(),hist_loss.item())),\n",
    "                logging.info(\"Training time for step %s : %s\"%(iters, tme2-tme1))\n",
    "\n",
    "            # Save metrics\n",
    "            cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','fm_loss','gp_loss','D(x)','D_G_z1','D_G_z2','time']\n",
    "            vals=[iters,epoch,errD_real.item(),errD_fake.item(),errD.item(),errG_adv.item(),errG.item(),spec_loss.item(),hist_loss.item(),fm_loss.item(),gp_loss.item(),D_x,D_G_z1,D_G_z2,tme2-tme1]\n",
    "            for col,val in zip(cols,vals):  metrics_df.loc[iters,col]=val\n",
    "\n",
    "            ### Checkpoint the best model\n",
    "            checkpoint=True\n",
    "            iters += 1  ### Model has been updated, so update iters before saving metrics and model.\n",
    "\n",
    "            ### Compute validation metrics for updated model\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                #fake = netG(fixed_noise).detach().cpu()\n",
    "                fake = netG(fixed_noise)\n",
    "                hist_gen=f_compute_hist(fake,bins=bns)\n",
    "                hist_chi=loss_hist(hist_gen,hist_val.to(device))\n",
    "                mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1,r.to(device),ind.to(device))\n",
    "                spec_chi=loss_spectrum(mean,mean_spec_val.to(device),sdev,sdev_spec_val.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])      \n",
    "            # Storing chi for next step\n",
    "            for col,val in zip(['spec_chi','hist_chi'],[spec_chi.item(),hist_chi.item()]):  metrics_df.loc[iters,col]=val            \n",
    "\n",
    "            # Checkpoint model for continuing run\n",
    "            if count == len(dataloader)-1: ## Check point at last step of epoch\n",
    "                f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_last.tar')  \n",
    "\n",
    "            if (checkpoint and (epoch > 1)): # Choose best models by metric\n",
    "                if hist_chi< best_chi1:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_hist.tar')\n",
    "                    best_chi1=hist_chi.item()\n",
    "                    logging.info(\"Saving best hist model at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                if  spec_chi< best_chi2:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_spec.tar')\n",
    "                    best_chi2=spec_chi.item()\n",
    "                    logging.info(\"Saving best spec model at epoch %s, step %s\"%(epoch,iters))\n",
    "                    \n",
    "                if iters in gdict['save_steps_list']:\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_{0}.tar'.format(iters))\n",
    "                    logging.info(\"Saving given-step at epoch %s, step %s.\"%(epoch,iters))\n",
    "                    \n",
    "            # Save G's output on fixed_noise\n",
    "            if ((iters % gdict['checkpoint_size'] == 0) or ((epoch == epochs-1) and (count == len(dataloader)-1))):\n",
    "                netG.eval()\n",
    "                with torch.no_grad():\n",
    "                    fake = netG(fixed_noise).detach().cpu()\n",
    "                    img_arr=np.array(fake[:,:,:,:])\n",
    "                    fname='gen_img_epoch-%s_step-%s'%(epoch,iters)\n",
    "                    np.save(save_dir+'/images/'+fname,img_arr)\n",
    "        \n",
    "        t_epoch_end=time.time()\n",
    "        logging.info(\"Time taken for epoch %s: %s\"%(epoch,t_epoch_end-t_epoch_start))\n",
    "        # Save Metrics to file after each epoch\n",
    "        metrics_df.to_pickle(save_dir+'/df_metrics.pkle')\n",
    "        \n",
    "    logging.info(\"best chis: {0}, {1}\".format(best_chi1,best_chi2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f_manual_add_argparse():\n",
    "    ''' use only in jpt notebook'''\n",
    "    args=argparse.Namespace()\n",
    "    args.config='config_2dgan.yaml'\n",
    "    args.mode='fresh'\n",
    "    args.ip_fldr=''\n",
    "#     args.mode='continue'\n",
    "#     args.ip_fldr='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201211_093818_nb_test/'\n",
    "    \n",
    "    return args\n",
    "\n",
    "def f_parse_args():\n",
    "    \"\"\"Parse command line arguments.Only for .py file\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Run script to train GAN using pytorch\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    add_arg = parser.add_argument\n",
    "    \n",
    "    add_arg('--config','-cfile',  type=str, default='/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/repositories/cosmogan_pytorch/cosmogan/main_code/config_128.yaml', help='Whether to start fresh run or continue previous run')\n",
    "    add_arg('--mode','-m',  type=str, choices=['fresh','continue'],default='fresh', help='Whether to start fresh run or continue previous run')\n",
    "    add_arg('--ip_fldr','-ip',  type=str, default='', help='The input folder for resuming a checkpointed run')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def f_init_gdict(args,gdict):\n",
    "    ''' Create global dictionary gdict from args and config file'''\n",
    "    \n",
    "    ## read config file\n",
    "    config_file=args.config\n",
    "    with open(config_file) as f:\n",
    "        config_dict= yaml.load(f, Loader=yaml.SafeLoader)\n",
    "        \n",
    "    gdict=config_dict['parameters']\n",
    "\n",
    "    ## Add args variables to gdict\n",
    "    for key in ['mode','config','ip_fldr']:\n",
    "        gdict[key]=vars(args)[key]\n",
    "        \n",
    "    return gdict\n",
    "\n",
    "def f_load_data_precompute(gdict):\n",
    "    #################################\n",
    "    ####### Read data and precompute ######\n",
    "    img=np.load(gdict['ip_fname'],mmap_mode='r')[:gdict['num_imgs']].transpose(0,1,2,3).copy()\n",
    "    t_img=torch.from_numpy(img)\n",
    "    print(\"%s, %s\"%(img.shape,t_img.shape))\n",
    "\n",
    "    dataset=TensorDataset(t_img)\n",
    "    data_loader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)\n",
    "\n",
    "    # Precompute metrics with validation data for computing losses\n",
    "    with torch.no_grad():\n",
    "        val_img=np.load(gdict['ip_fname'])[-3000:].transpose(0,1,2,3).copy()\n",
    "        t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "\n",
    "        # Precompute radial coordinates\n",
    "        r,ind=f_get_rad(img)\n",
    "        r=r.to(gdict['device']); ind=ind.to(gdict['device'])\n",
    "        # Stored mean and std of spectrum for full input data once\n",
    "        mean_spec_val,sdev_spec_val=f_torch_image_spectrum(f_invtransform(t_val_img),1,r,ind)\n",
    "        hist_val=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "        del val_img; del t_val_img; del img; del t_img\n",
    "\n",
    "    return data_loader,mean_spec_val,sdev_spec_val,hist_val,r,ind\n",
    "\n",
    "def f_init_GAN(gdict,print_model=False):\n",
    "    # Define Models\n",
    "    print(\"Building GAN networks\")\n",
    "    # Create Generator\n",
    "    netG = Generator(gdict).to(gdict['device'])\n",
    "    netG.apply(weights_init)\n",
    "    # Create Discriminator\n",
    "    netD = Discriminator(gdict).to(gdict['device'])\n",
    "    netD.apply(weights_init)\n",
    "    \n",
    "    if print_model:\n",
    "        print(netG)\n",
    "    #     summary(netG,(1,1,64))\n",
    "        print(netD)\n",
    "    #     summary(netD,(1,128,128))\n",
    "\n",
    "    print(\"Number of GPUs used %s\"%(gdict['ngpu']))\n",
    "    if (gdict['multi-gpu']):\n",
    "        netG = nn.DataParallel(netG, list(range(gdict['ngpu'])))\n",
    "        netD = nn.DataParallel(netD, list(range(gdict['ngpu'])))\n",
    "\n",
    "    #### Initialize networks ####\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if gdict['mode']=='fresh':\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=gdict['learn_rate'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        optimizerG = optim.Adam(netG.parameters(), lr=gdict['learn_rate'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        ### Initialize variables      \n",
    "        iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10    \n",
    "\n",
    "    ### Load network weights for continuing run\n",
    "    elif gdict['mode']=='continue':\n",
    "        iters,start_epoch,best_chi1,best_chi2=f_load_checkpoint(gdict['save_dir']+'/models/checkpoint_last.tar',netG,netD,optimizerG,optimizerD,gdict) \n",
    "        logging.info(\"Continuing existing run. Loading checkpoint with epoch {0} and step {1}\".format(start_epoch,iters))\n",
    "        start_epoch+=1  ## Start with the next epoch  \n",
    "\n",
    "    ## Add to gdict\n",
    "    for key,val in zip(['best_chi1','best_chi2','iters','start_epoch'],[best_chi1,best_chi2,iters,start_epoch]): gdict[key]=val\n",
    "\n",
    "    return netG,netD,criterion,optimizerD,optimizerG\n",
    "\n",
    "def f_setup(gdict,log):\n",
    "    ''' \n",
    "    Set up directories, Initialize random seeds, add GPU info, add logging info.\n",
    "    '''\n",
    "    \n",
    "    torch.backends.cudnn.benchmark=True\n",
    "#     torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    ###### Set up directories #######\n",
    "    if gdict['mode']=='fresh':\n",
    "        # Create prefix for foldername        \n",
    "        fldr_name=datetime.now().strftime('%Y%m%d_%H%M%S') ## time format\n",
    "        gdict['save_dir']=gdict['op_loc']+fldr_name+'_'+gdict['run_suffix']\n",
    "\n",
    "        if not os.path.exists(gdict['save_dir']):\n",
    "            os.makedirs(gdict['save_dir']+'/models')\n",
    "            os.makedirs(gdict['save_dir']+'/images')\n",
    "            shutil.copy(gdict['config'],gdict['save_dir'])\n",
    "\n",
    "    elif gdict['mode']=='continue': ## For checkpointed runs\n",
    "        gdict['save_dir']=args.ip_fldr\n",
    "        ### Read loss data\n",
    "        with open (gdict['save_dir']+'df_metrics.pkle','rb') as f:\n",
    "            metrics_dict=pickle.load(f)\n",
    "\n",
    "    ### Initialize random seed\n",
    "    manualSeed = np.random.randint(1, 10000) if gdict['seed']=='random' else int(gdict['seed'])\n",
    "    random.seed(manualSeed)\n",
    "    np.random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "    if gdict['deterministic']:\n",
    "        logging.info(\"Running with deterministic sequence. Performance will be slower\")\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "#         torch.backends.cudnn.enabled = False\n",
    "        torch.backends.cudnn.benchmark = False        \n",
    "\n",
    "    ## Special declarations\n",
    "    gdict['ngpu']=torch.cuda.device_count()\n",
    "    gdict['device']=torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    gdict['multi-gpu']=True if (gdict['device'].type == 'cuda') and (gdict['ngpu'] > 1) else False \n",
    "\n",
    "    \n",
    "    if log:\n",
    "        ### Write all logging.info statements to stdout and log file (different for jpt notebooks)\n",
    "        logfile=gdict['save_dir']+'/log.log'\n",
    "        logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "\n",
    "        Lg = logging.getLogger()\n",
    "        Lg.setLevel(logging.DEBUG)\n",
    "        lg_handler_file = logging.FileHandler(logfile)\n",
    "        lg_handler_stdout = logging.StreamHandler(sys.stdout)\n",
    "        Lg.addHandler(lg_handler_file)\n",
    "        Lg.addHandler(lg_handler_stdout)\n",
    "\n",
    "        logging.info('Args: {0}'.format(args))\n",
    "        logging.info('Start: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "        logging.info('Device:{0}'.format(gdict['device']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building GAN networks\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32768, bias=True)\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): View()\n",
      "    (4): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (14): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "    (13): Linear(in_features=32768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of GPUs used 1\n",
      "(40000, 1, 128, 128), torch.Size([40000, 1, 128, 128])\n",
      "Starting Training Loop...\n",
      "0 1 torch.Size([128, 1, 128, 128])\n",
      "0.7828207612037659\n",
      "0.6524453163146973\n",
      "2.529066562652588\n",
      "1 1 torch.Size([128, 1, 128, 128])\n",
      "0.24447385966777802\n",
      "0.5150293111801147\n",
      "4.3953537940979\n",
      "2 1 torch.Size([128, 1, 128, 128])\n",
      "0.3046662211418152\n",
      "0.07960699498653412\n",
      "3.7810354232788086\n",
      "3 1 torch.Size([128, 1, 128, 128])\n",
      "0.09142564237117767\n",
      "0.3838962912559509\n",
      "5.027755260467529\n",
      "4 1 torch.Size([128, 1, 128, 128])\n",
      "0.10388112813234329\n",
      "0.15257325768470764\n",
      "6.037596702575684\n",
      "5 1 torch.Size([128, 1, 128, 128])\n",
      "0.11707348376512527\n",
      "0.07628190517425537\n",
      "5.523626804351807\n",
      "6 1 torch.Size([128, 1, 128, 128])\n",
      "0.10691915452480316\n",
      "0.20345449447631836\n",
      "6.615776538848877\n",
      "7 1 torch.Size([128, 1, 128, 128])\n",
      "0.10950718075037003\n",
      "0.11086737364530563\n",
      "6.577586650848389\n",
      "8 1 torch.Size([128, 1, 128, 128])\n",
      "0.1127636730670929\n",
      "0.09315448999404907\n",
      "5.79403018951416\n",
      "9 1 torch.Size([128, 1, 128, 128])\n",
      "0.109200119972229\n",
      "0.1058315858244896\n",
      "7.03672981262207\n",
      "10 1 torch.Size([128, 1, 128, 128])\n",
      "0.06103801727294922\n",
      "0.08161643147468567\n",
      "6.488081932067871\n",
      "11 1 torch.Size([128, 1, 128, 128])\n",
      "0.09316876530647278\n",
      "0.08166223019361496\n",
      "5.0313401222229\n",
      "12 1 torch.Size([128, 1, 128, 128])\n",
      "0.08775763213634491\n",
      "0.5515226125717163\n",
      "13.554741859436035\n",
      "13 1 torch.Size([128, 1, 128, 128])\n",
      "0.5236992835998535\n",
      "0.15068864822387695\n",
      "10.889816284179688\n",
      "14 1 torch.Size([128, 1, 128, 128])\n",
      "0.09465448558330536\n",
      "0.07473307847976685\n",
      "4.379920959472656\n",
      "15 1 torch.Size([128, 1, 128, 128])\n",
      "0.10697281360626221\n",
      "1.585776925086975\n",
      "16.131855010986328\n",
      "16 1 torch.Size([128, 1, 128, 128])\n",
      "0.10388242453336716\n",
      "0.17961440980434418\n",
      "17.361228942871094\n",
      "17 1 torch.Size([128, 1, 128, 128])\n",
      "0.24465276300907135\n",
      "0.1644299030303955\n",
      "12.070423126220703\n",
      "18 1 torch.Size([128, 1, 128, 128])\n",
      "0.0976189374923706\n",
      "0.08388526737689972\n",
      "4.301454544067383\n",
      "19 1 torch.Size([128, 1, 128, 128])\n",
      "0.06776689738035202\n",
      "2.325324058532715\n",
      "15.98892879486084\n",
      "20 1 torch.Size([128, 1, 128, 128])\n",
      "0.9281812906265259\n",
      "0.15620383620262146\n",
      "14.033967971801758\n",
      "21 1 torch.Size([128, 1, 128, 128])\n",
      "0.08319808542728424\n",
      "0.10330943763256073\n",
      "5.803777694702148\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f208e6593852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmetrics_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Training Loop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mf_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_spec_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msdev_spec_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhist_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m## Generate images for best saved models ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72cb2c81ac69>\u001b[0m in \u001b[0;36mf_train_loop\u001b[0;34m(dataloader, metrics_df, gdict, fixed_noise, mean_spec_val, sdev_spec_val, hist_val, r, ind)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m#fake = netG(fixed_noise).detach().cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mhist_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_compute_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mhist_chi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhist_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_torch_image_spectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_invtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-451861109cf1>\u001b[0m in \u001b[0;36mf_compute_hist\u001b[0;34m(data, bins)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mhist_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;31m## A kind of normalization of histograms: divide by total sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mhist_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_data\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "#     jpt=False\n",
    "    jpt=True ##(different for jupyter notebook)\n",
    "\n",
    "    t0=time.time()\n",
    "    args=f_parse_args() if not jpt else f_manual_add_argparse()\n",
    "\n",
    "    #################################\n",
    "    ### Set up global dictionary###\n",
    "    gdict={}\n",
    "    gdict=f_init_gdict(args,gdict)\n",
    "    \n",
    "    if jpt: ## override for jpt nbks\n",
    "        gdict['num_imgs']=40000\n",
    "        gdict['run_suffix']='nb_test'\n",
    "        \n",
    "    f_setup(gdict,log=(not jpt))\n",
    "    \n",
    "    ## Build GAN\n",
    "    netG,netD,criterion,optimizerD,optimizerG=f_init_GAN(gdict,print_model=True)\n",
    "    fixed_noise = torch.randn(gdict['batch_size'], 1, 1, gdict['nz'], device=gdict['device']) #Latent vectors to view G progress    \n",
    "\n",
    "    ## Load data and precompute\n",
    "    dataloader,mean_spec_val,sdev_spec_val,hist_val,r,ind=f_load_data_precompute(gdict)\n",
    "    \n",
    "    #################################\n",
    "    ########## Train loop and save metrics and images ######\n",
    "    ### Set up metrics dataframe\n",
    "    cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','spec_chi','hist_chi','gp_loss','fm_loss','D(x)','D_G_z1','D_G_z2','time']\n",
    "    metrics_df=pd.DataFrame(columns=cols)\n",
    "    print(\"Starting Training Loop...\")\n",
    "    f_train_loop(dataloader,metrics_df,gdict,fixed_noise,mean_spec_val,sdev_spec_val,hist_val,r,ind)\n",
    "    \n",
    "    ## Generate images for best saved models ######\n",
    "    op_loc=gdict['save_dir']+'/images/'\n",
    "    ip_fname=gdict['save_dir']+'/models/checkpoint_best_spec.tar'\n",
    "    f_gen_images(gdict,netG,optimizerG,ip_fname,op_loc,op_strg='best_spec',op_size=200)\n",
    "    \n",
    "    ip_fname=gdict['save_dir']+'/models/checkpoint_best_hist.tar'\n",
    "    f_gen_images(gdict,netG,optimizerG,ip_fname,op_loc,op_strg='best_hist',op_size=200)\n",
    "    \n",
    "    tf=time.time()\n",
    "    print(\"Total time %s\"%(tf-t0))\n",
    "    print('End: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.plot('step','time')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d5e44ddcdb431b9b9702b5e2daf333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='step'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.plot('step','G_adv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/norm_1_train_val.npy',\n",
       " 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/',\n",
       " 'image_size': 128,\n",
       " 'num_imgs': 400,\n",
       " 'workers': 2,\n",
       " 'nc': 1,\n",
       " 'nz': 64,\n",
       " 'ngf': 64,\n",
       " 'ndf': 64,\n",
       " 'beta1': 0.5,\n",
       " 'kernel_size': 5,\n",
       " 'stride': 2,\n",
       " 'g_padding': 2,\n",
       " 'd_padding': 2,\n",
       " 'flip_prob': 0.01,\n",
       " 'checkpoint_size': 10,\n",
       " 'batch_size': 128,\n",
       " 'epochs': 5,\n",
       " 'learn_rate': 0.0002,\n",
       " 'bns': 50,\n",
       " 'deterministic': False,\n",
       " 'distributed': False,\n",
       " 'seed': 234373,\n",
       " 'lambda_spec_mean': 1.0,\n",
       " 'lambda_spec_var': 1.0,\n",
       " 'lambda_fm': 0.0,\n",
       " 'lambda_gp': 0.0,\n",
       " 'grad_clip': 1.0,\n",
       " 'save_steps_list': [5, 10],\n",
       " 'run_suffix': 'nb_test',\n",
       " 'description': '2d GAN: new loss',\n",
       " 'mode': 'fresh',\n",
       " 'config': 'config_2dgan.yaml',\n",
       " 'ip_fldr': '',\n",
       " 'save_dir': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20210408_102004_nb_test',\n",
       " 'ngpu': 1,\n",
       " 'device': device(type='cuda'),\n",
       " 'multi-gpu': False,\n",
       " 'best_chi1': 10000000000.0,\n",
       " 'best_chi2': 10000000000.0,\n",
       " 'iters': 0,\n",
       " 'start_epoch': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matching loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "#         ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "#             nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             View(shape=[-1,ngf*8,8,8]),\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, ip):\n",
    "#         return self.main(ip)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "#         ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "#             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(nc*ndf*8*8*8, 1)\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, ip):\n",
    "# #         print(ip.shape)\n",
    "#         results=[ip]\n",
    "#         lst_idx=[]\n",
    "#         for i,submodel in enumerate(self.main.children()):\n",
    "#             mid_output=submodel(results[-1])\n",
    "#             results.append(mid_output)\n",
    "#             ## Select indices in list corresponding to output of Conv layers\n",
    "#             if submodel.__class__.__name__.startswith('Conv'):\n",
    "# #                 print(submodel.__class__.__name__)\n",
    "# #                 print(mid_output.shape)\n",
    "#                 lst_idx.append(i)\n",
    "\n",
    "#         FMloss=True\n",
    "#         if FMloss:\n",
    "#             ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "#         else :\n",
    "#             ans=results[-1]\n",
    "#         return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator(gdict).to(gdict['device'])\n",
    "# netG.apply(weights_init)\n",
    "# # # #     print(netG)\n",
    "# # summary(netG,(1,1,64))\n",
    "# # Create Discriminator\n",
    "# netD = Discriminator(gdict).to(gdict['device'])\n",
    "# netD.apply(weights_init)\n",
    "# #     print(netD)\n",
    "# summary(netD,(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn(gdict['batchsize'], 1, 1, gdict['nz'], device=gdict['device'])\n",
    "# fake = netG(noise)            \n",
    "# # Forward pass real batch through D\n",
    "# output = netD(fake)\n",
    "# print([i.shape for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1, 128, 128), torch.Size([400, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "####### Read data and precompute ######\n",
    "img=np.load(gdict['ip_fname'],mmap_mode='r')[:gdict['num_imgs']].transpose(0,1,2,3).copy()\n",
    "t_img=torch.from_numpy(img)\n",
    "print(\"%s, %s\"%(img.shape,t_img.shape))\n",
    "\n",
    "dataset=TensorDataset(t_img)\n",
    "data_loader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "240 320\n",
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2aab4201d280>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_sample_data(ip_tensor,rank=0,num_ranks=1):\n",
    "    \n",
    "    data_size=ip_tensor.shape[0]\n",
    "    size=data_size//num_ranks\n",
    "    print(size)\n",
    "    print(rank*(size),(rank+1)*size)\n",
    "    dataset=TensorDataset(ip_tensor[rank*(size):(rank+1)*size])\n",
    "    data_loader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)\n",
    "    print(len(data_loader.dataset))\n",
    "    return data_loader\n",
    "\n",
    "f_sample_data(t_img,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_get_iterator',\n",
       " '_index_sampler',\n",
       " '_is_protocol',\n",
       " '_iterator',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'generator',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'persistent_workers',\n",
       " 'pin_memory',\n",
       " 'prefetch_factor',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 1, 128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 128, 128])\n",
      "torch.Size([128, 1, 128, 128])\n",
      "torch.Size([128, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
