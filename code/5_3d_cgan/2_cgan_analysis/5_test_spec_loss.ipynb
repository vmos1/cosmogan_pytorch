{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test spectral and histogram loss for 3D images\n",
    "Jan 6, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcurand.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-193f1ada314a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: libcurand.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.fft\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_get_rad(img):\n",
    "    ''' Get the radial tensor for use in f_torch_get_azimuthalAverage '''\n",
    "    \n",
    "    height,width,depth=img.shape[-3:]\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    z,y,x = np.indices([height,width,depth])\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0, (z.max()-z.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r= torch.tensor(np.sqrt((x-center[0])**2 + (y-center[1])**2 + (z-center[2])**2))\n",
    "        \n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    return r.detach(),ind.detach()\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image,r,ind):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "#     height,width,depth=img.shape[-3:]\n",
    "#     # Create a grid of points with x and y coordinates\n",
    "#     z,y,x = np.indices([height,width,depth])\n",
    "    \n",
    "#     center=[]\n",
    "#     if not center:\n",
    "#         center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0, (z.max()-z.min())/2.0])\n",
    "\n",
    "#     # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "#     r= torch.tensor(np.sqrt((x-center[0])**2 + (y-center[1])**2 + (z-center[2])**2))\n",
    "        \n",
    "#     # Get sorted radii\n",
    "#     ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    print(real.shape,imag.shape,real.size(),real.size(0))\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    \n",
    "#     y1=torch.rfft(arr,signal_ndim=3,onesided=False) ## 3D FFT\n",
    "#     real,imag=f_torch_fftshift(y1[:,:,:,0],y1[:,:,:,1])    ## last index is real/imag part\n",
    "    \n",
    "    y1=torch.fft.fftn(arr,dim=(-3,-2,-1))\n",
    "    real,imag=f_torch_fftshift(y1.real,y1.imag)\n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    return z1\n",
    "\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr,r,ind):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i,r,ind) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels,r,ind):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    var=[[] for i in range(num_channels)]    \n",
    "    \n",
    "    print(x.shape)\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:,:]\n",
    "        print(arr.shape)\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr,r,ind)\n",
    "        print(batch_pk.shape)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "        var[i]=torch.var(batch_pk,axis=0)\n",
    "    \n",
    "    mean=torch.stack(mean)\n",
    "    var=torch.stack(var)\n",
    "        \n",
    "    return mean,var\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    \n",
    "    try: \n",
    "        hist_data=torch.histc(data,bins=bins)\n",
    "        ## A kind of normalization of histograms: divide by total sum\n",
    "        hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hist_data=torch.zeros(bins)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_std,spec_std_ref,image_size,lambda1):\n",
    "    ''' Loss function for the spectrum : mean + variance \n",
    "    Log(sum( batch value - expect value) ^ 2 )) '''\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "        \n",
    "    spec_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    spec_sdev=torch.log(torch.mean(torch.pow(spec_std[:,:idx]-spec_std_ref[:,:idx],2)))\n",
    "    \n",
    "    lambda1=lambda1;\n",
    "    lambda2=lambda1;\n",
    "    ans=lambda1*spec_mean+lambda2*spec_sdev\n",
    "    \n",
    "    if torch.isnan(spec_sdev).any():    print(\"spec loss with nan\",ans)\n",
    "    \n",
    "    return ans\n",
    "    \n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return lambda1*torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numpy code\n",
    "def f_radial_profile(data, center=(None,None)):\n",
    "    ''' Module to compute radial profile of a 2D image '''\n",
    "    z, y, x = np.indices((data.shape)) # Get a grid of x and y values\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0, (z.max()-z.min())/2.0]) # compute centers\n",
    "        \n",
    "    # get radial values of every pair of points\n",
    "    r = np.sqrt((x - center[0])**2 + (y - center[1])**2+ + (z - center[2])**2)\n",
    "    r = r.astype(np.int)\n",
    "    \n",
    "    # Compute histogram of r values\n",
    "    tbin = np.bincount(r.ravel(), data.ravel())\n",
    "    nr = np.bincount(r.ravel()) \n",
    "    radialprofile = tbin / nr\n",
    "    \n",
    "    return radialprofile[1:-1]\n",
    "\n",
    "def f_compute_spectrum(arr):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=((arr - GLOBAL_MEAN)/GLOBAL_MEAN)\n",
    "    y1=np.fft.fftn(arr)\n",
    "    y1=np.fft.fftshift(y1)\n",
    "#     print(y1.shape)\n",
    "    y2=abs(y1)**2\n",
    "    z1=f_radial_profile(y2)\n",
    "    return(z1)\n",
    "   \n",
    "def f_compute_batch_spectrum(arr):\n",
    "    batch_pk=np.array([f_compute_spectrum(i) for i in arr])\n",
    "    return batch_pk\n",
    "\n",
    "def f_image_spectrum(x,num_channels):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    var=[[] for i in range(num_channels)]    \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:,:]\n",
    "#         print(i,arr.shape)\n",
    "        batch_pk=f_compute_batch_spectrum(arr)\n",
    "#         print(batch_pk)\n",
    "        mean[i]=np.mean(batch_pk,axis=0)\n",
    "        var[i]=np.var(batch_pk,axis=0)\n",
    "    mean=np.array(mean)\n",
    "    var=np.array(var)\n",
    "    return mean,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "## Read input\n",
    "ip_fname='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/3d_data/dataset1_smoothing_const_params_64cube_100k/full_with_smoothing_1.npy'\n",
    "img=np.load(ip_fname,mmap_mode='r')[:20]\n",
    "# img=np.expand_dims(img,axis=1).astype(float)\n",
    "t_img=torch.from_numpy(img)\n",
    "print(t_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 64]) torch.Size([262144])\n",
      "torch.Size([20, 1, 64, 64, 64])\n",
      "torch.Size([20, 64, 64, 64])\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) torch.Size([64, 64, 64]) 64\n",
      "torch.Size([20, 53])\n"
     ]
    }
   ],
   "source": [
    "np_mean,np_var=f_image_spectrum(img,1)\n",
    "r,ind=f_get_rad(t_img)\n",
    "print(r.shape,ind.shape)\n",
    "trch_mean,trch_var=f_torch_image_spectrum(t_img,1,r,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_torch_fftshift(real, imag):\n",
    "#     for dim in range(0, len(real.size())):\n",
    "#         real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "#         imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "#     return real, imag\n",
    "# trch_mean,trch_var=f_torch_image_spectrum(t_img[:1],1,r,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Comparison of numpy and pytorch results\n",
    "print(np.allclose(np_mean,trch_mean.numpy(),rtol=1e-2,atol=1e-8))\n",
    "print(np.allclose(np_var,trch_var.numpy(),rtol=1e-1,atol=1e-8))\n",
    "\n",
    "for i in range(np_var.shape[1]):\n",
    "    b=trch_var.numpy()\n",
    "    a=np_var[0,i];b=trch_var.numpy()[0,i]\n",
    "    if (np.abs(a-b)/(a) >0.1 ):     print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.10719086e+08, 2.91767463e+08, 1.77334734e+08, 1.28270119e+08,\n",
       "         9.94572520e+07, 7.87818523e+07, 6.28514297e+07, 5.18244073e+07,\n",
       "         4.17857302e+07, 3.48175773e+07, 2.94484534e+07, 2.43341285e+07,\n",
       "         2.02252346e+07, 1.65113071e+07, 1.37114477e+07, 1.12951316e+07,\n",
       "         9.39835009e+06, 7.71050217e+06, 6.39005970e+06, 5.23646511e+06,\n",
       "         4.31519587e+06, 3.58246827e+06, 3.00170705e+06, 2.49097110e+06,\n",
       "         2.08720952e+06, 1.77080074e+06, 1.50412270e+06, 1.28634519e+06,\n",
       "         1.10953798e+06, 9.75612599e+05, 8.64029137e+05, 7.31952770e+05,\n",
       "         5.98999974e+05, 4.86712463e+05, 4.04388042e+05, 3.31951601e+05,\n",
       "         2.79500569e+05, 2.35696127e+05, 2.00900660e+05, 1.71240670e+05,\n",
       "         1.52199265e+05, 1.28968247e+05, 1.13715134e+05, 9.46605564e+04,\n",
       "         7.69839578e+04, 6.30120982e+04, 5.26664982e+04, 4.54706596e+04,\n",
       "         4.13918847e+04, 3.67561613e+04, 3.38007635e+04, 3.34661198e+04,\n",
       "         3.08665671e+04]]),\n",
       " tensor([[5.1072e+08, 2.9177e+08, 1.7733e+08, 1.2827e+08, 9.9457e+07, 7.8782e+07,\n",
       "          6.2851e+07, 5.1824e+07, 4.1786e+07, 3.4818e+07, 2.9448e+07, 2.4334e+07,\n",
       "          2.0225e+07, 1.6511e+07, 1.3711e+07, 1.1295e+07, 9.3983e+06, 7.7105e+06,\n",
       "          6.3901e+06, 5.2365e+06, 4.3152e+06, 3.5825e+06, 3.0017e+06, 2.4910e+06,\n",
       "          2.0872e+06, 1.7708e+06, 1.5041e+06, 1.2863e+06, 1.1095e+06, 9.7561e+05,\n",
       "          8.6403e+05, 7.3195e+05, 5.9900e+05, 4.8671e+05, 4.0439e+05, 3.3195e+05,\n",
       "          2.7950e+05, 2.3569e+05, 2.0090e+05, 1.7124e+05, 1.5220e+05, 1.2897e+05,\n",
       "          1.1371e+05, 9.4663e+04, 7.6983e+04, 6.3019e+04, 5.2658e+04, 4.5475e+04,\n",
       "          4.1398e+04, 3.6742e+04, 3.3825e+04, 3.3423e+04, 3.1016e+04]]),\n",
       " array([[1.59476665e+17, 4.71284048e+16, 1.77955498e+16, 9.12247764e+15,\n",
       "         5.62804957e+15, 3.39116297e+15, 2.06493271e+15, 1.45372679e+15,\n",
       "         8.71690619e+14, 5.90060413e+14, 4.05035184e+14, 2.69485529e+14,\n",
       "         1.76690964e+14, 1.07151722e+14, 6.70411295e+13, 4.23557959e+13,\n",
       "         2.79753817e+13, 1.76634527e+13, 1.15942193e+13, 7.17870174e+12,\n",
       "         4.63813948e+12, 2.86864467e+12, 1.87615084e+12, 1.19449923e+12,\n",
       "         7.91369600e+11, 5.37613846e+11, 3.70351534e+11, 2.50405321e+11,\n",
       "         1.75361213e+11, 1.27482877e+11, 9.71643093e+10, 6.66487290e+10,\n",
       "         4.48499512e+10, 2.83166046e+10, 1.90982432e+10, 1.23734957e+10,\n",
       "         8.42193902e+09, 5.88761837e+09, 4.19235715e+09, 3.12551819e+09,\n",
       "         2.38808655e+09, 1.67414625e+09, 1.19897330e+09, 8.44699856e+08,\n",
       "         5.81757749e+08, 3.82604491e+08, 2.72626289e+08, 2.03834480e+08,\n",
       "         1.64744687e+08, 1.00547054e+08, 9.10725173e+07, 1.11566276e+08,\n",
       "         9.36821284e+07]]),\n",
       " tensor([[1.6787e+17, 4.9609e+16, 1.8732e+16, 9.6026e+15, 5.9243e+15, 3.5696e+15,\n",
       "          2.1736e+15, 1.5302e+15, 9.1757e+14, 6.2112e+14, 4.2635e+14, 2.8367e+14,\n",
       "          1.8599e+14, 1.1279e+14, 7.0570e+13, 4.4585e+13, 2.9448e+13, 1.8593e+13,\n",
       "          1.2204e+13, 7.5565e+12, 4.8823e+12, 3.0196e+12, 1.9749e+12, 1.2574e+12,\n",
       "          8.3303e+11, 5.6590e+11, 3.8984e+11, 2.6359e+11, 1.8459e+11, 1.3419e+11,\n",
       "          1.0228e+11, 7.0157e+10, 4.7210e+10, 2.9807e+10, 2.0103e+10, 1.3025e+10,\n",
       "          8.8655e+09, 6.1969e+09, 4.4130e+09, 3.2902e+09, 2.5136e+09, 1.7626e+09,\n",
       "          1.2617e+09, 8.8951e+08, 6.1210e+08, 4.0339e+08, 2.8621e+08, 2.1485e+08,\n",
       "          1.7409e+08, 1.0518e+08, 9.6378e+07, 1.1618e+08, 9.9361e+07]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_mean,trch_mean,np_var,trch_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OLCF-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
