{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cosmogan\n",
    "April 19, 2021\n",
    "\n",
    "Borrowing pieces of code from : \n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/11569e0db3599ac214b03e01956c2971b02c64ce/beginner_source/dcgan_faces_tutorial.py\n",
    "- https://github.com/exalearn/epiCorvid/tree/master/cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "# import torch.fft\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections\n",
    "import socket\n",
    "import shutil\n",
    "\n",
    "# # Import modules from other files\n",
    "# from utils import *\n",
    "# from spec_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4.) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s)\n",
    "\n",
    "  \n",
    "# Generator Code\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, gdict):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        ## Define new variables from dict\n",
    "        keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "        ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Linear(nz,nc*ngf*8**3*8),# 262144\n",
    "            nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            View(shape=[-1,ngf*8,8,8,8]),\n",
    "            nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "            nn.BatchNorm3d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm3d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, ip):\n",
    "        return self.main(ip)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, gdict):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        ## Define new variables from dict\n",
    "        keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "        ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64 x 64\n",
    "            # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Conv3d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "            nn.BatchNorm3d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm3d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm3d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm3d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc*ndf*8*8*8*8, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, ip):\n",
    "        results=[ip]\n",
    "        lst_idx=[]\n",
    "        for i,submodel in enumerate(self.main.children()):\n",
    "            mid_output=submodel(results[-1])\n",
    "            results.append(mid_output)\n",
    "            ## Select indices in list corresponding to output of Conv layers\n",
    "            if submodel.__class__.__name__.startswith('Conv'):\n",
    "#                 print(submodel.__class__.__name__)\n",
    "#                 print(mid_output.shape)\n",
    "                lst_idx.append(i)\n",
    "\n",
    "        FMloss=True\n",
    "        if FMloss:\n",
    "            ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "        else :\n",
    "            ans=results[-1]\n",
    "        return ans\n",
    "\n",
    "def f_gen_images(gdict,netG,optimizerG,ip_fname,op_loc,op_strg='inf_img_',op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: gdict, netG, optimizerG, \n",
    "                 ip_fname: name of input file\n",
    "                op_strg: [string name for output file]\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    nz,device=gdict['nz'],gdict['device']\n",
    "\n",
    "    try:# handling cpu vs gpu\n",
    "        if torch.cuda.is_available(): checkpoint=torch.load(ip_fname)\n",
    "        else: checkpoint=torch.load(ip_fname,map_location=torch.device('cpu'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "    \n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, 1, nz, device=device)\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    with torch.no_grad(): ## This is important. fails without it for multi-gpu\n",
    "        gen = netG(noise)\n",
    "        gen_images=gen.detach().cpu().numpy()\n",
    "        print(gen_images.shape)\n",
    "    \n",
    "    op_fname='%s_epoch-%s_step-%s.npy'%(op_strg,epoch,iters)\n",
    "    np.save(op_loc+op_fname,gen_images)\n",
    "\n",
    "    print(\"Image saved in \",op_fname)\n",
    "    \n",
    "def f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc):\n",
    "    ''' Checkpoint model '''\n",
    "    \n",
    "    if gdict['multi-gpu']: ## Dataparallel\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.module.state_dict(),'D_state':netD.module.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc) \n",
    "    else :\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.state_dict(),'D_state':netD.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc)\n",
    "    \n",
    "\n",
    "def f_load_checkpoint(ip_fname,netG,netD,optimizerG,optimizerD,gdict):\n",
    "    ''' Load saved checkpoint\n",
    "    Also loads step, epoch, best_chi1, best_chi2'''\n",
    "    \n",
    "    print(\"torch device\",torch.device('cuda',torch.cuda.current_device()))\n",
    "            \n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname,map_location=torch.device('cuda',torch.cuda.current_device()))\n",
    "    except Exception as e:\n",
    "        print(\"Error loading saved checkpoint\",ip_fname)\n",
    "        print(e)\n",
    "        raise SystemError\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "        netD.module.load_state_dict(checkpoint['D_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "        netD.load_state_dict(checkpoint['D_state'])\n",
    "    \n",
    "    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    best_chi1=checkpoint['best_chi1']\n",
    "    best_chi2=checkpoint['best_chi2']\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    return iters,epoch,best_chi1,best_chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_get_rad(img):\n",
    "    ''' Get the radial tensor for use in f_torch_get_azimuthalAverage '''\n",
    "    \n",
    "    height,width,depth=img.shape[-3:]\n",
    "    # Create a grid of points with x and y and z coordinates\n",
    "    z,y,x = np.indices([height,width,depth])\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0, (z.max()-z.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r= torch.tensor(np.sqrt((x-center[0])**2 + (y-center[1])**2 + (z-center[2])**2))\n",
    "        \n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    return r.detach(),ind.detach()\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image,r,ind):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "#     height, width = image.shape\n",
    "#     # Create a grid of points with x and y coordinates\n",
    "#     y, x = np.indices([height,width])\n",
    "\n",
    "#     if not center:\n",
    "#         center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "#     # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "#     r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "#     # Get sorted radii\n",
    "#     ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    \n",
    "    y1=torch.rfft(arr,signal_ndim=3,onesided=False)\n",
    "    real,imag=f_torch_fftshift(y1[:,:,:,0],y1[:,:,:,1])    ## last index is real/imag part  ## Mod for 3D\n",
    "    \n",
    "#     # For pytorch 1.8\n",
    "#     y1=torch.fft.fftn(arr,dim=(-3,-2,-1))\n",
    "#     real,imag=f_torch_fftshift(y1.real,y1.imag)    \n",
    "    \n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    return z1\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr,r,ind):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i,r,ind) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels,r,ind):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    var=[[] for i in range(num_channels)] \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:,:] # Mod for 3D\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr,r,ind)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "#         var[i]=torch.std(batch_pk,axis=0)/np.sqrt(batch_pk.shape[0])\n",
    "#         var[i]=torch.std(batch_pk,axis=0)\n",
    "        var[i]=torch.var(batch_pk,axis=0)\n",
    "    \n",
    "    mean=torch.stack(mean)\n",
    "    var=torch.stack(var)\n",
    "        \n",
    "    if (torch.isnan(mean).any() or torch.isnan(var).any()):\n",
    "        print(\"Nans in spectrum\",mean,var)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"Nans in Input image\")\n",
    "\n",
    "    return mean,var\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    \n",
    "    try: \n",
    "        hist_data=torch.histc(data,bins=bins)\n",
    "        ## A kind of normalization of histograms: divide by total sum\n",
    "        hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hist_data=torch.zeros(bins)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_var,spec_var_ref,image_size,lambda_spec_mean,lambda_spec_var):\n",
    "    ''' Loss function for the spectrum : mean + variance \n",
    "    Log(sum( batch value - expect value) ^ 2 )) '''\n",
    "    \n",
    "    if (torch.isnan(spec_mean).any() or torch.isnan(spec_var).any()):\n",
    "        ans=torch.tensor(float(\"inf\"))\n",
    "        return ans\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "        \n",
    "    loss_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    loss_var=torch.log(torch.mean(torch.pow(spec_var[:,:idx]-spec_var_ref[:,:idx],2)))\n",
    "    \n",
    "    ans=lambda_spec_mean*loss_mean+lambda_spec_var*loss_var\n",
    "    \n",
    "    if (torch.isnan(ans).any()) :    \n",
    "        print(\"loss spec mean %s, loss spec var %s\"%(loss_mean,loss_var))\n",
    "        print(\"spec mean %s, ref %s\"%(spec_mean, spec_mean_ref))\n",
    "        print(\"spec var %s, ref %s\"%(spec_var, spec_var_ref))\n",
    "#         raise SystemExit\n",
    "        \n",
    "    return ans\n",
    "    \n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return lambda1*torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n",
    "\n",
    "def f_FM_loss(real_output,fake_output,lambda_fm,gdict):\n",
    "    '''\n",
    "    Module to implement Feature-Matching loss. Reads all but last elements of Discriminator ouput\n",
    "    '''\n",
    "    FM=torch.Tensor([0.0]).to(gdict['device'])\n",
    "    for i,j in zip(real_output[:-1],fake_output[:-1]):\n",
    "#         print(i.shape,j.shape)\n",
    "        real_mean=torch.mean(i)\n",
    "        fake_mean=torch.mean(j)\n",
    "#         print(real_mean,fake_mean)\n",
    "        FM=FM.clone()+torch.sum(torch.square(real_mean-fake_mean))\n",
    "    return lambda_fm*FM\n",
    "\n",
    "def f_gp_loss(grads,l=1.0):\n",
    "    '''\n",
    "    Module to implement gradient penalty loss.\n",
    "    '''\n",
    "    loss=torch.mean(torch.sum(torch.square(grads),dim=[1,2,3]))\n",
    "    return l*loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train code ###\n",
    "def f_train_loop(gan_model,Dset,metrics_df,gdict,fixed_noise):\n",
    "    ''' Train epochs '''\n",
    "    ## Define new variables from dict\n",
    "    keys=['image_size','start_epoch','epochs','iters','best_chi1','best_chi2','save_dir','device','flip_prob','nz','batch_size','bns']\n",
    "    image_size,start_epoch,epochs,iters,best_chi1,best_chi2,save_dir,device,flip_prob,nz,batchsize,bns=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        t_epoch_start=time.time()\n",
    "        for count, data in enumerate(Dset.train_dataloader):\n",
    "\n",
    "            ####### Train GAN ########\n",
    "            gan_model.netG.train(); gan_model.netD.train();  ### Need to add these after inference and before training\n",
    "\n",
    "            tme1=time.time()\n",
    "            ### Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            gan_model.netD.zero_grad()\n",
    "\n",
    "            real_cpu = data[0].to(device)\n",
    "            real_cpu.requires_grad=True\n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), 1, device=device,dtype=float)\n",
    "            fake_label = torch.full((b_size,), 0, device=device,dtype=float)\n",
    "            g_label = torch.full((b_size,), 1, device=device,dtype=float) ## No flipping for Generator labels\n",
    "            # Flip labels with probability flip_prob\n",
    "            for idx in np.random.choice(np.arange(b_size),size=int(np.ceil(b_size*flip_prob))):\n",
    "                real_label[idx]=0; fake_label[idx]=1\n",
    "\n",
    "            # Generate fake image batch with G\n",
    "            noise = torch.randn(b_size, 1, 1, 1, nz, device=device) ### Mod for 3D\n",
    "            fake = gan_model.netG(noise)            \n",
    "\n",
    "            # Forward pass real batch through D\n",
    "            real_output = gan_model.netD(real_cpu)\n",
    "            errD_real = gan_model.criterion(real_output[-1].view(-1), real_label.float())\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            D_x = real_output[-1].mean().item()\n",
    "\n",
    "            # Forward pass fake batch through D\n",
    "            fake_output = gan_model.netD(fake.detach())   # The detach is important\n",
    "            errD_fake = gan_model.criterion(fake_output[-1].view(-1), fake_label.float())\n",
    "            errD_fake.backward(retain_graph=True)\n",
    "            D_G_z1 = fake_output[-1].mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake \n",
    "\n",
    "            if gdict['lambda_gp']: ## Add gradient - penalty loss\n",
    "                grads=torch.autograd.grad(outputs=real_output[-1],inputs=real_cpu,grad_outputs=torch.ones_like(real_output[-1]),allow_unused=False,create_graph=True)[0]\n",
    "                gp_loss=f_gp_loss(grads,gdict['lambda_gp'])\n",
    "                gp_loss.backward(retain_graph=True)\n",
    "                errD = errD + gp_loss\n",
    "            else:\n",
    "                gp_loss=torch.Tensor([np.nan])\n",
    "                \n",
    "            if gdict['grad_clip']:\n",
    "                nn.utils.clip_grad_norm_(gan_model.netD.parameters(),gdict['grad_clip'])\n",
    "\n",
    "            gan_model.optimizerD.step()\n",
    "            lr_d=gan_model.optimizerD.param_groups[0]['lr']\n",
    "            gan_model.schedulerD.step()\n",
    "            \n",
    "# dict_keys(['train_data_loader', 'r', 'ind', 'train_spec_mean', 'train_spec_var', 'train_hist', 'val_spec_mean', 'val_spec_var', 'val_hist'])\n",
    "\n",
    "            ###Update G network: maximize log(D(G(z)))\n",
    "            gan_model.netG.zero_grad()\n",
    "            output = gan_model.netD(fake)\n",
    "            errG_adv = gan_model.criterion(output[-1].view(-1), g_label.float())\n",
    "#             errG_adv.backward(retain_graph=True)\n",
    "            # Histogram pixel intensity loss\n",
    "            hist_gen=f_compute_hist(fake,bins=bns)\n",
    "            hist_loss=loss_hist(hist_gen,Dset.train_hist.to(device))\n",
    "\n",
    "            # Add spectral loss\n",
    "            mean,var=f_torch_image_spectrum(f_invtransform(fake),1,Dset.r.to(device),Dset.ind.to(device))\n",
    "            spec_loss=loss_spectrum(mean,Dset.train_spec_mean.to(device),var,Dset.train_spec_var.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])\n",
    "\n",
    "            errG=errG_adv\n",
    "            if gdict['lambda_spec_mean']: \n",
    "#                 spec_loss.backward(retain_graph=True)\n",
    "                errG = errG+ spec_loss \n",
    "            if gdict['lambda_fm']:## Add feature matching loss\n",
    "                fm_loss=f_FM_loss([i.detach() for i in real_output],output,gdict['lambda_fm'],gdict)\n",
    "#                 fm_loss.backward(retain_graph=True)\n",
    "                errG= errG+ fm_loss\n",
    "            else: \n",
    "                fm_loss=torch.Tensor([np.nan])\n",
    "\n",
    "            if torch.isnan(errG).any():\n",
    "                logging.info(errG)\n",
    "                raise SystemError\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output[-1].mean().item()\n",
    "            \n",
    "            ### Implement Gradient clipping\n",
    "            if gdict['grad_clip']:\n",
    "                nn.utils.clip_grad_norm_(gan_model.netG.parameters(),gdict['grad_clip'])\n",
    "            \n",
    "            gan_model.optimizerG.step()\n",
    "            lr_g=gan_model.optimizerG.param_groups[0]['lr']\n",
    "            gan_model.schedulerG.step()\n",
    "            \n",
    "            tme2=time.time()\n",
    "            ####### Store metrics ########\n",
    "            # Output training stats\n",
    "            if gdict['world_rank']==0:\n",
    "                if ((count % gdict['checkpoint_size'] == 0)):\n",
    "                    logging.info('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_adv: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                          % (epoch, epochs, count, len(Dset.train_dataloader), errD.item(), errG_adv.item(),errG.item(), D_x, D_G_z1, D_G_z2)),\n",
    "                    logging.info(\"Spec loss: %s,\\t hist loss: %s\"%(spec_loss.item(),hist_loss.item())),\n",
    "                    logging.info(\"Training time for step %s : %s\"%(iters, tme2-tme1))\n",
    "\n",
    "                # Save metrics\n",
    "                cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','fm_loss','gp_loss','D(x)','D_G_z1','D_G_z2','lr_d','lr_g','time']\n",
    "                vals=[iters,epoch,errD_real.item(),errD_fake.item(),errD.item(),errG_adv.item(),errG.item(),spec_loss.item(),hist_loss.item(),fm_loss.item(),gp_loss.item(),D_x,D_G_z1,D_G_z2,lr_d,lr_g,tme2-tme1]\n",
    "                for col,val in zip(cols,vals):  metrics_df.loc[iters,col]=val\n",
    "\n",
    "                ### Checkpoint the best model\n",
    "                checkpoint=True\n",
    "                iters += 1  ### Model has been updated, so update iters before saving metrics and model.\n",
    "\n",
    "                ### Compute validation metrics for updated model\n",
    "                gan_model.netG.eval()\n",
    "                with torch.no_grad():\n",
    "                    fake = gan_model.netG(fixed_noise)\n",
    "                    hist_gen=f_compute_hist(fake,bins=bns)\n",
    "                    hist_chi=loss_hist(hist_gen,Dset.val_hist.to(device))\n",
    "                    mean,var=f_torch_image_spectrum(f_invtransform(fake),1,Dset.r.to(device),Dset.ind.to(device))\n",
    "                    spec_chi=loss_spectrum(mean,Dset.val_spec_mean.to(device),var,Dset.val_spec_var.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])\n",
    "\n",
    "                # Storing chi for next step\n",
    "                for col,val in zip(['spec_chi','hist_chi'],[spec_chi.item(),hist_chi.item()]):  metrics_df.loc[iters,col]=val            \n",
    "\n",
    "                # Checkpoint model for continuing run\n",
    "                if count == len(Dset.train_dataloader)-1: ## Check point at last step of epoch\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_last.tar')  \n",
    "\n",
    "                if (checkpoint and (epoch > 1)): # Choose best models by metric\n",
    "                    if hist_chi< best_chi1:\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_best_hist.tar')\n",
    "                        best_chi1=hist_chi.item()\n",
    "                        logging.info(\"Saving best hist model at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                    if  spec_chi< best_chi2:\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_best_spec.tar')\n",
    "                        best_chi2=spec_chi.item()\n",
    "                        logging.info(\"Saving best spec model at epoch %s, step %s\"%(epoch,iters))\n",
    "\n",
    "#                    if (iters in gdict['save_steps_list']) :\n",
    "                    if ((gdict['save_steps_list']=='all') and (iters % gdict['checkpoint_size'] == 0)):\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_{0}.tar'.format(iters))\n",
    "                        logging.info(\"Saving given-step at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                # Save G's output on fixed_noise\n",
    "                if ((iters % gdict['checkpoint_size'] == 0) or ((epoch == epochs-1) and (count == len(Dset.train_dataloader)-1))):\n",
    "                    gan_model.netG.eval()\n",
    "                    with torch.no_grad():\n",
    "                        fake = gan_model.netG(fixed_noise).detach().cpu()\n",
    "                        img_arr=np.array(fake)\n",
    "                        fname='gen_img_epoch-%s_step-%s'%(epoch,iters)\n",
    "                        np.save(save_dir+'/images/'+fname,img_arr)\n",
    "        \n",
    "        t_epoch_end=time.time()\n",
    "        if gdict['world_rank']==0:\n",
    "            logging.info(\"Time taken for epoch %s, count %s: %s for rank %s\"%(epoch,count,t_epoch_end-t_epoch_start,gdict['world_rank']))\n",
    "            # Save Metrics to file after each epoch\n",
    "            metrics_df.to_pickle(save_dir+'/df_metrics.pkle')\n",
    "            logging.info(\"best chis: {0}, {1}\".format(best_chi1,best_chi2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Setup modules ###\n",
    "def f_manual_add_argparse():\n",
    "    ''' use only in jpt notebook'''\n",
    "    args=argparse.Namespace()\n",
    "    args.config='config_3d_Cgan.yaml'\n",
    "    args.mode='fresh'\n",
    "    args.local_rank=0\n",
    "    args.facility='cori'\n",
    "    args.distributed=False\n",
    "\n",
    "#     args.mode='continue'\n",
    "    \n",
    "    return args\n",
    "\n",
    "def f_parse_args():\n",
    "    \"\"\"Parse command line arguments.Only for .py file\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Run script to train GAN using pytorch\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    add_arg = parser.add_argument\n",
    "    \n",
    "    add_arg('--config','-cfile',  type=str, default='config_3d_Cgan.yaml', help='Name of config file')\n",
    "    add_arg('--mode','-m',  type=str, choices=['fresh','continue','fresh_load'],default='fresh', help='Whether to start fresh run or continue previous run or fresh run loading a config file.')\n",
    "    add_arg(\"--local_rank\", default=0, type=int,help='Local rank of GPU on node. Using for pytorch DDP. ')\n",
    "    add_arg(\"--facility\", default='cori', choices=['cori','summit'],type=str,help='Facility: cori or summit ')\n",
    "    add_arg(\"--ddp\", dest='distributed' ,default=False,action='store_true',help='use Distributed DataParallel for Pytorch or DataParallel')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def try_barrier(rank):\n",
    "    \"\"\"\n",
    "    Used in Distributed data parallel\n",
    "    Attempt a barrier but ignore any exceptions\n",
    "    \"\"\"\n",
    "    print('BAR %d'%rank)\n",
    "    try:\n",
    "        dist.barrier()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def f_init_gdict(args,gdict):\n",
    "    ''' Create global dictionary gdict from args and config file'''\n",
    "    \n",
    "    ## read config file\n",
    "    config_file=args.config\n",
    "    with open(config_file) as f:\n",
    "        config_dict= yaml.load(f, Loader=yaml.SafeLoader)\n",
    "        \n",
    "    gdict=config_dict['parameters']\n",
    "\n",
    "    args_dict=vars(args)\n",
    "    ## Add args variables to gdict\n",
    "    for key in args_dict.keys():\n",
    "        gdict[key]=args_dict[key]\n",
    "\n",
    "    if gdict['distributed']: \n",
    "        assert not gdict['lambda_gp'],\"GP couplings is %s. Cannot use Gradient penalty loss in pytorch DDP\"%(gdict['lambda_gp'])\n",
    "    else : print(\"Not using DDP\")\n",
    "    return gdict\n",
    "\n",
    "\n",
    "def f_get_img_samples(ip_arr,rank=0,num_ranks=1):\n",
    "    '''\n",
    "    Module to get part of the numpy image file\n",
    "    '''\n",
    "    \n",
    "    data_size=ip_arr.shape[0]\n",
    "    size=data_size//num_ranks\n",
    "    \n",
    "    if gdict['batch_size']>size:\n",
    "        print(\"Caution: batchsize %s is greater than samples per GPU %s\"%(gdict['batch_size'],size))\n",
    "        raise SystemExit\n",
    "        \n",
    "    ### Get a set of random indices from numpy array\n",
    "    random=False\n",
    "    if random:\n",
    "        idxs=np.arange(ip_arr.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        rnd_idxs=idxs[rank*(size):(rank+1)*size]\n",
    "        arr=ip_arr[rnd_idxs].copy()\n",
    "        \n",
    "    else: arr=ip_arr[rank*(size):(rank+1)*size].copy()\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def f_setup(gdict,metrics_df,log):\n",
    "    ''' \n",
    "    Set up directories, Initialize random seeds, add GPU info, add logging info.\n",
    "    '''\n",
    "    \n",
    "    torch.backends.cudnn.benchmark=True\n",
    "#     torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    ## New additions. Code taken from Jan B.\n",
    "    os.environ['MASTER_PORT'] = \"8885\"\n",
    "\n",
    "    if gdict['facility']=='summit':\n",
    "        get_master = \"echo $(cat {} | sort | uniq | grep -v batch | grep -v login | head -1)\".format(os.environ['LSB_DJOB_HOSTFILE'])\n",
    "        os.environ['MASTER_ADDR'] = str(subprocess.check_output(get_master, shell=True))[2:-3]\n",
    "        os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\n",
    "        os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']\n",
    "        gdict['local_rank'] = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "    else:\n",
    "        if gdict['distributed']:\n",
    "            os.environ['WORLD_SIZE'] = os.environ['SLURM_NTASKS']\n",
    "            os.environ['RANK'] = os.environ['SLURM_PROCID']\n",
    "            gdict['local_rank'] = int(os.environ['SLURM_LOCALID'])\n",
    "\n",
    "    ## Special declarations\n",
    "    gdict['ngpu']=torch.cuda.device_count()\n",
    "    gdict['device']=torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    gdict['multi-gpu']=True if (gdict['device'].type == 'cuda') and (gdict['ngpu'] > 1) else False \n",
    "    \n",
    "    ########################\n",
    "    ###### Set up Distributed Data parallel ######\n",
    "    if gdict['distributed']:\n",
    "#         gdict['local_rank']=args.local_rank  ## This is needed when using pytorch -m torch.distributed.launch\n",
    "        gdict['world_size']=int(os.environ['WORLD_SIZE'])\n",
    "        torch.cuda.set_device(gdict['local_rank']) ## Very important\n",
    "        dist.init_process_group(backend='nccl', init_method=\"env://\")  \n",
    "        gdict['world_rank']= dist.get_rank()\n",
    "        \n",
    "        device = torch.cuda.current_device()\n",
    "        logging.info(\"World size %s, world rank %s, local rank %s device %s, hostname %s, GPUs on node %s\\n\"%(gdict['world_size'],gdict['world_rank'],gdict['local_rank'],device,socket.gethostname(),gdict['ngpu']))\n",
    "        \n",
    "        # Divide batch size by number of GPUs\n",
    "#         gdict['batch_size']=gdict['batch_size']//gdict['world_size']\n",
    "    else:\n",
    "        gdict['world_size'],gdict['world_rank'],gdict['local_rank']=1,0,0\n",
    "    \n",
    "    ########################\n",
    "    ###### Set up directories #######\n",
    "    ### sync up so that time is the same for each GPU for DDP\n",
    "    if gdict['mode'] in ['fresh','fresh_load']:\n",
    "        ### Create prefix for foldername      \n",
    "        if gdict['world_rank']==0: ### For rank=0, create directory name string and make directories\n",
    "            dt_strg=datetime.now().strftime('%Y%m%d_%H%M%S') ## time format\n",
    "            dt_lst=[int(i) for i in dt_strg.split('_')] # List storing day and time            \n",
    "            dt_tnsr=torch.LongTensor(dt_lst).to(gdict['device'])  ## Create list to pass to other GPUs \n",
    "\n",
    "        else: dt_tnsr=torch.Tensor([0,0]).long().to(gdict['device'])\n",
    "        ### Pass directory name to other ranks\n",
    "        if gdict['distributed']: dist.broadcast(dt_tnsr, src=0)\n",
    "\n",
    "        gdict['save_dir']=gdict['op_loc']+str(int(dt_tnsr[0]))+'_'+str(int(dt_tnsr[1]))+'_'+gdict['run_suffix']\n",
    "        \n",
    "        if gdict['world_rank']==0: # Create directories for rank 0\n",
    "            ### Create directories\n",
    "            if not os.path.exists(gdict['save_dir']):\n",
    "                os.makedirs(gdict['save_dir']+'/models')\n",
    "                os.makedirs(gdict['save_dir']+'/images')\n",
    "                shutil.copy(gdict['config'],gdict['save_dir'])    \n",
    "    \n",
    "    elif gdict['mode']=='continue': ## For checkpointed runs\n",
    "        gdict['save_dir']=gdict['ip_fldr']\n",
    "        ### Read loss data\n",
    "        metrics_df=pd.read_pickle(gdict['save_dir']+'/df_metrics.pkle').astype(np.float64)\n",
    "   \n",
    "    ########################\n",
    "    ### Initialize random seed\n",
    "    \n",
    "    manualSeed = np.random.randint(1, 10000) if gdict['seed']=='random' else int(gdict['seed'])\n",
    "#     print(\"Seed\",manualSeed,gdict['world_rank'])\n",
    "    random.seed(manualSeed)\n",
    "    np.random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    \n",
    "    if gdict['deterministic']:\n",
    "        logging.info(\"Running with deterministic sequence. Performance will be slower\")\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "#         torch.backends.cudnn.enabled = False\n",
    "        torch.backends.cudnn.benchmark = False        \n",
    "    \n",
    "    ########################\n",
    "    if log:\n",
    "        ### Write all logging.info statements to stdout and log file\n",
    "        logfile=gdict['save_dir']+'/log.log'\n",
    "        if gdict['world_rank']==0:\n",
    "            logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "\n",
    "            Lg = logging.getLogger()\n",
    "            Lg.setLevel(logging.DEBUG)\n",
    "            lg_handler_file = logging.FileHandler(logfile)\n",
    "            lg_handler_stdout = logging.StreamHandler(sys.stdout)\n",
    "            Lg.addHandler(lg_handler_file)\n",
    "            Lg.addHandler(lg_handler_stdout)\n",
    "\n",
    "            logging.info('Args: {0}'.format(args))\n",
    "            logging.info('Start: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "        \n",
    "        if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "        if gdict['world_rank']!=0:\n",
    "                logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,gdict):\n",
    "        '''\n",
    "        Load training dataset and compute spectrum and histogram for a small batch of training and validation dataset.\n",
    "        '''\n",
    "        ## Load training dataset\n",
    "        t0a=time.time()\n",
    "        img=np.load(gdict['ip_fname'],mmap_mode='r')[:gdict['num_imgs']]\n",
    "    #     print(\"Shape of input file\",img.shape)\n",
    "        img=f_get_img_samples(img,gdict['world_rank'],gdict['world_size'])  \n",
    "\n",
    "        t_img=torch.from_numpy(img)\n",
    "        dataset=TensorDataset(t_img)\n",
    "        self.train_dataloader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)\n",
    "        logging.info(\"Size of dataset for GPU %s : %s\"%(gdict['world_rank'],len(self.train_dataloader.dataset)))\n",
    "\n",
    "        t0b=time.time()\n",
    "        logging.info(\"Time for creating dataloader\",t0b-t0a,gdict['world_rank'])\n",
    "        \n",
    "        # Precompute spectrum and histogram for small training and validation data for computing losses\n",
    "        with torch.no_grad():\n",
    "            val_img=np.load(gdict['ip_fname'],mmap_mode='r')[-100:].copy()\n",
    "            t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "            # Precompute radial coordinates\n",
    "            r,ind=f_get_rad(val_img)\n",
    "            self.r,self.ind=r.to(gdict['device']),ind.to(gdict['device'])\n",
    "\n",
    "            # Compute\n",
    "            self.train_spec_mean,self.train_spec_var=f_torch_image_spectrum(f_invtransform(t_val_img),1,self.r,self.ind)\n",
    "            self.train_hist=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "            \n",
    "            # Repeat for validation dataset\n",
    "            val_img=np.load(gdict['ip_fname'],mmap_mode='r')[-200:-100].copy()\n",
    "            t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "            \n",
    "            # Compute\n",
    "            self.val_spec_mean,self.val_spec_var=f_torch_image_spectrum(f_invtransform(t_val_img),1,self.r,self.ind)\n",
    "            self.val_hist=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "            del val_img; del t_val_img; del img; del t_img;\n",
    "\n",
    "class GAN_model():\n",
    "    def __init__(self,gdict,print_model=False):\n",
    "    \n",
    "        def weights_init(m):\n",
    "            '''custom weights initialization called on netG and netD '''\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('Conv') != -1:\n",
    "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            elif classname.find('BatchNorm') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "        # Create Generator\n",
    "        self.netG = Generator(gdict).to(gdict['device'])\n",
    "        self.netG.apply(weights_init)\n",
    "        # Create Discriminator\n",
    "        self.netD = Discriminator(gdict).to(gdict['device'])\n",
    "        self.netD.apply(weights_init)\n",
    "\n",
    "        if print_model:\n",
    "            if gdict['world_rank']==0:\n",
    "                print(self.netG)\n",
    "            #     summary(netG,(1,1,64))\n",
    "                print(self.netD)\n",
    "            #     summary(netD,(1,128,128))\n",
    "                print(\"Number of GPUs used %s\"%(gdict['ngpu']))\n",
    "\n",
    "        if (gdict['multi-gpu']):\n",
    "            if not gdict['distributed']:\n",
    "                self.netG = nn.DataParallel(self.netG, list(range(gdict['ngpu'])))\n",
    "                self.netD = nn.DataParallel(self.netD, list(range(gdict['ngpu'])))\n",
    "            else:\n",
    "                self.netG=DistributedDataParallel(self.netG,device_ids=[gdict['local_rank']],output_device=[gdict['local_rank']])\n",
    "                self.netD=DistributedDataParallel(self.netD,device_ids=[gdict['local_rank']],output_device=[gdict['local_rank']])\n",
    "\n",
    "        #### Initialize networks ####\n",
    "        # self.criterion = nn.BCELoss()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=gdict['learn_rate_d'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=gdict['learn_rate_g'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        \n",
    "        if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "        if gdict['mode']=='fresh':\n",
    "            iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10 \n",
    "            \n",
    "        elif gdict['mode']=='continue':\n",
    "            iters,start_epoch,best_chi1,best_chi2,self.netD,self.optimizerD,self.netG,self.optimizerG=f_load_checkpoint(gdict['save_dir']+'/models/checkpoint_last.tar',\\\n",
    "                                                                                                                        self.netG,self.netD,self.optimizerG,self.optimizerD,gdict) \n",
    "            if gdict['world_rank']==0: logging.info(\"\\nContinuing existing run. Loading checkpoint with epoch {0} and step {1}\\n\".format(start_epoch,iters))\n",
    "            if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "            start_epoch+=1  ## Start with the next epoch \n",
    "        \n",
    "        elif gdict['mode']=='fresh_load':\n",
    "            iters,start_epoch,best_chi1,best_chi2,self.netD,self.optimizerD,self.netG,self.optimizerG=f_load_checkpoint(gdict['chkpt_file'],\\\n",
    "                                                                                                                        self.netG,self.netD,self.optimizerG,self.optimizerD,gdict) \n",
    "            if gdict['world_rank']==0: logging.info(\"Fresh run loading checkpoint file {0}\".format(gdict['chkpt_file']))\n",
    "#             if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "            iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10 \n",
    "        \n",
    "        ## Add to gdict\n",
    "        for key,val in zip(['best_chi1','best_chi2','iters','start_epoch'],[best_chi1,best_chi2,iters,start_epoch]): gdict[key]=val\n",
    "        \n",
    "        ## Set up learn rate scheduler\n",
    "        lr_stepsize=int((gdict['num_imgs']*len(gdict['sigma_list']))/(gdict['batch_size']*gdict['world_size'])) # convert epoch number to step \n",
    "        lr_d_epochs=[i*lr_stepsize for i in gdict['lr_d_epochs']] \n",
    "        lr_g_epochs=[i*lr_stepsize for i in gdict['lr_g_epochs']]\n",
    "        self.schedulerD = optim.lr_scheduler.MultiStepLR(self.optimizerD, milestones=lr_d_epochs,gamma=gdict['lr_d_gamma'])\n",
    "        self.schedulerG = optim.lr_scheduler.MultiStepLR(self.optimizerG, milestones=lr_g_epochs,gamma=gdict['lr_g_gamma'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using DDP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-4e1b76dbd1f0>:79: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  y1=torch.rfft(arr,signal_ndim=3,onesided=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 64, 64, 64)\n",
      "Image saved in  best_spec_epoch-3_step-31.npy\n",
      "(100, 1, 64, 64, 64)\n",
      "Image saved in  best_hist_epoch-4_step-50.npy\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### Main code #######\n",
    "#########################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    jpt=False\n",
    "    jpt=True ##(different for jupyter notebook)\n",
    "    t0=time.time()\n",
    "    t0=time.time()\n",
    "    args=f_parse_args() if not jpt else f_manual_add_argparse()\n",
    "\n",
    "    #################################\n",
    "    ### Set up global dictionary###\n",
    "    gdict={}\n",
    "    gdict=f_init_gdict(args,gdict)\n",
    "#     gdict['num_imgs']=200\n",
    "\n",
    "    if jpt: ## override for jpt nbks\n",
    "        gdict['num_imgs']=400\n",
    "        gdict['run_suffix']='nb_test'\n",
    "        \n",
    "    ### Set up metrics dataframe\n",
    "    cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','spec_chi','hist_chi','gp_loss','fm_loss','D(x)','D_G_z1','D_G_z2','time']\n",
    "    metrics_df=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    # Setup\n",
    "    metrics_df=f_setup(gdict,metrics_df,log=(not jpt))\n",
    "    \n",
    "    ## Build GAN\n",
    "    gan_model=GAN_model(gdict,False)\n",
    "    fixed_noise = torch.randn(gdict['op_size'], 1, 1, 1, gdict['nz'], device=gdict['device']) #Latent vectors to view G progress    # Mod for 3D\n",
    "\n",
    "    if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "    ## Load data and precompute\n",
    "    Dset=Dataset(gdict)\n",
    "    \n",
    "    #################################\n",
    "    ########## Train loop and save metrics and images ######\n",
    "    if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "    if gdict['world_rank']==0: \n",
    "        logging.info(gdict)\n",
    "        logging.info(\"Starting Training Loop...\")\n",
    "        \n",
    "    f_train_loop(gan_model,Dset,metrics_df,gdict,fixed_noise)\n",
    "    \n",
    "    if gdict['world_rank']==0: ## Generate images for best saved models ######\n",
    "        op_loc=gdict['save_dir']+'/images/'\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_spec.tar'\n",
    "        f_gen_images(gdict,gan_model.netG,gan_model.optimizerG,ip_fname,op_loc,op_strg='best_spec',op_size=32)\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_hist.tar'\n",
    "        f_gen_images(gdict,gan_model.netG,gan_model.optimizerG,ip_fname,op_loc,op_strg='best_hist',op_size=32)\n",
    "    \n",
    "    tf=time.time()\n",
    "    logging.info(\"Total time %s\"%(tf-t0))\n",
    "    logging.info('End: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>epoch</th>\n",
       "      <th>Dreal</th>\n",
       "      <th>Dfake</th>\n",
       "      <th>Dfull</th>\n",
       "      <th>G_adv</th>\n",
       "      <th>G_full</th>\n",
       "      <th>spec_loss</th>\n",
       "      <th>hist_loss</th>\n",
       "      <th>spec_chi</th>\n",
       "      <th>hist_chi</th>\n",
       "      <th>gp_loss</th>\n",
       "      <th>fm_loss</th>\n",
       "      <th>D(x)</th>\n",
       "      <th>D_G_z1</th>\n",
       "      <th>D_G_z2</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.841915</td>\n",
       "      <td>0.715251</td>\n",
       "      <td>1.55717</td>\n",
       "      <td>3.43627</td>\n",
       "      <td>14.6913</td>\n",
       "      <td>11.2551</td>\n",
       "      <td>1.61228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0696436</td>\n",
       "      <td>-0.208424</td>\n",
       "      <td>-2.60994</td>\n",
       "      <td>0.804343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.528712</td>\n",
       "      <td>0.650194</td>\n",
       "      <td>1.17891</td>\n",
       "      <td>3.36176</td>\n",
       "      <td>14.4913</td>\n",
       "      <td>11.1296</td>\n",
       "      <td>1.61329</td>\n",
       "      <td>11.2589</td>\n",
       "      <td>1.74137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406427</td>\n",
       "      <td>-0.0443879</td>\n",
       "      <td>-2.78831</td>\n",
       "      <td>0.0998726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504869</td>\n",
       "      <td>0.585164</td>\n",
       "      <td>1.09003</td>\n",
       "      <td>3.40135</td>\n",
       "      <td>14.5729</td>\n",
       "      <td>11.1716</td>\n",
       "      <td>1.61501</td>\n",
       "      <td>11.2135</td>\n",
       "      <td>1.63144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.887204</td>\n",
       "      <td>0.0438551</td>\n",
       "      <td>-3.01401</td>\n",
       "      <td>0.101523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.560522</td>\n",
       "      <td>0.582506</td>\n",
       "      <td>1.14303</td>\n",
       "      <td>2.90748</td>\n",
       "      <td>14.0768</td>\n",
       "      <td>11.1694</td>\n",
       "      <td>1.61509</td>\n",
       "      <td>11.221</td>\n",
       "      <td>1.64355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997698</td>\n",
       "      <td>-0.707787</td>\n",
       "      <td>-2.30605</td>\n",
       "      <td>0.10299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.782728</td>\n",
       "      <td>0.531758</td>\n",
       "      <td>1.31449</td>\n",
       "      <td>2.47862</td>\n",
       "      <td>13.6299</td>\n",
       "      <td>11.1512</td>\n",
       "      <td>1.61201</td>\n",
       "      <td>11.1401</td>\n",
       "      <td>1.60881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0105866</td>\n",
       "      <td>-0.54982</td>\n",
       "      <td>-1.92131</td>\n",
       "      <td>0.101465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589947</td>\n",
       "      <td>0.804335</td>\n",
       "      <td>1.39428</td>\n",
       "      <td>3.5133</td>\n",
       "      <td>14.6216</td>\n",
       "      <td>11.1083</td>\n",
       "      <td>1.61322</td>\n",
       "      <td>11.1136</td>\n",
       "      <td>1.6276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3728</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>-3.29496</td>\n",
       "      <td>0.100096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.489124</td>\n",
       "      <td>0.485051</td>\n",
       "      <td>0.974174</td>\n",
       "      <td>2.63965</td>\n",
       "      <td>13.5502</td>\n",
       "      <td>10.9105</td>\n",
       "      <td>1.61516</td>\n",
       "      <td>10.9624</td>\n",
       "      <td>1.62754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>-0.918754</td>\n",
       "      <td>-2.14345</td>\n",
       "      <td>0.0999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495638</td>\n",
       "      <td>0.441089</td>\n",
       "      <td>0.936727</td>\n",
       "      <td>2.5943</td>\n",
       "      <td>13.7844</td>\n",
       "      <td>11.1901</td>\n",
       "      <td>1.61573</td>\n",
       "      <td>12.0794</td>\n",
       "      <td>1.62015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.929048</td>\n",
       "      <td>-0.936146</td>\n",
       "      <td>-2.0317</td>\n",
       "      <td>0.100204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.58862</td>\n",
       "      <td>0.817294</td>\n",
       "      <td>1.40591</td>\n",
       "      <td>3.66594</td>\n",
       "      <td>15.4566</td>\n",
       "      <td>11.7906</td>\n",
       "      <td>1.61728</td>\n",
       "      <td>12.5057</td>\n",
       "      <td>1.60816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.843806</td>\n",
       "      <td>0.300981</td>\n",
       "      <td>-3.44808</td>\n",
       "      <td>0.0998306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583421</td>\n",
       "      <td>0.443322</td>\n",
       "      <td>1.02674</td>\n",
       "      <td>2.62075</td>\n",
       "      <td>13.6924</td>\n",
       "      <td>11.0717</td>\n",
       "      <td>1.61571</td>\n",
       "      <td>11.467</td>\n",
       "      <td>1.60569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.758334</td>\n",
       "      <td>-1.23671</td>\n",
       "      <td>-2.09881</td>\n",
       "      <td>0.100337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20487</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>3.99325</td>\n",
       "      <td>2.60432</td>\n",
       "      <td>13.5959</td>\n",
       "      <td>10.9916</td>\n",
       "      <td>1.61131</td>\n",
       "      <td>11.0968</td>\n",
       "      <td>1.60122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81404</td>\n",
       "      <td>-0.461801</td>\n",
       "      <td>-2.13162</td>\n",
       "      <td>0.0984762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3.69734</td>\n",
       "      <td>0.531606</td>\n",
       "      <td>4.22895</td>\n",
       "      <td>2.92347</td>\n",
       "      <td>14.1069</td>\n",
       "      <td>11.1835</td>\n",
       "      <td>1.59483</td>\n",
       "      <td>11.1989</td>\n",
       "      <td>1.56076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.21364</td>\n",
       "      <td>-0.0348395</td>\n",
       "      <td>-2.6723</td>\n",
       "      <td>0.098561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.26956</td>\n",
       "      <td>0.459759</td>\n",
       "      <td>1.72932</td>\n",
       "      <td>3.82207</td>\n",
       "      <td>15.024</td>\n",
       "      <td>11.2019</td>\n",
       "      <td>1.57898</td>\n",
       "      <td>11.2503</td>\n",
       "      <td>1.57648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.74056</td>\n",
       "      <td>-0.493722</td>\n",
       "      <td>-3.67086</td>\n",
       "      <td>0.0996554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.99144</td>\n",
       "      <td>0.625213</td>\n",
       "      <td>2.61666</td>\n",
       "      <td>1.55021</td>\n",
       "      <td>12.7526</td>\n",
       "      <td>11.2024</td>\n",
       "      <td>1.56349</td>\n",
       "      <td>11.2579</td>\n",
       "      <td>1.56873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.04333</td>\n",
       "      <td>-1.87929</td>\n",
       "      <td>-0.515411</td>\n",
       "      <td>0.0998304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2.14958</td>\n",
       "      <td>1.09285</td>\n",
       "      <td>3.24243</td>\n",
       "      <td>3.77691</td>\n",
       "      <td>14.9852</td>\n",
       "      <td>11.2083</td>\n",
       "      <td>1.5457</td>\n",
       "      <td>11.2501</td>\n",
       "      <td>1.5211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.77106</td>\n",
       "      <td>0.489263</td>\n",
       "      <td>-3.67769</td>\n",
       "      <td>0.100532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.69497</td>\n",
       "      <td>0.53934</td>\n",
       "      <td>2.23431</td>\n",
       "      <td>4.07972</td>\n",
       "      <td>15.2881</td>\n",
       "      <td>11.2083</td>\n",
       "      <td>1.52685</td>\n",
       "      <td>11.2625</td>\n",
       "      <td>1.55508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.8329</td>\n",
       "      <td>-0.703641</td>\n",
       "      <td>-3.97985</td>\n",
       "      <td>0.100154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.04819</td>\n",
       "      <td>0.702449</td>\n",
       "      <td>4.75064</td>\n",
       "      <td>1.50705</td>\n",
       "      <td>12.7161</td>\n",
       "      <td>11.2091</td>\n",
       "      <td>1.51086</td>\n",
       "      <td>11.251</td>\n",
       "      <td>1.48602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.66749</td>\n",
       "      <td>-1.88185</td>\n",
       "      <td>-0.798562</td>\n",
       "      <td>0.100773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4.30621</td>\n",
       "      <td>0.903435</td>\n",
       "      <td>5.20965</td>\n",
       "      <td>2.29545</td>\n",
       "      <td>13.5015</td>\n",
       "      <td>11.206</td>\n",
       "      <td>1.49202</td>\n",
       "      <td>11.2523</td>\n",
       "      <td>1.46738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.84459</td>\n",
       "      <td>0.28773</td>\n",
       "      <td>-2.00049</td>\n",
       "      <td>0.099766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.209</td>\n",
       "      <td>0.807181</td>\n",
       "      <td>2.01618</td>\n",
       "      <td>4.72973</td>\n",
       "      <td>15.9398</td>\n",
       "      <td>11.2101</td>\n",
       "      <td>1.46969</td>\n",
       "      <td>11.2517</td>\n",
       "      <td>1.43762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.82841</td>\n",
       "      <td>0.0552055</td>\n",
       "      <td>-4.6996</td>\n",
       "      <td>0.100549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698346</td>\n",
       "      <td>0.593591</td>\n",
       "      <td>1.29194</td>\n",
       "      <td>3.06198</td>\n",
       "      <td>14.2679</td>\n",
       "      <td>11.2059</td>\n",
       "      <td>1.44643</td>\n",
       "      <td>11.2563</td>\n",
       "      <td>1.41283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0463</td>\n",
       "      <td>-1.73429</td>\n",
       "      <td>-2.89667</td>\n",
       "      <td>0.100495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.869765</td>\n",
       "      <td>0.606929</td>\n",
       "      <td>1.47669</td>\n",
       "      <td>1.71623</td>\n",
       "      <td>12.9262</td>\n",
       "      <td>11.21</td>\n",
       "      <td>1.4215</td>\n",
       "      <td>11.2633</td>\n",
       "      <td>1.43628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.79404</td>\n",
       "      <td>-1.70906</td>\n",
       "      <td>-1.25762</td>\n",
       "      <td>0.104186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153738</td>\n",
       "      <td>0.724801</td>\n",
       "      <td>0.878539</td>\n",
       "      <td>2.72737</td>\n",
       "      <td>13.9393</td>\n",
       "      <td>11.2119</td>\n",
       "      <td>1.39965</td>\n",
       "      <td>11.263</td>\n",
       "      <td>1.41128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.558859</td>\n",
       "      <td>-0.0412358</td>\n",
       "      <td>-2.504</td>\n",
       "      <td>0.109031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.592364</td>\n",
       "      <td>0.774222</td>\n",
       "      <td>1.36659</td>\n",
       "      <td>3.3853</td>\n",
       "      <td>14.5956</td>\n",
       "      <td>11.2103</td>\n",
       "      <td>1.38029</td>\n",
       "      <td>11.2637</td>\n",
       "      <td>1.39796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.275968</td>\n",
       "      <td>0.0990108</td>\n",
       "      <td>-3.30767</td>\n",
       "      <td>0.101891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1.80235</td>\n",
       "      <td>0.729755</td>\n",
       "      <td>2.53211</td>\n",
       "      <td>1.19739</td>\n",
       "      <td>12.4094</td>\n",
       "      <td>11.212</td>\n",
       "      <td>1.36664</td>\n",
       "      <td>11.2664</td>\n",
       "      <td>1.45473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.54237</td>\n",
       "      <td>-1.29815</td>\n",
       "      <td>-0.597376</td>\n",
       "      <td>0.100012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1.47765</td>\n",
       "      <td>1.13905</td>\n",
       "      <td>2.6167</td>\n",
       "      <td>2.18511</td>\n",
       "      <td>13.3967</td>\n",
       "      <td>11.2115</td>\n",
       "      <td>1.34835</td>\n",
       "      <td>11.2659</td>\n",
       "      <td>1.40785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.91149</td>\n",
       "      <td>0.246781</td>\n",
       "      <td>-1.94268</td>\n",
       "      <td>0.100083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1.13772</td>\n",
       "      <td>0.871386</td>\n",
       "      <td>2.00911</td>\n",
       "      <td>5.52023</td>\n",
       "      <td>16.7312</td>\n",
       "      <td>11.211</td>\n",
       "      <td>1.32286</td>\n",
       "      <td>11.2648</td>\n",
       "      <td>1.37665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.18269</td>\n",
       "      <td>0.60957</td>\n",
       "      <td>-5.51062</td>\n",
       "      <td>0.100914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1.33803</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>2.11469</td>\n",
       "      <td>2.90884</td>\n",
       "      <td>14.1167</td>\n",
       "      <td>11.2079</td>\n",
       "      <td>1.29488</td>\n",
       "      <td>11.2619</td>\n",
       "      <td>1.33919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.531</td>\n",
       "      <td>-2.83703</td>\n",
       "      <td>-2.76262</td>\n",
       "      <td>0.100271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2.63493</td>\n",
       "      <td>0.553062</td>\n",
       "      <td>3.188</td>\n",
       "      <td>0.833248</td>\n",
       "      <td>12.0368</td>\n",
       "      <td>11.2035</td>\n",
       "      <td>1.25681</td>\n",
       "      <td>11.2573</td>\n",
       "      <td>1.31078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.543228</td>\n",
       "      <td>-1.27251</td>\n",
       "      <td>-0.00142056</td>\n",
       "      <td>0.0993879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2.61084</td>\n",
       "      <td>1.19425</td>\n",
       "      <td>3.80509</td>\n",
       "      <td>3.31901</td>\n",
       "      <td>14.5182</td>\n",
       "      <td>11.1991</td>\n",
       "      <td>1.22366</td>\n",
       "      <td>11.2533</td>\n",
       "      <td>1.22023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.154936</td>\n",
       "      <td>1.51298</td>\n",
       "      <td>-3.2434</td>\n",
       "      <td>0.1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.921814</td>\n",
       "      <td>0.419062</td>\n",
       "      <td>1.34088</td>\n",
       "      <td>5.12459</td>\n",
       "      <td>16.3186</td>\n",
       "      <td>11.194</td>\n",
       "      <td>1.17828</td>\n",
       "      <td>11.2481</td>\n",
       "      <td>1.12108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.16927</td>\n",
       "      <td>-0.345537</td>\n",
       "      <td>-5.10684</td>\n",
       "      <td>0.10117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1.46182</td>\n",
       "      <td>0.913846</td>\n",
       "      <td>2.37567</td>\n",
       "      <td>2.23997</td>\n",
       "      <td>13.4322</td>\n",
       "      <td>11.1922</td>\n",
       "      <td>1.11128</td>\n",
       "      <td>11.2464</td>\n",
       "      <td>1.16185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.732278</td>\n",
       "      <td>-2.85397</td>\n",
       "      <td>-2.08862</td>\n",
       "      <td>0.100111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>1.82442</td>\n",
       "      <td>0.70478</td>\n",
       "      <td>2.5292</td>\n",
       "      <td>1.0355</td>\n",
       "      <td>12.2276</td>\n",
       "      <td>11.1921</td>\n",
       "      <td>1.06022</td>\n",
       "      <td>11.2462</td>\n",
       "      <td>1.10784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817855</td>\n",
       "      <td>-0.919677</td>\n",
       "      <td>-0.491249</td>\n",
       "      <td>0.109707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.49298</td>\n",
       "      <td>1.0738</td>\n",
       "      <td>1.56678</td>\n",
       "      <td>4.23647</td>\n",
       "      <td>15.4294</td>\n",
       "      <td>11.1929</td>\n",
       "      <td>1.00434</td>\n",
       "      <td>11.2477</td>\n",
       "      <td>1.11482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23348</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>-4.21963</td>\n",
       "      <td>0.101155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.98471</td>\n",
       "      <td>0.721434</td>\n",
       "      <td>2.70614</td>\n",
       "      <td>0.995683</td>\n",
       "      <td>12.1902</td>\n",
       "      <td>11.1946</td>\n",
       "      <td>0.934379</td>\n",
       "      <td>11.2488</td>\n",
       "      <td>1.06519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.16377</td>\n",
       "      <td>-2.2542</td>\n",
       "      <td>-0.461679</td>\n",
       "      <td>0.102338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.20737</td>\n",
       "      <td>0.833153</td>\n",
       "      <td>2.04052</td>\n",
       "      <td>1.72787</td>\n",
       "      <td>12.9238</td>\n",
       "      <td>11.1959</td>\n",
       "      <td>0.900001</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.965878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38565</td>\n",
       "      <td>-0.0509534</td>\n",
       "      <td>-1.50889</td>\n",
       "      <td>0.120106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.473738</td>\n",
       "      <td>0.772769</td>\n",
       "      <td>1.24651</td>\n",
       "      <td>4.11354</td>\n",
       "      <td>15.3118</td>\n",
       "      <td>11.1983</td>\n",
       "      <td>0.85576</td>\n",
       "      <td>11.2523</td>\n",
       "      <td>0.972302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89982</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>-4.09219</td>\n",
       "      <td>0.100959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>2.53975</td>\n",
       "      <td>0.8948</td>\n",
       "      <td>3.43456</td>\n",
       "      <td>0.573334</td>\n",
       "      <td>11.7746</td>\n",
       "      <td>11.2013</td>\n",
       "      <td>0.826851</td>\n",
       "      <td>11.2553</td>\n",
       "      <td>0.969046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.45302</td>\n",
       "      <td>-2.81797</td>\n",
       "      <td>0.313377</td>\n",
       "      <td>0.100037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>1.23886</td>\n",
       "      <td>0.808697</td>\n",
       "      <td>2.04756</td>\n",
       "      <td>1.44947</td>\n",
       "      <td>12.6528</td>\n",
       "      <td>11.2034</td>\n",
       "      <td>0.819589</td>\n",
       "      <td>11.2574</td>\n",
       "      <td>0.886828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719767</td>\n",
       "      <td>0.758505</td>\n",
       "      <td>-1.07573</td>\n",
       "      <td>0.100214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>2.07151</td>\n",
       "      <td>0.863904</td>\n",
       "      <td>2.93541</td>\n",
       "      <td>2.06873</td>\n",
       "      <td>13.2746</td>\n",
       "      <td>11.2058</td>\n",
       "      <td>0.787714</td>\n",
       "      <td>11.2591</td>\n",
       "      <td>0.809471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.866808</td>\n",
       "      <td>0.59147</td>\n",
       "      <td>-1.91333</td>\n",
       "      <td>0.0982697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>0.291777</td>\n",
       "      <td>0.696452</td>\n",
       "      <td>0.988229</td>\n",
       "      <td>1.90282</td>\n",
       "      <td>13.1109</td>\n",
       "      <td>11.2081</td>\n",
       "      <td>0.823517</td>\n",
       "      <td>11.2615</td>\n",
       "      <td>0.842563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.167167</td>\n",
       "      <td>-0.39687</td>\n",
       "      <td>-1.7307</td>\n",
       "      <td>0.0985754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>1.08521</td>\n",
       "      <td>0.435966</td>\n",
       "      <td>1.52118</td>\n",
       "      <td>1.13061</td>\n",
       "      <td>12.3412</td>\n",
       "      <td>11.2106</td>\n",
       "      <td>0.679193</td>\n",
       "      <td>11.2641</td>\n",
       "      <td>0.894124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.424743</td>\n",
       "      <td>-0.886661</td>\n",
       "      <td>-0.656285</td>\n",
       "      <td>0.0983136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>0.955603</td>\n",
       "      <td>0.907025</td>\n",
       "      <td>1.86263</td>\n",
       "      <td>2.28116</td>\n",
       "      <td>13.4937</td>\n",
       "      <td>11.2125</td>\n",
       "      <td>0.748529</td>\n",
       "      <td>11.2661</td>\n",
       "      <td>0.818299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4204</td>\n",
       "      <td>0.387026</td>\n",
       "      <td>-2.16673</td>\n",
       "      <td>0.0987625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.990429</td>\n",
       "      <td>0.703057</td>\n",
       "      <td>1.69349</td>\n",
       "      <td>1.30522</td>\n",
       "      <td>12.5197</td>\n",
       "      <td>11.2145</td>\n",
       "      <td>0.71038</td>\n",
       "      <td>11.2683</td>\n",
       "      <td>0.83984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.171297</td>\n",
       "      <td>-1.02699</td>\n",
       "      <td>-0.983049</td>\n",
       "      <td>0.0979102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>1.04903</td>\n",
       "      <td>0.789921</td>\n",
       "      <td>1.83896</td>\n",
       "      <td>2.77567</td>\n",
       "      <td>13.9916</td>\n",
       "      <td>11.2159</td>\n",
       "      <td>0.653013</td>\n",
       "      <td>11.2698</td>\n",
       "      <td>0.821115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5008</td>\n",
       "      <td>0.0960995</td>\n",
       "      <td>-2.70586</td>\n",
       "      <td>0.101518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>0.704441</td>\n",
       "      <td>0.570204</td>\n",
       "      <td>1.27465</td>\n",
       "      <td>1.31756</td>\n",
       "      <td>12.5343</td>\n",
       "      <td>11.2168</td>\n",
       "      <td>0.708753</td>\n",
       "      <td>11.2708</td>\n",
       "      <td>0.779938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0701612</td>\n",
       "      <td>-1.66046</td>\n",
       "      <td>-0.943415</td>\n",
       "      <td>0.100199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>1.16434</td>\n",
       "      <td>0.745966</td>\n",
       "      <td>1.91031</td>\n",
       "      <td>0.964203</td>\n",
       "      <td>12.1813</td>\n",
       "      <td>11.2171</td>\n",
       "      <td>0.531314</td>\n",
       "      <td>11.2711</td>\n",
       "      <td>0.709618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0606656</td>\n",
       "      <td>0.0531233</td>\n",
       "      <td>-0.457265</td>\n",
       "      <td>0.111958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>0.979172</td>\n",
       "      <td>0.924309</td>\n",
       "      <td>1.90348</td>\n",
       "      <td>1.79854</td>\n",
       "      <td>13.0155</td>\n",
       "      <td>11.2169</td>\n",
       "      <td>0.539395</td>\n",
       "      <td>11.271</td>\n",
       "      <td>0.708124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.238748</td>\n",
       "      <td>0.395966</td>\n",
       "      <td>-1.61127</td>\n",
       "      <td>0.104536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982435</td>\n",
       "      <td>0.796679</td>\n",
       "      <td>1.77911</td>\n",
       "      <td>0.731489</td>\n",
       "      <td>11.948</td>\n",
       "      <td>11.2165</td>\n",
       "      <td>0.621234</td>\n",
       "      <td>11.2706</td>\n",
       "      <td>0.650403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.11223</td>\n",
       "      <td>-0.89465</td>\n",
       "      <td>-0.0629667</td>\n",
       "      <td>0.100091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.832271</td>\n",
       "      <td>1.00472</td>\n",
       "      <td>1.837</td>\n",
       "      <td>1.0879</td>\n",
       "      <td>12.3039</td>\n",
       "      <td>11.216</td>\n",
       "      <td>0.62002</td>\n",
       "      <td>11.2701</td>\n",
       "      <td>0.664165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.649326</td>\n",
       "      <td>0.687367</td>\n",
       "      <td>-0.669136</td>\n",
       "      <td>0.0980763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>0.93544</td>\n",
       "      <td>0.677963</td>\n",
       "      <td>1.6134</td>\n",
       "      <td>2.10455</td>\n",
       "      <td>13.3202</td>\n",
       "      <td>11.2156</td>\n",
       "      <td>0.540564</td>\n",
       "      <td>11.2698</td>\n",
       "      <td>0.571897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.385565</td>\n",
       "      <td>0.481052</td>\n",
       "      <td>-1.94086</td>\n",
       "      <td>0.100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2696</td>\n",
       "      <td>0.536229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step epoch     Dreal     Dfake     Dfull     G_adv   G_full spec_loss  \\\n",
       "0     0     0  0.841915  0.715251   1.55717   3.43627  14.6913   11.2551   \n",
       "1     1     0  0.528712  0.650194   1.17891   3.36176  14.4913   11.1296   \n",
       "2     2     0  0.504869  0.585164   1.09003   3.40135  14.5729   11.1716   \n",
       "3     3     0  0.560522  0.582506   1.14303   2.90748  14.0768   11.1694   \n",
       "4     4     0  0.782728  0.531758   1.31449   2.47862  13.6299   11.1512   \n",
       "5     5     0  0.589947  0.804335   1.39428    3.5133  14.6216   11.1083   \n",
       "6     6     0  0.489124  0.485051  0.974174   2.63965  13.5502   10.9105   \n",
       "7     7     0  0.495638  0.441089  0.936727    2.5943  13.7844   11.1901   \n",
       "8     8     0   0.58862  0.817294   1.40591   3.66594  15.4566   11.7906   \n",
       "9     9     0  0.583421  0.443322   1.02674   2.62075  13.6924   11.0717   \n",
       "10   10     1   3.20487  0.788378   3.99325   2.60432  13.5959   10.9916   \n",
       "11   11     1   3.69734  0.531606   4.22895   2.92347  14.1069   11.1835   \n",
       "12   12     1   1.26956  0.459759   1.72932   3.82207   15.024   11.2019   \n",
       "13   13     1   1.99144  0.625213   2.61666   1.55021  12.7526   11.2024   \n",
       "14   14     1   2.14958   1.09285   3.24243   3.77691  14.9852   11.2083   \n",
       "15   15     1   1.69497   0.53934   2.23431   4.07972  15.2881   11.2083   \n",
       "16   16     1   4.04819  0.702449   4.75064   1.50705  12.7161   11.2091   \n",
       "17   17     1   4.30621  0.903435   5.20965   2.29545  13.5015    11.206   \n",
       "18   18     1     1.209  0.807181   2.01618   4.72973  15.9398   11.2101   \n",
       "19   19     1  0.698346  0.593591   1.29194   3.06198  14.2679   11.2059   \n",
       "20   20     2  0.869765  0.606929   1.47669   1.71623  12.9262     11.21   \n",
       "21   21     2  0.153738  0.724801  0.878539   2.72737  13.9393   11.2119   \n",
       "22   22     2  0.592364  0.774222   1.36659    3.3853  14.5956   11.2103   \n",
       "23   23     2   1.80235  0.729755   2.53211   1.19739  12.4094    11.212   \n",
       "24   24     2   1.47765   1.13905    2.6167   2.18511  13.3967   11.2115   \n",
       "25   25     2   1.13772  0.871386   2.00911   5.52023  16.7312    11.211   \n",
       "26   26     2   1.33803  0.776665   2.11469   2.90884  14.1167   11.2079   \n",
       "27   27     2   2.63493  0.553062     3.188  0.833248  12.0368   11.2035   \n",
       "28   28     2   2.61084   1.19425   3.80509   3.31901  14.5182   11.1991   \n",
       "29   29     2  0.921814  0.419062   1.34088   5.12459  16.3186    11.194   \n",
       "30   30     3   1.46182  0.913846   2.37567   2.23997  13.4322   11.1922   \n",
       "31   31     3   1.82442   0.70478    2.5292    1.0355  12.2276   11.1921   \n",
       "32   32     3   0.49298    1.0738   1.56678   4.23647  15.4294   11.1929   \n",
       "33   33     3   1.98471  0.721434   2.70614  0.995683  12.1902   11.1946   \n",
       "34   34     3   1.20737  0.833153   2.04052   1.72787  12.9238   11.1959   \n",
       "35   35     3  0.473738  0.772769   1.24651   4.11354  15.3118   11.1983   \n",
       "36   36     3   2.53975    0.8948   3.43456  0.573334  11.7746   11.2013   \n",
       "37   37     3   1.23886  0.808697   2.04756   1.44947  12.6528   11.2034   \n",
       "38   38     3   2.07151  0.863904   2.93541   2.06873  13.2746   11.2058   \n",
       "39   39     3  0.291777  0.696452  0.988229   1.90282  13.1109   11.2081   \n",
       "40   40     4   1.08521  0.435966   1.52118   1.13061  12.3412   11.2106   \n",
       "41   41     4  0.955603  0.907025   1.86263   2.28116  13.4937   11.2125   \n",
       "42   42     4  0.990429  0.703057   1.69349   1.30522  12.5197   11.2145   \n",
       "43   43     4   1.04903  0.789921   1.83896   2.77567  13.9916   11.2159   \n",
       "44   44     4  0.704441  0.570204   1.27465   1.31756  12.5343   11.2168   \n",
       "45   45     4   1.16434  0.745966   1.91031  0.964203  12.1813   11.2171   \n",
       "46   46     4  0.979172  0.924309   1.90348   1.79854  13.0155   11.2169   \n",
       "47   47     4  0.982435  0.796679   1.77911  0.731489   11.948   11.2165   \n",
       "48   48     4  0.832271   1.00472     1.837    1.0879  12.3039    11.216   \n",
       "49   49     4   0.93544  0.677963    1.6134   2.10455  13.3202   11.2156   \n",
       "50  NaN   NaN       NaN       NaN       NaN       NaN      NaN       NaN   \n",
       "\n",
       "   hist_loss spec_chi  hist_chi gp_loss fm_loss       D(x)     D_G_z1  \\\n",
       "0    1.61228      NaN       NaN     NaN     NaN -0.0696436  -0.208424   \n",
       "1    1.61329  11.2589   1.74137     NaN     NaN   0.406427 -0.0443879   \n",
       "2    1.61501  11.2135   1.63144     NaN     NaN   0.887204  0.0438551   \n",
       "3    1.61509   11.221   1.64355     NaN     NaN   0.997698  -0.707787   \n",
       "4    1.61201  11.1401   1.60881     NaN     NaN  0.0105866   -0.54982   \n",
       "5    1.61322  11.1136    1.6276     NaN     NaN     1.3728   0.192068   \n",
       "6    1.61516  10.9624   1.62754     NaN     NaN   0.412303  -0.918754   \n",
       "7    1.61573  12.0794   1.62015     NaN     NaN   0.929048  -0.936146   \n",
       "8    1.61728  12.5057   1.60816     NaN     NaN   0.843806   0.300981   \n",
       "9    1.61571   11.467   1.60569     NaN     NaN   0.758334   -1.23671   \n",
       "10   1.61131  11.0968   1.60122     NaN     NaN    1.81404  -0.461801   \n",
       "11   1.59483  11.1989   1.56076     NaN     NaN   -1.21364 -0.0348395   \n",
       "12   1.57898  11.2503   1.57648     NaN     NaN    4.74056  -0.493722   \n",
       "13   1.56349  11.2579   1.56873     NaN     NaN   -1.04333   -1.87929   \n",
       "14    1.5457  11.2501    1.5211     NaN     NaN    1.77106   0.489263   \n",
       "15   1.52685  11.2625   1.55508     NaN     NaN     5.8329  -0.703641   \n",
       "16   1.51086   11.251   1.48602     NaN     NaN   -1.66749   -1.88185   \n",
       "17   1.49202  11.2523   1.46738     NaN     NaN   -1.84459    0.28773   \n",
       "18   1.46969  11.2517   1.43762     NaN     NaN    3.82841  0.0552055   \n",
       "19   1.44643  11.2563   1.41283     NaN     NaN     2.0463   -1.73429   \n",
       "20    1.4215  11.2633   1.43628     NaN     NaN    1.79404   -1.70906   \n",
       "21   1.39965   11.263   1.41128     NaN     NaN  -0.558859 -0.0412358   \n",
       "22   1.38029  11.2637   1.39796     NaN     NaN   0.275968  0.0990108   \n",
       "23   1.36664  11.2664   1.45473     NaN     NaN   -2.54237   -1.29815   \n",
       "24   1.34835  11.2659   1.40785     NaN     NaN   -1.91149   0.246781   \n",
       "25   1.32286  11.2648   1.37665     NaN     NaN    3.18269    0.60957   \n",
       "26   1.29488  11.2619   1.33919     NaN     NaN      1.531   -2.83703   \n",
       "27   1.25681  11.2573   1.31078     NaN     NaN  -0.543228   -1.27251   \n",
       "28   1.22366  11.2533   1.22023     NaN     NaN  -0.154936    1.51298   \n",
       "29   1.17828  11.2481   1.12108     NaN     NaN    3.16927  -0.345537   \n",
       "30   1.11128  11.2464   1.16185     NaN     NaN   0.732278   -2.85397   \n",
       "31   1.06022  11.2462   1.10784     NaN     NaN   0.817855  -0.919677   \n",
       "32   1.00434  11.2477   1.11482     NaN     NaN    2.23348     1.0232   \n",
       "33  0.934379  11.2488   1.06519     NaN     NaN   -3.16377    -2.2542   \n",
       "34  0.900001    11.25  0.965878     NaN     NaN    1.38565 -0.0509534   \n",
       "35   0.85576  11.2523  0.972302     NaN     NaN    1.89982   0.410772   \n",
       "36  0.826851  11.2553  0.969046     NaN     NaN   -4.45302   -2.81797   \n",
       "37  0.819589  11.2574  0.886828     NaN     NaN   0.719767   0.758505   \n",
       "38  0.787714  11.2591  0.809471     NaN     NaN  -0.866808    0.59147   \n",
       "39  0.823517  11.2615  0.842563     NaN     NaN   0.167167   -0.39687   \n",
       "40  0.679193  11.2641  0.894124     NaN     NaN   0.424743  -0.886661   \n",
       "41  0.748529  11.2661  0.818299     NaN     NaN     1.4204   0.387026   \n",
       "42   0.71038  11.2683   0.83984     NaN     NaN  -0.171297   -1.02699   \n",
       "43  0.653013  11.2698  0.821115     NaN     NaN     2.5008  0.0960995   \n",
       "44  0.708753  11.2708  0.779938     NaN     NaN -0.0701612   -1.66046   \n",
       "45  0.531314  11.2711  0.709618     NaN     NaN -0.0606656  0.0531233   \n",
       "46  0.539395   11.271  0.708124     NaN     NaN   0.238748   0.395966   \n",
       "47  0.621234  11.2706  0.650403     NaN     NaN   -1.11223   -0.89465   \n",
       "48   0.62002  11.2701  0.664165     NaN     NaN  -0.649326   0.687367   \n",
       "49  0.540564  11.2698  0.571897     NaN     NaN   0.385565   0.481052   \n",
       "50       NaN  11.2696  0.536229     NaN     NaN        NaN        NaN   \n",
       "\n",
       "        D_G_z2       time  \n",
       "0     -2.60994   0.804343  \n",
       "1     -2.78831  0.0998726  \n",
       "2     -3.01401   0.101523  \n",
       "3     -2.30605    0.10299  \n",
       "4     -1.92131   0.101465  \n",
       "5     -3.29496   0.100096  \n",
       "6     -2.14345  0.0999992  \n",
       "7      -2.0317   0.100204  \n",
       "8     -3.44808  0.0998306  \n",
       "9     -2.09881   0.100337  \n",
       "10    -2.13162  0.0984762  \n",
       "11     -2.6723   0.098561  \n",
       "12    -3.67086  0.0996554  \n",
       "13   -0.515411  0.0998304  \n",
       "14    -3.67769   0.100532  \n",
       "15    -3.97985   0.100154  \n",
       "16   -0.798562   0.100773  \n",
       "17    -2.00049   0.099766  \n",
       "18     -4.6996   0.100549  \n",
       "19    -2.89667   0.100495  \n",
       "20    -1.25762   0.104186  \n",
       "21      -2.504   0.109031  \n",
       "22    -3.30767   0.101891  \n",
       "23   -0.597376   0.100012  \n",
       "24    -1.94268   0.100083  \n",
       "25    -5.51062   0.100914  \n",
       "26    -2.76262   0.100271  \n",
       "27 -0.00142056  0.0993879  \n",
       "28     -3.2434     0.1004  \n",
       "29    -5.10684    0.10117  \n",
       "30    -2.08862   0.100111  \n",
       "31   -0.491249   0.109707  \n",
       "32    -4.21963   0.101155  \n",
       "33   -0.461679   0.102338  \n",
       "34    -1.50889   0.120106  \n",
       "35    -4.09219   0.100959  \n",
       "36    0.313377   0.100037  \n",
       "37    -1.07573   0.100214  \n",
       "38    -1.91333  0.0982697  \n",
       "39     -1.7307  0.0985754  \n",
       "40   -0.656285  0.0983136  \n",
       "41    -2.16673  0.0987625  \n",
       "42   -0.983049  0.0979102  \n",
       "43    -2.70586   0.101518  \n",
       "44   -0.943415   0.100199  \n",
       "45   -0.457265   0.111958  \n",
       "46    -1.61127   0.104536  \n",
       "47  -0.0629667   0.100091  \n",
       "48   -0.669136  0.0980763  \n",
       "49    -1.94086   0.100001  \n",
       "50         NaN        NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics_df.plot('step','time')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "#         ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "#             nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             View(shape=[-1,ngf*8,8,8]),\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, ip):\n",
    "#         return self.main(ip)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "#         ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "#             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(nc*ndf*8*8*8, 1)\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, ip):\n",
    "# #         print(ip.shape)\n",
    "#         results=[ip]\n",
    "#         lst_idx=[]\n",
    "#         for i,submodel in enumerate(self.main.children()):\n",
    "#             mid_output=submodel(results[-1])\n",
    "#             results.append(mid_output)\n",
    "#             ## Select indices in list corresponding to output of Conv layers\n",
    "#             if submodel.__class__.__name__.startswith('Conv'):\n",
    "# #                 print(submodel.__class__.__name__)\n",
    "# #                 print(mid_output.shape)\n",
    "#                 lst_idx.append(i)\n",
    "\n",
    "#         FMloss=True\n",
    "#         if FMloss:\n",
    "#             ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "#         else :\n",
    "#             ans=results[-1]\n",
    "#         return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator(gdict).to(gdict['device'])\n",
    "# netG.apply(weights_init)\n",
    "# # # #     print(netG)\n",
    "# # summary(netG,(1,1,64))\n",
    "# # Create Discriminator\n",
    "# netD = Discriminator(gdict).to(gdict['device'])\n",
    "# netD.apply(weights_init)\n",
    "# #     print(netD)\n",
    "# summary(netD,(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn(gdict['batchsize'], 1, 1, gdict['nz'], device=gdict['device'])\n",
    "# fake = netG(noise)            \n",
    "# # Forward pass real batch through D\n",
    "# output = netD(fake)\n",
    "# print([i.shape for i in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
