{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cosmogan\n",
    "April 19, 2021\n",
    "\n",
    "Borrowing pieces of code from : \n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/11569e0db3599ac214b03e01956c2971b02c64ce/beginner_source/dcgan_faces_tutorial.py\n",
    "- https://github.com/exalearn/epiCorvid/tree/master/cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "# import torch.fft\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections\n",
    "import socket\n",
    "import shutil\n",
    "\n",
    "# # Import modules from other files\n",
    "# from utils import *\n",
    "# from spec_loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4.) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s)\n",
    "\n",
    "  \n",
    "# Generator Code\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "def f_get_model(gdict):\n",
    "    ''' Module to define Generator and Discriminator'''\n",
    "\n",
    "    if gdict['image_size']==64:\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8**3),# 262144\n",
    "                    nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,4,4,4]),\n",
    "                    nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, ip):\n",
    "                return self.main(ip)\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64 x 64\n",
    "                    # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv3d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm3d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, ip):\n",
    "        #         print(ip.shape)\n",
    "                results=[ip]\n",
    "                lst_idx=[]\n",
    "                for i,submodel in enumerate(self.main.children()):\n",
    "                    mid_output=submodel(results[-1])\n",
    "                    results.append(mid_output)\n",
    "                    ## Select indices in list corresponding to output of Conv layers\n",
    "                    if submodel.__class__.__name__.startswith('Conv'):\n",
    "        #                 print(submodel.__class__.__name__)\n",
    "        #                 print(mid_output.shape)\n",
    "                        lst_idx.append(i)\n",
    "\n",
    "                FMloss=True\n",
    "                if FMloss:\n",
    "                    ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "                else :\n",
    "                    ans=results[-1]\n",
    "                return ans\n",
    "\n",
    "    elif gdict['image_size']==128:\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8**3*8),# 262144\n",
    "                    nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,8,8,8]),\n",
    "                    nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    nn.BatchNorm3d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, ip):\n",
    "                return self.main(ip)\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64 x 64\n",
    "                    # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv3d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    nn.BatchNorm3d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    nn.BatchNorm3d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "            def forward(self, ip):\n",
    "                results=[ip]\n",
    "                lst_idx=[]\n",
    "                for i,submodel in enumerate(self.main.children()):\n",
    "                    mid_output=submodel(results[-1])\n",
    "                    results.append(mid_output)\n",
    "                    ## Select indices in list corresponding to output of Conv layers\n",
    "                    if submodel.__class__.__name__.startswith('Conv'):\n",
    "        #                 print(submodel.__class__.__name__)\n",
    "        #                 print(mid_output.shape)\n",
    "                        lst_idx.append(i)\n",
    "\n",
    "                FMloss=True\n",
    "                if FMloss:\n",
    "                    ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "                else :\n",
    "                    ans=results[-1]\n",
    "                return ans\n",
    "    \n",
    "    return Generator, Discriminator\n",
    "\n",
    "\n",
    "def f_gen_images(gdict,netG,optimizerG,ip_fname,op_loc,op_strg='inf_img_',op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: gdict, netG, optimizerG, \n",
    "                 ip_fname: name of input file\n",
    "                op_strg: [string name for output file]\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    nz,device=gdict['nz'],gdict['device']\n",
    "\n",
    "    try:# handling cpu vs gpu\n",
    "        if torch.cuda.is_available(): checkpoint=torch.load(ip_fname)\n",
    "        else: checkpoint=torch.load(ip_fname,map_location=torch.device('cpu'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "    \n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, 1, nz, device=device)\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    with torch.no_grad(): ## This is important. fails without it for multi-gpu\n",
    "        gen = netG(noise)\n",
    "        gen_images=gen.detach().cpu().numpy()\n",
    "        print(gen_images.shape)\n",
    "    \n",
    "    op_fname='%s_epoch-%s_step-%s.npy'%(op_strg,epoch,iters)\n",
    "    np.save(op_loc+op_fname,gen_images)\n",
    "\n",
    "    print(\"Image saved in \",op_fname)\n",
    "    \n",
    "def f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc):\n",
    "    ''' Checkpoint model '''\n",
    "    \n",
    "    if gdict['multi-gpu']: ## Dataparallel\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.module.state_dict(),'D_state':netD.module.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc) \n",
    "    else :\n",
    "        torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.state_dict(),'D_state':netD.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc)\n",
    "    \n",
    "def f_load_checkpoint(ip_fname,netG,netD,optimizerG,optimizerD,gdict):\n",
    "    ''' Load saved checkpoint\n",
    "    Also loads step, epoch, best_chi1, best_chi2'''\n",
    "    \n",
    "    print(\"torch device\",torch.device('cuda',torch.cuda.current_device()))\n",
    "            \n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname,map_location=torch.device('cuda',torch.cuda.current_device()))\n",
    "    except Exception as e:\n",
    "        print(\"Error loading saved checkpoint\",ip_fname)\n",
    "        print(e)\n",
    "        raise SystemError\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "        netD.module.load_state_dict(checkpoint['D_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "        netD.load_state_dict(checkpoint['D_state'])\n",
    "    \n",
    "    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    best_chi1=checkpoint['best_chi1']\n",
    "    best_chi2=checkpoint['best_chi2']\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    return iters,epoch,best_chi1,best_chi2,netD,optimizerD,netG,optimizerG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_get_rad(img):\n",
    "    ''' Get the radial tensor for use in f_torch_get_azimuthalAverage '''\n",
    "    \n",
    "    height,width,depth=img.shape[-3:]\n",
    "    # Create a grid of points with x and y and z coordinates\n",
    "    z,y,x = np.indices([height,width,depth])\n",
    "    \n",
    "    center=[]\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0, (z.max()-z.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r= torch.tensor(np.sqrt((x-center[0])**2 + (y-center[1])**2 + (z-center[2])**2))\n",
    "        \n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    return r.detach(),ind.detach()\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image,r,ind):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "#     height, width = image.shape\n",
    "#     # Create a grid of points with x and y coordinates\n",
    "#     y, x = np.indices([height,width])\n",
    "\n",
    "#     if not center:\n",
    "#         center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "#     # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "#     r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "#     # Get sorted radii\n",
    "#     ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "def f_torch_compute_spectrum(arr,r,ind):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    \n",
    "    y1=torch.rfft(arr,signal_ndim=3,onesided=False)\n",
    "    real,imag=f_torch_fftshift(y1[:,:,:,0],y1[:,:,:,1])    ## last index is real/imag part  ## Mod for 3D\n",
    "    \n",
    "#     # For pytorch 1.8\n",
    "#     y1=torch.fft.fftn(arr,dim=(-3,-2,-1))\n",
    "#     real,imag=f_torch_fftshift(y1.real,y1.imag)    \n",
    "    \n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    z1=f_torch_get_azimuthalAverage(y2,r,ind)     ## Compute radial profile\n",
    "    return z1\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr,r,ind):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i,r,ind) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels,r,ind):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    var=[[] for i in range(num_channels)] \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:,:] # Mod for 3D\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr,r,ind)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "#         var[i]=torch.std(batch_pk,axis=0)/np.sqrt(batch_pk.shape[0])\n",
    "#         var[i]=torch.std(batch_pk,axis=0)\n",
    "        var[i]=torch.var(batch_pk,axis=0)\n",
    "    \n",
    "    mean=torch.stack(mean)\n",
    "    var=torch.stack(var)\n",
    "        \n",
    "    if (torch.isnan(mean).any() or torch.isnan(var).any()):\n",
    "        print(\"Nans in spectrum\",mean,var)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"Nans in Input image\")\n",
    "\n",
    "    return mean,var\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    \n",
    "    try: \n",
    "        hist_data=torch.histc(data,bins=bins)\n",
    "        ## A kind of normalization of histograms: divide by total sum\n",
    "        hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hist_data=torch.zeros(bins)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_var,spec_var_ref,image_size,lambda_spec_mean,lambda_spec_var):\n",
    "    ''' Loss function for the spectrum : mean + variance \n",
    "    Log(sum( batch value - expect value) ^ 2 )) '''\n",
    "    \n",
    "    if (torch.isnan(spec_mean).any() or torch.isnan(spec_var).any()):\n",
    "        ans=torch.tensor(float(\"inf\"))\n",
    "        return ans\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "        \n",
    "    loss_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    loss_var=torch.log(torch.mean(torch.pow(spec_var[:,:idx]-spec_var_ref[:,:idx],2)))\n",
    "    \n",
    "    ans=lambda_spec_mean*loss_mean+lambda_spec_var*loss_var\n",
    "    \n",
    "    if (torch.isnan(ans).any()) :    \n",
    "        print(\"loss spec mean %s, loss spec var %s\"%(loss_mean,loss_var))\n",
    "        print(\"spec mean %s, ref %s\"%(spec_mean, spec_mean_ref))\n",
    "        print(\"spec var %s, ref %s\"%(spec_var, spec_var_ref))\n",
    "#         raise SystemExit\n",
    "        \n",
    "    return ans\n",
    "    \n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return lambda1*torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n",
    "\n",
    "def f_FM_loss(real_output,fake_output,lambda_fm,gdict):\n",
    "    '''\n",
    "    Module to implement Feature-Matching loss. Reads all but last elements of Discriminator ouput\n",
    "    '''\n",
    "    FM=torch.Tensor([0.0]).to(gdict['device'])\n",
    "    for i,j in zip(real_output[:-1],fake_output[:-1]):\n",
    "#         print(i.shape,j.shape)\n",
    "        real_mean=torch.mean(i)\n",
    "        fake_mean=torch.mean(j)\n",
    "#         print(real_mean,fake_mean)\n",
    "        FM=FM.clone()+torch.sum(torch.square(real_mean-fake_mean))\n",
    "    return lambda_fm*FM\n",
    "\n",
    "def f_gp_loss(grads,l=1.0):\n",
    "    '''\n",
    "    Module to implement gradient penalty loss.\n",
    "    '''\n",
    "    loss=torch.mean(torch.sum(torch.square(grads),dim=[1,2,3]))\n",
    "    return l*loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train code ###\n",
    "def f_train_loop(gan_model,Dset,metrics_df,gdict,fixed_noise):\n",
    "    ''' Train epochs '''\n",
    "    ## Define new variables from dict\n",
    "    keys=['image_size','start_epoch','epochs','iters','best_chi1','best_chi2','save_dir','device','flip_prob','nz','batch_size','bns']\n",
    "    image_size,start_epoch,epochs,iters,best_chi1,best_chi2,save_dir,device,flip_prob,nz,batchsize,bns=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "    \n",
    "    for epoch in range(start_epoch,epochs):\n",
    "        t_epoch_start=time.time()\n",
    "        for count, data in enumerate(Dset.train_dataloader):\n",
    "\n",
    "            ####### Train GAN ########\n",
    "            gan_model.netG.train(); gan_model.netD.train();  ### Need to add these after inference and before training\n",
    "\n",
    "            tme1=time.time()\n",
    "            ### Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            gan_model.netD.zero_grad()\n",
    "\n",
    "            real_cpu = data[0].to(device)\n",
    "            real_cpu.requires_grad=True\n",
    "            b_size = real_cpu.size(0)\n",
    "            real_label = torch.full((b_size,), 1, device=device,dtype=float)\n",
    "            fake_label = torch.full((b_size,), 0, device=device,dtype=float)\n",
    "            g_label = torch.full((b_size,), 1, device=device,dtype=float) ## No flipping for Generator labels\n",
    "            # Flip labels with probability flip_prob\n",
    "            for idx in np.random.choice(np.arange(b_size),size=int(np.ceil(b_size*flip_prob))):\n",
    "                real_label[idx]=0; fake_label[idx]=1\n",
    "\n",
    "            # Generate fake image batch with G\n",
    "            noise = torch.randn(b_size, 1, 1, 1, nz, device=device) ### Mod for 3D\n",
    "            fake = gan_model.netG(noise)            \n",
    "\n",
    "            # Forward pass real batch through D\n",
    "            real_output = gan_model.netD(real_cpu)\n",
    "            errD_real = gan_model.criterion(real_output[-1].view(-1), real_label.float())\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            D_x = real_output[-1].mean().item()\n",
    "\n",
    "            # Forward pass fake batch through D\n",
    "            fake_output = gan_model.netD(fake.detach())   # The detach is important\n",
    "            errD_fake = gan_model.criterion(fake_output[-1].view(-1), fake_label.float())\n",
    "            errD_fake.backward(retain_graph=True)\n",
    "            D_G_z1 = fake_output[-1].mean().item()\n",
    "            \n",
    "            errD = errD_real + errD_fake \n",
    "\n",
    "            if gdict['lambda_gp']: ## Add gradient - penalty loss\n",
    "                grads=torch.autograd.grad(outputs=real_output[-1],inputs=real_cpu,grad_outputs=torch.ones_like(real_output[-1]),allow_unused=False,create_graph=True)[0]\n",
    "                gp_loss=f_gp_loss(grads,gdict['lambda_gp'])\n",
    "                gp_loss.backward(retain_graph=True)\n",
    "                errD = errD + gp_loss\n",
    "            else:\n",
    "                gp_loss=torch.Tensor([np.nan])\n",
    "                \n",
    "            if gdict['grad_clip']:\n",
    "                nn.utils.clip_grad_norm_(gan_model.netD.parameters(),gdict['grad_clip'])\n",
    "\n",
    "            gan_model.optimizerD.step()\n",
    "            lr_d=gan_model.optimizerD.param_groups[0]['lr']\n",
    "            gan_model.schedulerD.step()\n",
    "            \n",
    "# dict_keys(['train_data_loader', 'r', 'ind', 'train_spec_mean', 'train_spec_var', 'train_hist', 'val_spec_mean', 'val_spec_var', 'val_hist'])\n",
    "\n",
    "            ###Update G network: maximize log(D(G(z)))\n",
    "            gan_model.netG.zero_grad()\n",
    "            output = gan_model.netD(fake)\n",
    "            errG_adv = gan_model.criterion(output[-1].view(-1), g_label.float())\n",
    "#             errG_adv.backward(retain_graph=True)\n",
    "            # Histogram pixel intensity loss\n",
    "            hist_gen=f_compute_hist(fake,bins=bns)\n",
    "            hist_loss=loss_hist(hist_gen,Dset.train_hist.to(device))\n",
    "\n",
    "            # Add spectral loss\n",
    "            mean,var=f_torch_image_spectrum(f_invtransform(fake),1,Dset.r.to(device),Dset.ind.to(device))\n",
    "            spec_loss=loss_spectrum(mean,Dset.train_spec_mean.to(device),var,Dset.train_spec_var.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])\n",
    "\n",
    "            errG=errG_adv\n",
    "            if gdict['lambda_spec_mean']: \n",
    "#                 spec_loss.backward(retain_graph=True)\n",
    "                errG = errG+ spec_loss \n",
    "            if gdict['lambda_fm']:## Add feature matching loss\n",
    "                fm_loss=f_FM_loss([i.detach() for i in real_output],output,gdict['lambda_fm'],gdict)\n",
    "#                 fm_loss.backward(retain_graph=True)\n",
    "                errG= errG+ fm_loss\n",
    "            else: \n",
    "                fm_loss=torch.Tensor([np.nan])\n",
    "\n",
    "            if torch.isnan(errG).any():\n",
    "                logging.info(errG)\n",
    "                raise SystemError\n",
    "            \n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output[-1].mean().item()\n",
    "            \n",
    "            ### Implement Gradient clipping\n",
    "            if gdict['grad_clip']:\n",
    "                nn.utils.clip_grad_norm_(gan_model.netG.parameters(),gdict['grad_clip'])\n",
    "            \n",
    "            gan_model.optimizerG.step()\n",
    "            lr_g=gan_model.optimizerG.param_groups[0]['lr']\n",
    "            gan_model.schedulerG.step()\n",
    "            \n",
    "            tme2=time.time()\n",
    "            ####### Store metrics ########\n",
    "            # Output training stats\n",
    "            if gdict['world_rank']==0:\n",
    "                if ((count % gdict['checkpoint_size'] == 0)):\n",
    "                    logging.info('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_adv: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                          % (epoch, epochs, count, len(Dset.train_dataloader), errD.item(), errG_adv.item(),errG.item(), D_x, D_G_z1, D_G_z2)),\n",
    "                    logging.info(\"Spec loss: %s,\\t hist loss: %s\"%(spec_loss.item(),hist_loss.item())),\n",
    "                    logging.info(\"Training time for step %s : %s\"%(iters, tme2-tme1))\n",
    "\n",
    "                # Save metrics\n",
    "                cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','fm_loss','gp_loss','D(x)','D_G_z1','D_G_z2','lr_d','lr_g','time']\n",
    "                vals=[iters,epoch,errD_real.item(),errD_fake.item(),errD.item(),errG_adv.item(),errG.item(),spec_loss.item(),hist_loss.item(),fm_loss.item(),gp_loss.item(),D_x,D_G_z1,D_G_z2,lr_d,lr_g,tme2-tme1]\n",
    "                for col,val in zip(cols,vals):  metrics_df.loc[iters,col]=val\n",
    "\n",
    "                ### Checkpoint the best model\n",
    "                checkpoint=True\n",
    "                iters += 1  ### Model has been updated, so update iters before saving metrics and model.\n",
    "\n",
    "                ### Compute validation metrics for updated model\n",
    "                gan_model.netG.eval()\n",
    "                with torch.no_grad():\n",
    "                    fake = gan_model.netG(fixed_noise)\n",
    "                    hist_gen=f_compute_hist(fake,bins=bns)\n",
    "                    hist_chi=loss_hist(hist_gen,Dset.val_hist.to(device))\n",
    "                    mean,var=f_torch_image_spectrum(f_invtransform(fake),1,Dset.r.to(device),Dset.ind.to(device))\n",
    "                    spec_chi=loss_spectrum(mean,Dset.val_spec_mean.to(device),var,Dset.val_spec_var.to(device),image_size,gdict['lambda_spec_mean'],gdict['lambda_spec_var'])\n",
    "\n",
    "                # Storing chi for next step\n",
    "                for col,val in zip(['spec_chi','hist_chi'],[spec_chi.item(),hist_chi.item()]):  metrics_df.loc[iters,col]=val            \n",
    "\n",
    "                # Checkpoint model for continuing run\n",
    "                if count == len(Dset.train_dataloader)-1: ## Check point at last step of epoch\n",
    "                    f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_last.tar')  \n",
    "\n",
    "                if (checkpoint and (epoch > 1)): # Choose best models by metric\n",
    "                    if hist_chi< best_chi1:\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_best_hist.tar')\n",
    "                        best_chi1=hist_chi.item()\n",
    "                        logging.info(\"Saving best hist model at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                    if  spec_chi< best_chi2:\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_best_spec.tar')\n",
    "                        best_chi2=spec_chi.item()\n",
    "                        logging.info(\"Saving best spec model at epoch %s, step %s\"%(epoch,iters))\n",
    "\n",
    "#                    if (iters in gdict['save_steps_list']) :\n",
    "                    if ((gdict['save_steps_list']=='all') and (iters % gdict['checkpoint_size'] == 0)):\n",
    "                        f_save_checkpoint(gdict,epoch,iters,best_chi1,best_chi2,gan_model.netG,gan_model.netD,gan_model.optimizerG,gan_model.optimizerD,save_loc=save_dir+'/models/checkpoint_{0}.tar'.format(iters))\n",
    "                        logging.info(\"Saving given-step at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "                # Save G's output on fixed_noise\n",
    "                if ((iters % gdict['checkpoint_size'] == 0) or ((epoch == epochs-1) and (count == len(Dset.train_dataloader)-1))):\n",
    "                    gan_model.netG.eval()\n",
    "                    with torch.no_grad():\n",
    "                        fake = gan_model.netG(fixed_noise).detach().cpu()\n",
    "                        img_arr=np.array(fake)\n",
    "                        fname='gen_img_epoch-%s_step-%s'%(epoch,iters)\n",
    "                        np.save(save_dir+'/images/'+fname,img_arr)\n",
    "        \n",
    "        t_epoch_end=time.time()\n",
    "        if gdict['world_rank']==0:\n",
    "            logging.info(\"Time taken for epoch %s, count %s: %s for rank %s\"%(epoch,count,t_epoch_end-t_epoch_start,gdict['world_rank']))\n",
    "            # Save Metrics to file after each epoch\n",
    "            metrics_df.to_pickle(save_dir+'/df_metrics.pkle')\n",
    "            logging.info(\"best chis: {0}, {1}\".format(best_chi1,best_chi2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Setup modules ###\n",
    "def f_manual_add_argparse():\n",
    "    ''' use only in jpt notebook'''\n",
    "    args=argparse.Namespace()\n",
    "    args.config='config_3d_gan.yaml'\n",
    "    args.mode='fresh'\n",
    "    args.local_rank=0\n",
    "    args.facility='cori'\n",
    "    args.distributed=False\n",
    "\n",
    "#     args.mode='continue'\n",
    "    \n",
    "    return args\n",
    "\n",
    "def f_parse_args():\n",
    "    \"\"\"Parse command line arguments.Only for .py file\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Run script to train GAN using pytorch\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    add_arg = parser.add_argument\n",
    "    \n",
    "    add_arg('--config','-cfile',  type=str, default='config_3d_Cgan.yaml', help='Name of config file')\n",
    "    add_arg('--mode','-m',  type=str, choices=['fresh','continue','fresh_load'],default='fresh', help='Whether to start fresh run or continue previous run or fresh run loading a config file.')\n",
    "    add_arg(\"--local_rank\", default=0, type=int,help='Local rank of GPU on node. Using for pytorch DDP. ')\n",
    "    add_arg(\"--facility\", default='cori', choices=['cori','summit'],type=str,help='Facility: cori or summit ')\n",
    "    add_arg(\"--ddp\", dest='distributed' ,default=False,action='store_true',help='use Distributed DataParallel for Pytorch or DataParallel')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def try_barrier(rank):\n",
    "    \"\"\"\n",
    "    Used in Distributed data parallel\n",
    "    Attempt a barrier but ignore any exceptions\n",
    "    \"\"\"\n",
    "    print('BAR %d'%rank)\n",
    "    try:\n",
    "        dist.barrier()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def f_init_gdict(args,gdict):\n",
    "    ''' Create global dictionary gdict from args and config file'''\n",
    "    \n",
    "    ## read config file\n",
    "    config_file=args.config\n",
    "    with open(config_file) as f:\n",
    "        config_dict= yaml.load(f, Loader=yaml.SafeLoader)\n",
    "        \n",
    "    gdict=config_dict['parameters']\n",
    "\n",
    "    args_dict=vars(args)\n",
    "    ## Add args variables to gdict\n",
    "    for key in args_dict.keys():\n",
    "        gdict[key]=args_dict[key]\n",
    "\n",
    "    if gdict['distributed']: \n",
    "        assert not gdict['lambda_gp'],\"GP couplings is %s. Cannot use Gradient penalty loss in pytorch DDP\"%(gdict['lambda_gp'])\n",
    "    else : print(\"Not using DDP\")\n",
    "    return gdict\n",
    "\n",
    "\n",
    "def f_get_img_samples(ip_arr,rank=0,num_ranks=1):\n",
    "    '''\n",
    "    Module to get part of the numpy image file\n",
    "    '''\n",
    "    \n",
    "    data_size=ip_arr.shape[0]\n",
    "    size=data_size//num_ranks\n",
    "    \n",
    "    if gdict['batch_size']>size:\n",
    "        print(\"Caution: batchsize %s is greater than samples per GPU %s\"%(gdict['batch_size'],size))\n",
    "        raise SystemExit\n",
    "        \n",
    "    ### Get a set of random indices from numpy array\n",
    "    random=False\n",
    "    if random:\n",
    "        idxs=np.arange(ip_arr.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        rnd_idxs=idxs[rank*(size):(rank+1)*size]\n",
    "        arr=ip_arr[rnd_idxs].copy()\n",
    "        \n",
    "    else: arr=ip_arr[rank*(size):(rank+1)*size].copy()\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def f_setup(gdict,metrics_df,log):\n",
    "    ''' \n",
    "    Set up directories, Initialize random seeds, add GPU info, add logging info.\n",
    "    '''\n",
    "    \n",
    "    torch.backends.cudnn.benchmark=True\n",
    "#     torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    ## New additions. Code taken from Jan B.\n",
    "    os.environ['MASTER_PORT'] = \"8885\"\n",
    "\n",
    "    if gdict['facility']=='summit':\n",
    "        get_master = \"echo $(cat {} | sort | uniq | grep -v batch | grep -v login | head -1)\".format(os.environ['LSB_DJOB_HOSTFILE'])\n",
    "        os.environ['MASTER_ADDR'] = str(subprocess.check_output(get_master, shell=True))[2:-3]\n",
    "        os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\n",
    "        os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']\n",
    "        gdict['local_rank'] = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "    else:\n",
    "        if gdict['distributed']:\n",
    "            os.environ['WORLD_SIZE'] = os.environ['SLURM_NTASKS']\n",
    "            os.environ['RANK'] = os.environ['SLURM_PROCID']\n",
    "            gdict['local_rank'] = int(os.environ['SLURM_LOCALID'])\n",
    "\n",
    "    ## Special declarations\n",
    "    gdict['ngpu']=torch.cuda.device_count()\n",
    "    gdict['device']=torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "    gdict['multi-gpu']=True if (gdict['device'].type == 'cuda') and (gdict['ngpu'] > 1) else False \n",
    "    \n",
    "    ########################\n",
    "    ###### Set up Distributed Data parallel ######\n",
    "    if gdict['distributed']:\n",
    "#         gdict['local_rank']=args.local_rank  ## This is needed when using pytorch -m torch.distributed.launch\n",
    "        gdict['world_size']=int(os.environ['WORLD_SIZE'])\n",
    "        torch.cuda.set_device(gdict['local_rank']) ## Very important\n",
    "        dist.init_process_group(backend='nccl', init_method=\"env://\")  \n",
    "        gdict['world_rank']= dist.get_rank()\n",
    "        \n",
    "        device = torch.cuda.current_device()\n",
    "        logging.info(\"World size %s, world rank %s, local rank %s device %s, hostname %s, GPUs on node %s\\n\"%(gdict['world_size'],gdict['world_rank'],gdict['local_rank'],device,socket.gethostname(),gdict['ngpu']))\n",
    "        \n",
    "        # Divide batch size by number of GPUs\n",
    "#         gdict['batch_size']=gdict['batch_size']//gdict['world_size']\n",
    "    else:\n",
    "        gdict['world_size'],gdict['world_rank'],gdict['local_rank']=1,0,0\n",
    "    \n",
    "    ########################\n",
    "    ###### Set up directories #######\n",
    "    ### sync up so that time is the same for each GPU for DDP\n",
    "    if gdict['mode'] in ['fresh','fresh_load']:\n",
    "        ### Create prefix for foldername      \n",
    "        if gdict['world_rank']==0: ### For rank=0, create directory name string and make directories\n",
    "            dt_strg=datetime.now().strftime('%Y%m%d_%H%M%S') ## time format\n",
    "            dt_lst=[int(i) for i in dt_strg.split('_')] # List storing day and time            \n",
    "            dt_tnsr=torch.LongTensor(dt_lst).to(gdict['device'])  ## Create list to pass to other GPUs \n",
    "\n",
    "        else: dt_tnsr=torch.Tensor([0,0]).long().to(gdict['device'])\n",
    "        ### Pass directory name to other ranks\n",
    "        if gdict['distributed']: dist.broadcast(dt_tnsr, src=0)\n",
    "\n",
    "        gdict['save_dir']=gdict['op_loc']+str(int(dt_tnsr[0]))+'_'+str(int(dt_tnsr[1]))+'_'+gdict['run_suffix']\n",
    "        \n",
    "        if gdict['world_rank']==0: # Create directories for rank 0\n",
    "            ### Create directories\n",
    "            if not os.path.exists(gdict['save_dir']):\n",
    "                os.makedirs(gdict['save_dir']+'/models')\n",
    "                os.makedirs(gdict['save_dir']+'/images')\n",
    "                shutil.copy(gdict['config'],gdict['save_dir'])    \n",
    "    \n",
    "    elif gdict['mode']=='continue': ## For checkpointed runs\n",
    "        gdict['save_dir']=gdict['ip_fldr']\n",
    "        ### Read loss data\n",
    "        metrics_df=pd.read_pickle(gdict['save_dir']+'/df_metrics.pkle').astype(np.float64)\n",
    "   \n",
    "    ########################\n",
    "    ### Initialize random seed\n",
    "    \n",
    "    manualSeed = np.random.randint(1, 10000) if gdict['seed']=='random' else int(gdict['seed'])\n",
    "#     print(\"Seed\",manualSeed,gdict['world_rank'])\n",
    "    random.seed(manualSeed)\n",
    "    np.random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    \n",
    "    if gdict['deterministic']:\n",
    "        logging.info(\"Running with deterministic sequence. Performance will be slower\")\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "#         torch.backends.cudnn.enabled = False\n",
    "        torch.backends.cudnn.benchmark = False        \n",
    "    \n",
    "    ########################\n",
    "    if log:\n",
    "        ### Write all logging.info statements to stdout and log file\n",
    "        logfile=gdict['save_dir']+'/log.log'\n",
    "        if gdict['world_rank']==0:\n",
    "            logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "\n",
    "            Lg = logging.getLogger()\n",
    "            Lg.setLevel(logging.DEBUG)\n",
    "            lg_handler_file = logging.FileHandler(logfile)\n",
    "            lg_handler_stdout = logging.StreamHandler(sys.stdout)\n",
    "            Lg.addHandler(lg_handler_file)\n",
    "            Lg.addHandler(lg_handler_stdout)\n",
    "\n",
    "            logging.info('Args: {0}'.format(args))\n",
    "            logging.info('Start: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n",
    "        \n",
    "        if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "        if gdict['world_rank']!=0:\n",
    "                logging.basicConfig(level=logging.DEBUG, filename=logfile, filemode=\"a+\", format=\"%(asctime)-15s %(levelname)-8s %(message)s\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,gdict):\n",
    "        '''\n",
    "        Load training dataset and compute spectrum and histogram for a small batch of training and validation dataset.\n",
    "        '''\n",
    "        ## Load training dataset\n",
    "        t0a=time.time()\n",
    "        img=np.load(gdict['ip_fname'],mmap_mode='r')[:gdict['num_imgs']]\n",
    "    #     print(\"Shape of input file\",img.shape)\n",
    "        img=f_get_img_samples(img,gdict['world_rank'],gdict['world_size'])  \n",
    "\n",
    "        t_img=torch.from_numpy(img)\n",
    "        dataset=TensorDataset(t_img)\n",
    "        self.train_dataloader=DataLoader(dataset,batch_size=gdict['batch_size'],shuffle=True,num_workers=0,drop_last=True)\n",
    "        logging.info(\"Size of dataset for GPU %s : %s\"%(gdict['world_rank'],len(self.train_dataloader.dataset)))\n",
    "\n",
    "        t0b=time.time()\n",
    "        logging.info(\"Time for creating dataloader\",t0b-t0a,gdict['world_rank'])\n",
    "        \n",
    "        # Precompute spectrum and histogram for small training and validation data for computing losses\n",
    "        with torch.no_grad():\n",
    "            val_img=np.load(gdict['ip_fname'],mmap_mode='r')[-100:].copy()\n",
    "            t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "            # Precompute radial coordinates\n",
    "            r,ind=f_get_rad(val_img)\n",
    "            self.r,self.ind=r.to(gdict['device']),ind.to(gdict['device'])\n",
    "\n",
    "            # Compute\n",
    "            self.train_spec_mean,self.train_spec_var=f_torch_image_spectrum(f_invtransform(t_val_img),1,self.r,self.ind)\n",
    "            self.train_hist=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "            \n",
    "            # Repeat for validation dataset\n",
    "            val_img=np.load(gdict['ip_fname'],mmap_mode='r')[-200:-100].copy()\n",
    "            t_val_img=torch.from_numpy(val_img).to(gdict['device'])\n",
    "            \n",
    "            # Compute\n",
    "            self.val_spec_mean,self.val_spec_var=f_torch_image_spectrum(f_invtransform(t_val_img),1,self.r,self.ind)\n",
    "            self.val_hist=f_compute_hist(t_val_img,bins=gdict['bns'])\n",
    "            del val_img; del t_val_img; del img; del t_img;\n",
    "\n",
    "class GAN_model():\n",
    "    def __init__(self,gdict,print_model=False):\n",
    "    \n",
    "        def weights_init(m):\n",
    "            '''custom weights initialization called on netG and netD '''\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('Conv') != -1:\n",
    "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            elif classname.find('BatchNorm') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "        ## Choose model\n",
    "        Generator, Discriminator=f_get_model(gdict) ## Mod for cGAN\n",
    "        \n",
    "        # Create Generator\n",
    "        self.netG = Generator(gdict).to(gdict['device'])\n",
    "        self.netG.apply(weights_init)\n",
    "        # Create Discriminator\n",
    "        self.netD = Discriminator(gdict).to(gdict['device'])\n",
    "        self.netD.apply(weights_init)\n",
    "\n",
    "        if print_model:\n",
    "            if gdict['world_rank']==0:\n",
    "                print(self.netG)\n",
    "            #     summary(netG,(1,1,64))\n",
    "                print(self.netD)\n",
    "            #     summary(netD,(1,128,128))\n",
    "                print(\"Number of GPUs used %s\"%(gdict['ngpu']))\n",
    "\n",
    "        if (gdict['multi-gpu']):\n",
    "            if not gdict['distributed']:\n",
    "                self.netG = nn.DataParallel(self.netG, list(range(gdict['ngpu'])))\n",
    "                self.netD = nn.DataParallel(self.netD, list(range(gdict['ngpu'])))\n",
    "            else:\n",
    "                self.netG=DistributedDataParallel(self.netG,device_ids=[gdict['local_rank']],output_device=[gdict['local_rank']])\n",
    "                self.netD=DistributedDataParallel(self.netD,device_ids=[gdict['local_rank']],output_device=[gdict['local_rank']])\n",
    "\n",
    "        #### Initialize networks ####\n",
    "        # self.criterion = nn.BCELoss()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.optimizerD = optim.Adam(self.netD.parameters(), lr=gdict['learn_rate_d'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        self.optimizerG = optim.Adam(self.netG.parameters(), lr=gdict['learn_rate_g'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "        \n",
    "        if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "        if gdict['mode']=='fresh':\n",
    "            iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10 \n",
    "            \n",
    "        elif gdict['mode']=='continue':\n",
    "            iters,start_epoch,best_chi1,best_chi2,self.netD,self.optimizerD,self.netG,self.optimizerG=f_load_checkpoint(gdict['save_dir']+'/models/checkpoint_last.tar',\\\n",
    "                                                                                                                        self.netG,self.netD,self.optimizerG,self.optimizerD,gdict) \n",
    "            if gdict['world_rank']==0: logging.info(\"\\nContinuing existing run. Loading checkpoint with epoch {0} and step {1}\\n\".format(start_epoch,iters))\n",
    "            if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "            start_epoch+=1  ## Start with the next epoch \n",
    "        \n",
    "        elif gdict['mode']=='fresh_load':\n",
    "            iters,start_epoch,best_chi1,best_chi2,self.netD,self.optimizerD,self.netG,self.optimizerG=f_load_checkpoint(gdict['chkpt_file'],\\\n",
    "                                                                                                                        self.netG,self.netD,self.optimizerG,self.optimizerD,gdict) \n",
    "            if gdict['world_rank']==0: logging.info(\"Fresh run loading checkpoint file {0}\".format(gdict['chkpt_file']))\n",
    "#             if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "            iters,start_epoch,best_chi1,best_chi2=0,0,1e10,1e10 \n",
    "        \n",
    "        ## Add to gdict\n",
    "        for key,val in zip(['best_chi1','best_chi2','iters','start_epoch'],[best_chi1,best_chi2,iters,start_epoch]): gdict[key]=val\n",
    "        \n",
    "        ## Set up learn rate scheduler\n",
    "        lr_stepsize=int((gdict['num_imgs'])/(gdict['batch_size']*gdict['world_size'])) # convert epoch number to step \n",
    "        lr_d_epochs=[i*lr_stepsize for i in gdict['lr_d_epochs']] \n",
    "        lr_g_epochs=[i*lr_stepsize for i in gdict['lr_g_epochs']]\n",
    "        self.schedulerD = optim.lr_scheduler.MultiStepLR(self.optimizerD, milestones=lr_d_epochs,gamma=gdict['lr_d_gamma'])\n",
    "        self.schedulerG = optim.lr_scheduler.MultiStepLR(self.optimizerG, milestones=lr_g_epochs,gamma=gdict['lr_g_gamma'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using DDP\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>) tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0', grad_fn=<StackBackward>)\n",
      "Nans in spectrum tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0') tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "(100, 1, 64, 64, 64)\n",
      "Image saved in  best_spec_epoch-4_step-48.npy\n",
      "(100, 1, 64, 64, 64)\n",
      "Image saved in  best_hist_epoch-2_step-22.npy\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### Main code #######\n",
    "#########################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    jpt=False\n",
    "    jpt=True ##(different for jupyter notebook)\n",
    "    t0=time.time()\n",
    "    t0=time.time()\n",
    "    args=f_parse_args() if not jpt else f_manual_add_argparse()\n",
    "\n",
    "    #################################\n",
    "    ### Set up global dictionary###\n",
    "    gdict={}\n",
    "    gdict=f_init_gdict(args,gdict)\n",
    "#     gdict['num_imgs']=200\n",
    "\n",
    "    if jpt: ## override for jpt nbks\n",
    "        gdict['num_imgs']=400\n",
    "        gdict['run_suffix']='nb_test'\n",
    "        \n",
    "    ### Set up metrics dataframe\n",
    "    cols=['step','epoch','Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','spec_chi','hist_chi','gp_loss','fm_loss','D(x)','D_G_z1','D_G_z2','time']\n",
    "    metrics_df=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    # Setup\n",
    "    metrics_df=f_setup(gdict,metrics_df,log=(not jpt))\n",
    "    \n",
    "    ## Build GAN\n",
    "    gan_model=GAN_model(gdict,False)\n",
    "    fixed_noise = torch.randn(gdict['op_size'], 1, 1, 1, gdict['nz'], device=gdict['device']) #Latent vectors to view G progress    # Mod for 3D\n",
    "\n",
    "    if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "    ## Load data and precompute\n",
    "    Dset=Dataset(gdict)\n",
    "    \n",
    "    #################################\n",
    "    ########## Train loop and save metrics and images ######\n",
    "    if gdict['distributed']:  try_barrier(gdict['world_rank'])\n",
    "\n",
    "    if gdict['world_rank']==0: \n",
    "        logging.info(gdict)\n",
    "        logging.info(\"Starting Training Loop...\")\n",
    "        \n",
    "    f_train_loop(gan_model,Dset,metrics_df,gdict,fixed_noise)\n",
    "    \n",
    "    if gdict['world_rank']==0: ## Generate images for best saved models ######\n",
    "        op_loc=gdict['save_dir']+'/images/'\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_spec.tar'\n",
    "        f_gen_images(gdict,gan_model.netG,gan_model.optimizerG,ip_fname,op_loc,op_strg='best_spec',op_size=32)\n",
    "        ip_fname=gdict['save_dir']+'/models/checkpoint_best_hist.tar'\n",
    "        f_gen_images(gdict,gan_model.netG,gan_model.optimizerG,ip_fname,op_loc,op_strg='best_hist',op_size=32)\n",
    "    \n",
    "    tf=time.time()\n",
    "    logging.info(\"Total time %s\"%(tf-t0))\n",
    "    logging.info('End: %s'%(datetime.now().strftime('%Y-%m-%d  %H:%M:%S')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>epoch</th>\n",
       "      <th>Dreal</th>\n",
       "      <th>Dfake</th>\n",
       "      <th>Dfull</th>\n",
       "      <th>G_adv</th>\n",
       "      <th>G_full</th>\n",
       "      <th>spec_loss</th>\n",
       "      <th>hist_loss</th>\n",
       "      <th>spec_chi</th>\n",
       "      <th>hist_chi</th>\n",
       "      <th>gp_loss</th>\n",
       "      <th>fm_loss</th>\n",
       "      <th>D(x)</th>\n",
       "      <th>D_G_z1</th>\n",
       "      <th>D_G_z2</th>\n",
       "      <th>time</th>\n",
       "      <th>lr_d</th>\n",
       "      <th>lr_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840994</td>\n",
       "      <td>0.715251</td>\n",
       "      <td>1.556245</td>\n",
       "      <td>22.584732</td>\n",
       "      <td>33.839798</td>\n",
       "      <td>11.255068</td>\n",
       "      <td>1.612284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.07203</td>\n",
       "      <td>-0.208424</td>\n",
       "      <td>-17.184401</td>\n",
       "      <td>0.103338</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.299035</td>\n",
       "      <td>2.241939</td>\n",
       "      <td>3.540974</td>\n",
       "      <td>28.161259</td>\n",
       "      <td>39.285881</td>\n",
       "      <td>11.12462</td>\n",
       "      <td>1.21745</td>\n",
       "      <td>11.183544</td>\n",
       "      <td>0.758419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.140594</td>\n",
       "      <td>2.899389</td>\n",
       "      <td>-28.161259</td>\n",
       "      <td>0.103051</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.559894</td>\n",
       "      <td>5.898785</td>\n",
       "      <td>16.458679</td>\n",
       "      <td>4.830589</td>\n",
       "      <td>15.81959</td>\n",
       "      <td>10.989</td>\n",
       "      <td>1.072545</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.118342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-13.925358</td>\n",
       "      <td>-22.697987</td>\n",
       "      <td>-4.822292</td>\n",
       "      <td>0.103292</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49855</td>\n",
       "      <td>0.666836</td>\n",
       "      <td>1.165386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.048596</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.91501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>-1.888329</td>\n",
       "      <td>23.090822</td>\n",
       "      <td>0.102724</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.799689</td>\n",
       "      <td>17.408419</td>\n",
       "      <td>21.208107</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.064357</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.919925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.109097</td>\n",
       "      <td>23.190767</td>\n",
       "      <td>4.118836</td>\n",
       "      <td>0.101315</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.962665</td>\n",
       "      <td>3.645362</td>\n",
       "      <td>5.608027</td>\n",
       "      <td>1.896615</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.058407</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.995578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.117305</td>\n",
       "      <td>4.861509</td>\n",
       "      <td>-1.73393</td>\n",
       "      <td>0.101913</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.791798</td>\n",
       "      <td>1.431333</td>\n",
       "      <td>7.104887</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.111167</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.144822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23758</td>\n",
       "      <td>0.376904</td>\n",
       "      <td>-7.104041</td>\n",
       "      <td>0.101668</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.044566</td>\n",
       "      <td>0.753196</td>\n",
       "      <td>2.797762</td>\n",
       "      <td>2.649329</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.279575</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.351185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.029798</td>\n",
       "      <td>-3.322231</td>\n",
       "      <td>-2.571097</td>\n",
       "      <td>0.101727</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339161</td>\n",
       "      <td>3.919532</td>\n",
       "      <td>4.258693</td>\n",
       "      <td>5.264164</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.356508</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.388227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.735115</td>\n",
       "      <td>5.148591</td>\n",
       "      <td>-5.258847</td>\n",
       "      <td>0.101463</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.161517</td>\n",
       "      <td>0.857188</td>\n",
       "      <td>3.018704</td>\n",
       "      <td>8.289993</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.382153</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.442253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.884884</td>\n",
       "      <td>0.559244</td>\n",
       "      <td>-8.289726</td>\n",
       "      <td>0.101667</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.607818</td>\n",
       "      <td>1.156635</td>\n",
       "      <td>2.764453</td>\n",
       "      <td>2.534675</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.355521</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.397588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.580127</td>\n",
       "      <td>-2.747048</td>\n",
       "      <td>-2.409121</td>\n",
       "      <td>0.103268</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2.512406</td>\n",
       "      <td>4.59638</td>\n",
       "      <td>7.108786</td>\n",
       "      <td>0.074068</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.260334</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.309304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.625006</td>\n",
       "      <td>6.169009</td>\n",
       "      <td>2.574606</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4.045979</td>\n",
       "      <td>2.661495</td>\n",
       "      <td>6.707474</td>\n",
       "      <td>1.427036</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.170642</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.115715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.642909</td>\n",
       "      <td>3.548816</td>\n",
       "      <td>-1.1124</td>\n",
       "      <td>0.103769</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60083</td>\n",
       "      <td>6.019423</td>\n",
       "      <td>6.620253</td>\n",
       "      <td>0.07088</td>\n",
       "      <td>11.764748</td>\n",
       "      <td>11.693868</td>\n",
       "      <td>1.066364</td>\n",
       "      <td>11.816168</td>\n",
       "      <td>1.165101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.340359</td>\n",
       "      <td>8.010704</td>\n",
       "      <td>2.682267</td>\n",
       "      <td>0.103745</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074608</td>\n",
       "      <td>4.501818</td>\n",
       "      <td>4.576426</td>\n",
       "      <td>0.35901</td>\n",
       "      <td>11.568026</td>\n",
       "      <td>11.209015</td>\n",
       "      <td>0.936369</td>\n",
       "      <td>11.267967</td>\n",
       "      <td>1.048636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.562078</td>\n",
       "      <td>5.977832</td>\n",
       "      <td>0.875132</td>\n",
       "      <td>0.103367</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321476</td>\n",
       "      <td>5.722028</td>\n",
       "      <td>6.043504</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>11.214976</td>\n",
       "      <td>11.213362</td>\n",
       "      <td>0.680569</td>\n",
       "      <td>11.268319</td>\n",
       "      <td>0.822254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.049763</td>\n",
       "      <td>7.693453</td>\n",
       "      <td>6.458798</td>\n",
       "      <td>0.103181</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5.345942</td>\n",
       "      <td>5.126259</td>\n",
       "      <td>10.472201</td>\n",
       "      <td>0.00398</td>\n",
       "      <td>11.128026</td>\n",
       "      <td>11.124045</td>\n",
       "      <td>0.359812</td>\n",
       "      <td>11.196077</td>\n",
       "      <td>0.347672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.328562</td>\n",
       "      <td>6.839894</td>\n",
       "      <td>5.533284</td>\n",
       "      <td>0.103117</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2.530772</td>\n",
       "      <td>3.453708</td>\n",
       "      <td>5.984479</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.280335</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.24733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.939397</td>\n",
       "      <td>4.717561</td>\n",
       "      <td>3.469499</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.441267</td>\n",
       "      <td>2.805986</td>\n",
       "      <td>3.247254</td>\n",
       "      <td>0.108195</td>\n",
       "      <td>12.691287</td>\n",
       "      <td>12.583092</td>\n",
       "      <td>0.203713</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.086006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.749582</td>\n",
       "      <td>3.565971</td>\n",
       "      <td>2.197622</td>\n",
       "      <td>0.103179</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>7.762487</td>\n",
       "      <td>2.071068</td>\n",
       "      <td>9.833555</td>\n",
       "      <td>0.254836</td>\n",
       "      <td>11.430453</td>\n",
       "      <td>11.175617</td>\n",
       "      <td>0.19257</td>\n",
       "      <td>11.492989</td>\n",
       "      <td>-0.013188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.490625</td>\n",
       "      <td>2.540437</td>\n",
       "      <td>1.328907</td>\n",
       "      <td>0.10347</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6.399721</td>\n",
       "      <td>3.894982</td>\n",
       "      <td>10.294703</td>\n",
       "      <td>0.018688</td>\n",
       "      <td>11.201758</td>\n",
       "      <td>11.183071</td>\n",
       "      <td>0.499532</td>\n",
       "      <td>11.226034</td>\n",
       "      <td>0.360962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.766015</td>\n",
       "      <td>5.194318</td>\n",
       "      <td>4.005749</td>\n",
       "      <td>0.104187</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.613512</td>\n",
       "      <td>5.652824</td>\n",
       "      <td>6.266336</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>11.133889</td>\n",
       "      <td>11.131786</td>\n",
       "      <td>0.690429</td>\n",
       "      <td>11.204365</td>\n",
       "      <td>0.691253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.496244</td>\n",
       "      <td>7.477771</td>\n",
       "      <td>6.174532</td>\n",
       "      <td>0.103933</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1.341518</td>\n",
       "      <td>4.226704</td>\n",
       "      <td>5.568222</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>11.375445</td>\n",
       "      <td>11.362945</td>\n",
       "      <td>0.61466</td>\n",
       "      <td>11.295197</td>\n",
       "      <td>0.422998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367482</td>\n",
       "      <td>5.761881</td>\n",
       "      <td>4.438017</td>\n",
       "      <td>0.103761</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4.820993</td>\n",
       "      <td>3.706799</td>\n",
       "      <td>8.527792</td>\n",
       "      <td>0.02862</td>\n",
       "      <td>11.200165</td>\n",
       "      <td>11.171545</td>\n",
       "      <td>0.719856</td>\n",
       "      <td>11.265398</td>\n",
       "      <td>0.516008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.346298</td>\n",
       "      <td>4.907294</td>\n",
       "      <td>3.556771</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944917</td>\n",
       "      <td>4.040998</td>\n",
       "      <td>4.985914</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>11.150699</td>\n",
       "      <td>11.127851</td>\n",
       "      <td>0.932732</td>\n",
       "      <td>11.172544</td>\n",
       "      <td>0.834833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.55501</td>\n",
       "      <td>5.158665</td>\n",
       "      <td>3.869469</td>\n",
       "      <td>0.103921</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5.313784</td>\n",
       "      <td>5.723547</td>\n",
       "      <td>11.037331</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>11.081049</td>\n",
       "      <td>11.079434</td>\n",
       "      <td>1.101739</td>\n",
       "      <td>11.148342</td>\n",
       "      <td>1.104033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.165342</td>\n",
       "      <td>7.697487</td>\n",
       "      <td>6.450365</td>\n",
       "      <td>0.104235</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>1.544312</td>\n",
       "      <td>4.675431</td>\n",
       "      <td>6.219743</td>\n",
       "      <td>0.008747</td>\n",
       "      <td>11.318174</td>\n",
       "      <td>11.309427</td>\n",
       "      <td>1.014285</td>\n",
       "      <td>11.3016</td>\n",
       "      <td>0.838141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.954718</td>\n",
       "      <td>6.053892</td>\n",
       "      <td>4.783102</td>\n",
       "      <td>0.103127</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0.907214</td>\n",
       "      <td>3.934793</td>\n",
       "      <td>4.842007</td>\n",
       "      <td>0.02099</td>\n",
       "      <td>11.143877</td>\n",
       "      <td>11.122888</td>\n",
       "      <td>1.031171</td>\n",
       "      <td>11.214119</td>\n",
       "      <td>0.912131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.30056</td>\n",
       "      <td>5.189731</td>\n",
       "      <td>3.856922</td>\n",
       "      <td>0.103374</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>8.303823</td>\n",
       "      <td>5.327021</td>\n",
       "      <td>13.630844</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>11.134513</td>\n",
       "      <td>11.131445</td>\n",
       "      <td>1.105303</td>\n",
       "      <td>11.181268</td>\n",
       "      <td>1.143089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.973653</td>\n",
       "      <td>7.087047</td>\n",
       "      <td>5.796177</td>\n",
       "      <td>0.103264</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>3.492603</td>\n",
       "      <td>4.285252</td>\n",
       "      <td>7.777855</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>11.080451</td>\n",
       "      <td>11.06967</td>\n",
       "      <td>1.032933</td>\n",
       "      <td>11.12428</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.789773</td>\n",
       "      <td>5.783184</td>\n",
       "      <td>4.536328</td>\n",
       "      <td>0.104059</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2.134353</td>\n",
       "      <td>4.2134</td>\n",
       "      <td>6.347753</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>11.119074</td>\n",
       "      <td>11.10538</td>\n",
       "      <td>1.055872</td>\n",
       "      <td>11.162046</td>\n",
       "      <td>0.819675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.358664</td>\n",
       "      <td>5.591814</td>\n",
       "      <td>4.333258</td>\n",
       "      <td>0.103972</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>6.217463</td>\n",
       "      <td>6.025445</td>\n",
       "      <td>12.242908</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>11.036731</td>\n",
       "      <td>11.035299</td>\n",
       "      <td>1.220333</td>\n",
       "      <td>11.123133</td>\n",
       "      <td>1.360623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.983981</td>\n",
       "      <td>8.182545</td>\n",
       "      <td>6.825007</td>\n",
       "      <td>0.103837</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2.298704</td>\n",
       "      <td>4.540798</td>\n",
       "      <td>6.839501</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>11.055281</td>\n",
       "      <td>11.045465</td>\n",
       "      <td>1.138323</td>\n",
       "      <td>11.062212</td>\n",
       "      <td>1.258932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.103831</td>\n",
       "      <td>6.219594</td>\n",
       "      <td>4.836287</td>\n",
       "      <td>0.104198</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>4.407728</td>\n",
       "      <td>4.646824</td>\n",
       "      <td>9.054552</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>11.075969</td>\n",
       "      <td>11.069096</td>\n",
       "      <td>1.168882</td>\n",
       "      <td>11.119804</td>\n",
       "      <td>1.18589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.917669</td>\n",
       "      <td>6.209856</td>\n",
       "      <td>4.977396</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2.079747</td>\n",
       "      <td>3.731516</td>\n",
       "      <td>5.811263</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>11.029324</td>\n",
       "      <td>11.003066</td>\n",
       "      <td>1.093294</td>\n",
       "      <td>11.09104</td>\n",
       "      <td>0.916625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.934727</td>\n",
       "      <td>4.869924</td>\n",
       "      <td>3.663758</td>\n",
       "      <td>0.103584</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>3.784305</td>\n",
       "      <td>5.089522</td>\n",
       "      <td>8.873827</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>10.990738</td>\n",
       "      <td>10.986725</td>\n",
       "      <td>1.290748</td>\n",
       "      <td>11.135329</td>\n",
       "      <td>1.482544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.324183</td>\n",
       "      <td>6.861547</td>\n",
       "      <td>5.571341</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704188</td>\n",
       "      <td>1.901862</td>\n",
       "      <td>2.60605</td>\n",
       "      <td>0.27652</td>\n",
       "      <td>12.76205</td>\n",
       "      <td>12.48553</td>\n",
       "      <td>1.30601</td>\n",
       "      <td>11.845675</td>\n",
       "      <td>1.399745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.476755</td>\n",
       "      <td>2.529946</td>\n",
       "      <td>1.206748</td>\n",
       "      <td>0.103489</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>7.195691</td>\n",
       "      <td>2.779227</td>\n",
       "      <td>9.974918</td>\n",
       "      <td>0.082462</td>\n",
       "      <td>11.380715</td>\n",
       "      <td>11.298253</td>\n",
       "      <td>1.384943</td>\n",
       "      <td>11.338472</td>\n",
       "      <td>1.412411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.301804</td>\n",
       "      <td>3.756595</td>\n",
       "      <td>2.485371</td>\n",
       "      <td>0.103236</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>1.682376</td>\n",
       "      <td>6.775151</td>\n",
       "      <td>8.457526</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>11.052495</td>\n",
       "      <td>11.052113</td>\n",
       "      <td>1.514614</td>\n",
       "      <td>11.076221</td>\n",
       "      <td>1.467451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>9.087991</td>\n",
       "      <td>7.910095</td>\n",
       "      <td>0.103083</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>7.232984</td>\n",
       "      <td>4.877656</td>\n",
       "      <td>12.11064</td>\n",
       "      <td>0.00556</td>\n",
       "      <td>11.702019</td>\n",
       "      <td>11.696458</td>\n",
       "      <td>1.494106</td>\n",
       "      <td>12.064867</td>\n",
       "      <td>1.514915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.128729</td>\n",
       "      <td>6.392653</td>\n",
       "      <td>5.230658</td>\n",
       "      <td>0.102993</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>2.443495</td>\n",
       "      <td>2.808254</td>\n",
       "      <td>5.251749</td>\n",
       "      <td>0.082987</td>\n",
       "      <td>11.251769</td>\n",
       "      <td>11.168782</td>\n",
       "      <td>1.391978</td>\n",
       "      <td>11.237629</td>\n",
       "      <td>1.37048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.327753</td>\n",
       "      <td>3.562088</td>\n",
       "      <td>2.481458</td>\n",
       "      <td>0.104078</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>2.64913</td>\n",
       "      <td>4.889613</td>\n",
       "      <td>7.538743</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>11.080647</td>\n",
       "      <td>11.075848</td>\n",
       "      <td>1.333035</td>\n",
       "      <td>11.10035</td>\n",
       "      <td>1.279308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.429281</td>\n",
       "      <td>6.502707</td>\n",
       "      <td>5.342144</td>\n",
       "      <td>0.103256</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>3.842153</td>\n",
       "      <td>3.509541</td>\n",
       "      <td>7.351694</td>\n",
       "      <td>0.036724</td>\n",
       "      <td>11.075215</td>\n",
       "      <td>11.038491</td>\n",
       "      <td>1.202407</td>\n",
       "      <td>11.078953</td>\n",
       "      <td>1.225515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.576773</td>\n",
       "      <td>4.572283</td>\n",
       "      <td>3.384054</td>\n",
       "      <td>0.103039</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>3.432261</td>\n",
       "      <td>5.499122</td>\n",
       "      <td>8.931383</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>11.028139</td>\n",
       "      <td>11.025966</td>\n",
       "      <td>1.355862</td>\n",
       "      <td>11.100739</td>\n",
       "      <td>1.387352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.449967</td>\n",
       "      <td>7.413724</td>\n",
       "      <td>6.19753</td>\n",
       "      <td>0.103042</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>4.478375</td>\n",
       "      <td>4.992278</td>\n",
       "      <td>9.470654</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>11.07872</td>\n",
       "      <td>11.074603</td>\n",
       "      <td>1.333596</td>\n",
       "      <td>11.058439</td>\n",
       "      <td>1.392644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.637343</td>\n",
       "      <td>6.79369</td>\n",
       "      <td>5.580664</td>\n",
       "      <td>0.103903</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>7.070223</td>\n",
       "      <td>4.659056</td>\n",
       "      <td>11.729279</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>11.002435</td>\n",
       "      <td>10.994049</td>\n",
       "      <td>1.341646</td>\n",
       "      <td>11.022075</td>\n",
       "      <td>1.278337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.020784</td>\n",
       "      <td>6.054476</td>\n",
       "      <td>4.849959</td>\n",
       "      <td>0.103731</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>3.012581</td>\n",
       "      <td>4.121998</td>\n",
       "      <td>7.134579</td>\n",
       "      <td>0.013492</td>\n",
       "      <td>11.081608</td>\n",
       "      <td>11.068116</td>\n",
       "      <td>1.336769</td>\n",
       "      <td>11.097574</td>\n",
       "      <td>1.351441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.572943</td>\n",
       "      <td>5.560693</td>\n",
       "      <td>4.324705</td>\n",
       "      <td>0.103197</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>4.416131</td>\n",
       "      <td>3.717647</td>\n",
       "      <td>8.133778</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>11.000645</td>\n",
       "      <td>10.969846</td>\n",
       "      <td>1.392597</td>\n",
       "      <td>11.024275</td>\n",
       "      <td>1.380873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.165476</td>\n",
       "      <td>5.056445</td>\n",
       "      <td>3.983271</td>\n",
       "      <td>0.103438</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>1.867409</td>\n",
       "      <td>5.564745</td>\n",
       "      <td>7.432154</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>10.976764</td>\n",
       "      <td>10.97476</td>\n",
       "      <td>1.465851</td>\n",
       "      <td>10.999959</td>\n",
       "      <td>1.551714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.787785</td>\n",
       "      <td>7.451334</td>\n",
       "      <td>6.222286</td>\n",
       "      <td>0.103916</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>2.30946</td>\n",
       "      <td>4.922924</td>\n",
       "      <td>7.232385</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.525691</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.423766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.123101</td>\n",
       "      <td>6.466078</td>\n",
       "      <td>5.019753</td>\n",
       "      <td>0.101843</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.610977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step epoch      Dreal      Dfake      Dfull      G_adv     G_full  \\\n",
       "0     0     0   0.840994   0.715251   1.556245  22.584732  33.839798   \n",
       "1     1     0   1.299035   2.241939   3.540974  28.161259  39.285881   \n",
       "2     2     0  10.559894   5.898785  16.458679   4.830589   15.81959   \n",
       "3     3     0    0.49855   0.666836   1.165386        0.0        inf   \n",
       "4     4     0   3.799689  17.408419  21.208107   0.016709        inf   \n",
       "5     5     0   1.962665   3.645362   5.608027   1.896615        inf   \n",
       "6     6     0   0.639535   0.791798   1.431333   7.104887        inf   \n",
       "7     7     0   2.044566   0.753196   2.797762   2.649329        inf   \n",
       "8     8     0   0.339161   3.919532   4.258693   5.264164        inf   \n",
       "9     9     0   2.161517   0.857188   3.018704   8.289993        inf   \n",
       "10   10     1   1.607818   1.156635   2.764453   2.534675        inf   \n",
       "11   11     1   2.512406    4.59638   7.108786   0.074068        inf   \n",
       "12   12     1   4.045979   2.661495   6.707474   1.427036        inf   \n",
       "13   13     1    0.60083   6.019423   6.620253    0.07088  11.764748   \n",
       "14   14     1   0.074608   4.501818   4.576426    0.35901  11.568026   \n",
       "15   15     1   0.321476   5.722028   6.043504   0.001615  11.214976   \n",
       "16   16     1   5.345942   5.126259  10.472201    0.00398  11.128026   \n",
       "17   17     1   2.530772   3.453708   5.984479   0.035916        inf   \n",
       "18   18     1   0.441267   2.805986   3.247254   0.108195  12.691287   \n",
       "19   19     1   7.762487   2.071068   9.833555   0.254836  11.430453   \n",
       "20   20     2   6.399721   3.894982  10.294703   0.018688  11.201758   \n",
       "21   21     2   0.613512   5.652824   6.266336   0.002103  11.133889   \n",
       "22   22     2   1.341518   4.226704   5.568222     0.0125  11.375445   \n",
       "23   23     2   4.820993   3.706799   8.527792    0.02862  11.200165   \n",
       "24   24     2   0.944917   4.040998   4.985914   0.022848  11.150699   \n",
       "25   25     2   5.313784   5.723547  11.037331   0.001615  11.081049   \n",
       "26   26     2   1.544312   4.675431   6.219743   0.008747  11.318174   \n",
       "27   27     2   0.907214   3.934793   4.842007    0.02099  11.143877   \n",
       "28   28     2   8.303823   5.327021  13.630844   0.003068  11.134513   \n",
       "29   29     2   3.492603   4.285252   7.777855   0.010782  11.080451   \n",
       "30   30     3   2.134353     4.2134   6.347753   0.013694  11.119074   \n",
       "31   31     3   6.217463   6.025445  12.242908   0.001432  11.036731   \n",
       "32   32     3   2.298704   4.540798   6.839501   0.009815  11.055281   \n",
       "33   33     3   4.407728   4.646824   9.054552   0.006873  11.075969   \n",
       "34   34     3   2.079747   3.731516   5.811263   0.026257  11.029324   \n",
       "35   35     3   3.784305   5.089522   8.873827   0.004013  10.990738   \n",
       "36   36     3   0.704188   1.901862    2.60605    0.27652   12.76205   \n",
       "37   37     3   7.195691   2.779227   9.974918   0.082462  11.380715   \n",
       "38   38     3   1.682376   6.775151   8.457526   0.000383  11.052495   \n",
       "39   39     3   7.232984   4.877656   12.11064    0.00556  11.702019   \n",
       "40   40     4   2.443495   2.808254   5.251749   0.082987  11.251769   \n",
       "41   41     4    2.64913   4.889613   7.538743   0.004799  11.080647   \n",
       "42   42     4   3.842153   3.509541   7.351694   0.036724  11.075215   \n",
       "43   43     4   3.432261   5.499122   8.931383   0.002173  11.028139   \n",
       "44   44     4   4.478375   4.992278   9.470654   0.004117   11.07872   \n",
       "45   45     4   7.070223   4.659056  11.729279   0.008385  11.002435   \n",
       "46   46     4   3.012581   4.121998   7.134579   0.013492  11.081608   \n",
       "47   47     4   4.416131   3.717647   8.133778   0.030799  11.000645   \n",
       "48   48     4   1.867409   5.564745   7.432154   0.002003  10.976764   \n",
       "49   49     4    2.30946   4.922924   7.232385   0.006759        inf   \n",
       "50  NaN   NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "    spec_loss hist_loss   spec_chi  hist_chi gp_loss fm_loss       D(x)  \\\n",
       "0   11.255068  1.612284        NaN       NaN     NaN     NaN   -0.07203   \n",
       "1    11.12462   1.21745  11.183544  0.758419     NaN     NaN   6.140594   \n",
       "2      10.989  1.072545        inf  1.118342     NaN     NaN -13.925358   \n",
       "3         inf  1.048596        inf   0.91501     NaN     NaN   0.014235   \n",
       "4         inf  1.064357        inf  0.919925     NaN     NaN  16.109097   \n",
       "5         inf  1.058407        inf  0.995578     NaN     NaN   9.117305   \n",
       "6         inf  1.111167        inf  1.144822     NaN     NaN    3.23758   \n",
       "7         inf  1.279575        inf  1.351185     NaN     NaN  -3.029798   \n",
       "8         inf  1.356508        inf  1.388227     NaN     NaN   2.735115   \n",
       "9         inf  1.382153        inf  1.442253     NaN     NaN  -0.884884   \n",
       "10        inf  1.355521        inf  1.397588     NaN     NaN   6.580127   \n",
       "11        inf  1.260334        inf  1.309304     NaN     NaN  -1.625006   \n",
       "12        inf  1.170642        inf  1.115715     NaN     NaN  -5.642909   \n",
       "13  11.693868  1.066364  11.816168  1.165101     NaN     NaN   0.340359   \n",
       "14  11.209015  0.936369  11.267967  1.048636     NaN     NaN  -0.562078   \n",
       "15  11.213362  0.680569  11.268319  0.822254     NaN     NaN   3.049763   \n",
       "16  11.124045  0.359812  11.196077  0.347672     NaN     NaN  -5.328562   \n",
       "17        inf  0.280335        inf   0.24733     NaN     NaN   7.939397   \n",
       "18  12.583092  0.203713        inf  0.086006     NaN     NaN   2.749582   \n",
       "19  11.175617   0.19257  11.492989 -0.013188     NaN     NaN  -3.490625   \n",
       "20  11.183071  0.499532  11.226034  0.360962     NaN     NaN  -5.766015   \n",
       "21  11.131786  0.690429  11.204365  0.691253     NaN     NaN   1.496244   \n",
       "22  11.362945   0.61466  11.295197  0.422998     NaN     NaN   0.367482   \n",
       "23  11.171545  0.719856  11.265398  0.516008     NaN     NaN  -4.346298   \n",
       "24  11.127851  0.932732  11.172544  0.834833     NaN     NaN   -0.55501   \n",
       "25  11.079434  1.101739  11.148342  1.104033     NaN     NaN  -8.165342   \n",
       "26  11.309427  1.014285    11.3016  0.838141     NaN     NaN   2.954718   \n",
       "27  11.122888  1.031171  11.214119  0.912131     NaN     NaN   -1.30056   \n",
       "28  11.131445  1.105303  11.181268  1.143089     NaN     NaN  -5.973653   \n",
       "29   11.06967  1.032933   11.12428  0.859296     NaN     NaN  -4.789773   \n",
       "30   11.10538  1.055872  11.162046  0.819675     NaN     NaN  -3.358664   \n",
       "31  11.035299  1.220333  11.123133  1.360623     NaN     NaN  -3.983981   \n",
       "32  11.045465  1.138323  11.062212  1.258932     NaN     NaN  -2.103831   \n",
       "33  11.069096  1.168882  11.119804   1.18589     NaN     NaN  -4.917669   \n",
       "34  11.003066  1.093294   11.09104  0.916625     NaN     NaN  -2.934727   \n",
       "35  10.986725  1.290748  11.135329  1.482544     NaN     NaN  -2.324183   \n",
       "36   12.48553   1.30601  11.845675  1.399745     NaN     NaN  -3.476755   \n",
       "37  11.298253  1.384943  11.338472  1.412411     NaN     NaN  -8.301804   \n",
       "38  11.052113  1.514614  11.076221  1.467451     NaN     NaN   0.247414   \n",
       "39  11.696458  1.494106  12.064867  1.514915     NaN     NaN  -5.128729   \n",
       "40  11.168782  1.391978  11.237629   1.37048     NaN     NaN  -3.327753   \n",
       "41  11.075848  1.333035   11.10035  1.279308     NaN     NaN  -4.429281   \n",
       "42  11.038491  1.202407  11.078953  1.225515     NaN     NaN  -6.576773   \n",
       "43  11.025966  1.355862  11.100739  1.387352     NaN     NaN  -4.449967   \n",
       "44  11.074603  1.333596  11.058439  1.392644     NaN     NaN  -8.637343   \n",
       "45  10.994049  1.341646  11.022075  1.278337     NaN     NaN  -2.020784   \n",
       "46  11.068116  1.336769  11.097574  1.351441     NaN     NaN  -2.572943   \n",
       "47  10.969846  1.392597  11.024275  1.380873     NaN     NaN  -3.165476   \n",
       "48   10.97476  1.465851  10.999959  1.551714     NaN     NaN  -1.787785   \n",
       "49        inf  1.525691        inf  1.423766     NaN     NaN  -3.123101   \n",
       "50        NaN       NaN        inf  1.610977     NaN     NaN        NaN   \n",
       "\n",
       "       D_G_z1     D_G_z2      time      lr_d   lr_g  \n",
       "0   -0.208424 -17.184401  0.103338  0.001000  0.001  \n",
       "1    2.899389 -28.161259  0.103051  0.001000  0.001  \n",
       "2  -22.697987  -4.822292  0.103292  0.001000  0.001  \n",
       "3   -1.888329  23.090822  0.102724  0.001000  0.001  \n",
       "4   23.190767   4.118836  0.101315  0.001000  0.001  \n",
       "5    4.861509   -1.73393  0.101913  0.000250  0.001  \n",
       "6    0.376904  -7.104041  0.101668  0.000250  0.001  \n",
       "7   -3.322231  -2.571097  0.101727  0.000250  0.001  \n",
       "8    5.148591  -5.258847  0.101463  0.000250  0.001  \n",
       "9    0.559244  -8.289726  0.101667  0.000250  0.001  \n",
       "10  -2.747048  -2.409121  0.103268  0.000063  0.001  \n",
       "11   6.169009   2.574606  0.104421  0.000063  0.001  \n",
       "12   3.548816    -1.1124  0.103769  0.000063  0.001  \n",
       "13   8.010704   2.682267  0.103745  0.000063  0.001  \n",
       "14   5.977832   0.875132  0.103367  0.000063  0.001  \n",
       "15   7.693453   6.458798  0.103181  0.000016  0.001  \n",
       "16   6.839894   5.533284  0.103117  0.000016  0.001  \n",
       "17   4.717561   3.469499  0.103036  0.000016  0.001  \n",
       "18   3.565971   2.197622  0.103179  0.000016  0.001  \n",
       "19   2.540437   1.328907   0.10347  0.000016  0.001  \n",
       "20   5.194318   4.005749  0.104187  0.000016  0.001  \n",
       "21   7.477771   6.174532  0.103933  0.000016  0.001  \n",
       "22   5.761881   4.438017  0.103761  0.000016  0.001  \n",
       "23   4.907294   3.556771  0.102953  0.000016  0.001  \n",
       "24   5.158665   3.869469  0.103921  0.000016  0.001  \n",
       "25   7.697487   6.450365  0.104235  0.000016  0.001  \n",
       "26   6.053892   4.783102  0.103127  0.000016  0.001  \n",
       "27   5.189731   3.856922  0.103374  0.000016  0.001  \n",
       "28   7.087047   5.796177  0.103264  0.000016  0.001  \n",
       "29   5.783184   4.536328  0.104059  0.000016  0.001  \n",
       "30   5.591814   4.333258  0.103972  0.000016  0.001  \n",
       "31   8.182545   6.825007  0.103837  0.000016  0.001  \n",
       "32   6.219594   4.836287  0.104198  0.000016  0.001  \n",
       "33   6.209856   4.977396    0.1036  0.000016  0.001  \n",
       "34   4.869924   3.663758  0.103584  0.000016  0.001  \n",
       "35   6.861547   5.571341  0.103024  0.000016  0.001  \n",
       "36   2.529946   1.206748  0.103489  0.000016  0.001  \n",
       "37   3.756595   2.485371  0.103236  0.000016  0.001  \n",
       "38   9.087991   7.910095  0.103083  0.000016  0.001  \n",
       "39   6.392653   5.230658  0.102993  0.000016  0.001  \n",
       "40   3.562088   2.481458  0.104078  0.000016  0.001  \n",
       "41   6.502707   5.342144  0.103256  0.000016  0.001  \n",
       "42   4.572283   3.384054  0.103039  0.000016  0.001  \n",
       "43   7.413724    6.19753  0.103042  0.000016  0.001  \n",
       "44    6.79369   5.580664  0.103903  0.000016  0.001  \n",
       "45   6.054476   4.849959  0.103731  0.000016  0.001  \n",
       "46   5.560693   4.324705  0.103197  0.000016  0.001  \n",
       "47   5.056445   3.983271  0.103438  0.000016  0.001  \n",
       "48   7.451334   6.222286  0.103916  0.000016  0.001  \n",
       "49   6.466078   5.019753  0.101843  0.000016  0.001  \n",
       "50        NaN        NaN       NaN       NaN    NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics_df.plot('step','time')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.001000\n",
       "1     0.001000\n",
       "2     0.001000\n",
       "3     0.001000\n",
       "4     0.001000\n",
       "5     0.000250\n",
       "6     0.000250\n",
       "7     0.000250\n",
       "8     0.000250\n",
       "9     0.000250\n",
       "10    0.000063\n",
       "11    0.000063\n",
       "12    0.000063\n",
       "13    0.000063\n",
       "14    0.000063\n",
       "15    0.000016\n",
       "16    0.000016\n",
       "17    0.000016\n",
       "18    0.000016\n",
       "19    0.000016\n",
       "20    0.000016\n",
       "21    0.000016\n",
       "22    0.000016\n",
       "23    0.000016\n",
       "24    0.000016\n",
       "25    0.000016\n",
       "26    0.000016\n",
       "27    0.000016\n",
       "28    0.000016\n",
       "29    0.000016\n",
       "30    0.000016\n",
       "31    0.000016\n",
       "32    0.000016\n",
       "33    0.000016\n",
       "34    0.000016\n",
       "35    0.000016\n",
       "36    0.000016\n",
       "37    0.000016\n",
       "38    0.000016\n",
       "39    0.000016\n",
       "40    0.000016\n",
       "41    0.000016\n",
       "42    0.000016\n",
       "43    0.000016\n",
       "44    0.000016\n",
       "45    0.000016\n",
       "46    0.000016\n",
       "47    0.000016\n",
       "48    0.000016\n",
       "49    0.000016\n",
       "50         NaN\n",
       "Name: lr_d, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_model.optimizerG.param_groups[0]['lr']\n",
    "metrics_df['lr_d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "#         ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "#             nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             View(shape=[-1,ngf*8,8,8]),\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*4) x 8 x 8\n",
    "#             nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf*2) x 16 x 16\n",
    "#             nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # state size. (ngf) x 32 x 32\n",
    "#             nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, ip):\n",
    "#         return self.main(ip)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, gdict):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         ## Define new variables from dict\n",
    "#         keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "#         ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())        \n",
    "\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "#             nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "#             nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf) x 32 x 32\n",
    "#             nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*2) x 16 x 16\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*4) x 8 x 8\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "#             nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. (ndf*8) x 4 x 4\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(nc*ndf*8*8*8, 1)\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, ip):\n",
    "# #         print(ip.shape)\n",
    "#         results=[ip]\n",
    "#         lst_idx=[]\n",
    "#         for i,submodel in enumerate(self.main.children()):\n",
    "#             mid_output=submodel(results[-1])\n",
    "#             results.append(mid_output)\n",
    "#             ## Select indices in list corresponding to output of Conv layers\n",
    "#             if submodel.__class__.__name__.startswith('Conv'):\n",
    "# #                 print(submodel.__class__.__name__)\n",
    "# #                 print(mid_output.shape)\n",
    "#                 lst_idx.append(i)\n",
    "\n",
    "#         FMloss=True\n",
    "#         if FMloss:\n",
    "#             ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "#         else :\n",
    "#             ans=results[-1]\n",
    "#         return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG = Generator(gdict).to(gdict['device'])\n",
    "# netG.apply(weights_init)\n",
    "# # # #     print(netG)\n",
    "# # summary(netG,(1,1,64))\n",
    "# # Create Discriminator\n",
    "# netD = Discriminator(gdict).to(gdict['device'])\n",
    "# netD.apply(weights_init)\n",
    "# #     print(netD)\n",
    "# summary(netD,(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn(gdict['batchsize'], 1, 1, gdict['nz'], device=gdict['device'])\n",
    "# fake = netG(noise)            \n",
    "# # Forward pass real batch through D\n",
    "# output = netD(fake)\n",
    "# print([i.shape for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009765625"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182.29166666666666"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "70000/(8*6*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ip_fname', 'op_loc', 'image_size', 'num_imgs', 'workers', 'nc', 'nz', 'ngf', 'ndf', 'beta1', 'kernel_size', 'stride', 'g_padding', 'd_padding', 'flip_prob', 'bns', 'checkpoint_size', 'batch_size', 'epochs', 'learn_rate', 'op_size', 'lr_epochs', 'lr_stepsize', 'deterministic', 'seed', 'lambda_spec_mean', 'lambda_spec_var', 'lambda_fm', 'lambda_gp', 'grad_clip', 'save_steps_list', 'run_suffix', 'description', 'config', 'mode', 'ip_fldr', 'facility', 'distributed', 'ngpu', 'device', 'multi-gpu', 'world_size', 'world_rank', 'local_rank', 'save_dir', 'best_chi1', 'best_chi2', 'iters', 'start_epoch'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 4\n",
      "num_imgs 40\n",
      "ngpu 1\n"
     ]
    }
   ],
   "source": [
    "for key in ['batch_size','num_imgs','ngpu']:\n",
    "    print(key,gdict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdict['world_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OLCF-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
