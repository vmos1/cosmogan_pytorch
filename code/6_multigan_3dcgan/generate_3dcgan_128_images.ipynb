{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Images\n",
    "March 1, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/repositories/cosmogan_pytorch/code/5_3d_cgan/1_main_code/')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_load_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    return config\n",
    "\n",
    "def f_manual_add_argparse():\n",
    "    ''' use only in jpt notebook'''\n",
    "    args=argparse.Namespace()\n",
    "    args.config='1_main_code/config_3d_cgan_128_cori.yaml'\n",
    "    args.mode='fresh'\n",
    "    args.ip_fldr=''\n",
    "#     args.local_rank=0\n",
    "    args.facility='cori'\n",
    "    args.distributed=False\n",
    "    args.ngpu=1\n",
    "#     args.mode='continue'\n",
    "#     args.ip_fldr='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201211_093818_nb_test/'\n",
    "    \n",
    "    return args\n",
    "             \n",
    "    \n",
    "def weights_init(m):\n",
    "    '''custom weights initialization called on netG and netD '''\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def f_init_gdict(args,gdict):\n",
    "    ''' Create global dictionary gdict from args and config file'''\n",
    "    \n",
    "    ## read config file\n",
    "    config_file=args.config\n",
    "    with open(config_file) as f:\n",
    "        config_dict= yaml.load(f, Loader=yaml.SafeLoader)\n",
    "        \n",
    "    gdict=config_dict['parameters']\n",
    "\n",
    "    args_dict=vars(args)\n",
    "    ## Add args variables to gdict\n",
    "    for key in args_dict.keys():\n",
    "        gdict[key]=args_dict[key]\n",
    "    return gdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gen_images(gdict,netG,optimizerG,sigma,ip_fname,op_loc,op_strg='inf_img_',op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: gdict, netG, optimizerG, \n",
    "                 sigma : sigma input value\n",
    "                 ip_fname: name of input file\n",
    "                op_strg: [string name for output file]\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    nz,device=gdict['nz'],gdict['device']\n",
    "\n",
    "    try:\n",
    "        if torch.cuda.is_available(): checkpoint=torch.load(ip_fname)\n",
    "        else: checkpoint=torch.load(ip_fname,map_location=torch.device('cpu'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    if gdict['multi-gpu']:\n",
    "        netG.module.load_state_dict(checkpoint['G_state'])\n",
    "    else:\n",
    "        netG.load_state_dict(checkpoint['G_state'])\n",
    "    \n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, 1, nz, device=device)\n",
    "    tnsr_cosm_params=(torch.ones(op_size,device=device)*sigma).view(op_size,1)\n",
    "\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    with torch.no_grad(): ## This is important. fails without it for multi-gpu\n",
    "        gen = netG(noise,tnsr_cosm_params)\n",
    "        gen_images=gen.detach().cpu().numpy()\n",
    "        print(gen_images.shape)\n",
    "\n",
    "    op_fname='%s_label-%s_epoch-%s_step-%s.npy'%(op_strg,sigma,epoch,iters)\n",
    "    np.save(op_loc+op_fname,gen_images)\n",
    "    \n",
    "    print(\"Image saved in \",op_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mod for 3D\n",
    "def f_get_model(model_name,gdict):\n",
    "    ''' Module to define Generator and Discriminator'''\n",
    "#     print(\"Model name\",model_name)\n",
    "\n",
    "    if model_name==2: #### Concatenate sigma input\n",
    "        if gdict['image_size']==64:\n",
    "            class Generator(nn.Module):\n",
    "                def __init__(self, gdict):\n",
    "                    super(Generator, self).__init__()\n",
    "\n",
    "                    ## Define new variables from dict\n",
    "                    keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                    ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                    self.main = nn.Sequential(\n",
    "                        # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                        nn.Linear(nz+1,nc*ngf*8**3),# 262144\n",
    "                        nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        View(shape=[-1,ngf*8,4,4,4]),\n",
    "                        nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf*4) x 8 x 8\n",
    "                        nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf*2) x 16 x 16\n",
    "                        nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf) x 32 x 32\n",
    "                        nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                        nn.Tanh()\n",
    "                    )\n",
    "\n",
    "                def forward(self, noise,labels):\n",
    "                    x=labels.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).float()\n",
    "                    gen_input=torch.cat((noise,x),-1)\n",
    "                    img=self.main(gen_input)\n",
    "\n",
    "                    return img\n",
    "\n",
    "            class Discriminator(nn.Module):\n",
    "                def __init__(self, gdict):\n",
    "                    super(Discriminator, self).__init__()\n",
    "\n",
    "                    ## Define new variables from dict\n",
    "                    keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                    ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                    self.linear_transf=nn.Linear(4,4)\n",
    "                    self.main = nn.Sequential(\n",
    "                        # input is (nc) x 64 x 64\n",
    "                        # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                        nn.Conv3d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                        nn.BatchNorm3d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf) x 32 x 32\n",
    "                        nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*2) x 16 x 16\n",
    "                        nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*4) x 8 x 8\n",
    "                        nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*8) x 4 x 4\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(nc*ndf*8*8*8, 1)\n",
    "            #             nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "                def forward(self, img,labels):\n",
    "                    img_size=gdict['image_size']\n",
    "                    x=labels.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,1,4).float() # get to size (batch,1,1,1,4)\n",
    "                    x=self.linear_transf(x)\n",
    "                    x=torch.repeat_interleave(x,int((img_size*img_size*img_size)/4)) # get to size (batch* img^3)\n",
    "                    x=x.view(labels.size(0),1,img_size,img_size,img_size) ## Get to size (batch,1,img,img,img)\n",
    "\n",
    "                    ip=torch.cat((img,x),axis=1)\n",
    "\n",
    "                    results=[ip]\n",
    "                    lst_idx=[]\n",
    "                    for i,submodel in enumerate(self.main.children()):\n",
    "                        mid_output=submodel(results[-1])\n",
    "                        results.append(mid_output)\n",
    "                        ## Select indices in list corresponding to output of Conv layers\n",
    "                        if submodel.__class__.__name__.startswith('Conv'):\n",
    "            #                 print(submodel.__class__.__name__)\n",
    "            #                 print(mid_output.shape)\n",
    "                            lst_idx.append(i)\n",
    "\n",
    "                    FMloss=True\n",
    "                    if FMloss:\n",
    "                        ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "                    else :\n",
    "                        ans=results[-1]\n",
    "\n",
    "                    return ans    \n",
    "        \n",
    "        elif gdict['image_size']==128:\n",
    "\n",
    "            class Generator(nn.Module):\n",
    "                def __init__(self, gdict):\n",
    "                    super(Generator, self).__init__()\n",
    "\n",
    "                    ## Define new variables from dict\n",
    "                    keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                    ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                    self.main = nn.Sequential(\n",
    "                        # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                        nn.Linear(nz+1,nc*ngf*8**3*8),# 262144\n",
    "                        nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        View(shape=[-1,ngf*8,8,8,8]),\n",
    "                        nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf*4) x 8 x 8\n",
    "                        nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf*2) x 16 x 16\n",
    "                        nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                        nn.BatchNorm3d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        # state size. (ngf) x 32 x 32\n",
    "                        nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                        nn.Tanh()\n",
    "                    )\n",
    "\n",
    "                def forward(self, noise,labels):\n",
    "                    x=labels.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).float()\n",
    "                    gen_input=torch.cat((noise,x),-1)\n",
    "                    img=self.main(gen_input)\n",
    "\n",
    "                    return img\n",
    "                \n",
    "\n",
    "            class Discriminator(nn.Module):\n",
    "                def __init__(self, gdict):\n",
    "                    super(Discriminator, self).__init__()\n",
    "\n",
    "                    ## Define new variables from dict\n",
    "                    keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                    ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                    self.linear_transf=nn.Linear(4,4)\n",
    "                    self.main = nn.Sequential(\n",
    "                        # input is (nc) x 64 x 64\n",
    "                        # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                        nn.Conv3d(nc+1, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                        nn.BatchNorm3d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf) x 32 x 32\n",
    "                        nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*2) x 16 x 16\n",
    "                        nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*4) x 8 x 8\n",
    "                        nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                        nn.BatchNorm3d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        # state size. (ndf*8) x 4 x 4\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(nc*ndf*8*8*8*8, 1)\n",
    "            #             nn.Sigmoid()\n",
    "                    )\n",
    "\n",
    "                def forward(self, img,labels):\n",
    "                    img_size=gdict['image_size']\n",
    "                    x=labels.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,1,4).float() # get to size (batch,1,1,1,4)\n",
    "                    x=self.linear_transf(x)\n",
    "                    x=torch.repeat_interleave(x,int((img_size*img_size*img_size)/4)) # get to size (batch* img^3)\n",
    "                    x=x.view(labels.size(0),1,img_size,img_size,img_size) ## Get to size (batch,1,img,img,img)\n",
    "\n",
    "                    ip=torch.cat((img,x),axis=1)\n",
    "\n",
    "                    results=[ip]\n",
    "                    lst_idx=[]\n",
    "                    for i,submodel in enumerate(self.main.children()):\n",
    "                        mid_output=submodel(results[-1])\n",
    "                        results.append(mid_output)\n",
    "                        ## Select indices in list corresponding to output of Conv layers\n",
    "                        if submodel.__class__.__name__.startswith('Conv'):\n",
    "            #                 print(submodel.__class__.__name__)\n",
    "            #                 print(mid_output.shape)\n",
    "                            lst_idx.append(i)\n",
    "\n",
    "                    FMloss=True\n",
    "                    if FMloss:\n",
    "                        ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "                    else :\n",
    "                        ans=results[-1]\n",
    "\n",
    "                    return ans \n",
    "                \n",
    "                \n",
    "                \n",
    "    elif model_name==3:#### Model 3: with ConditionalInstanceNorm2d,\n",
    "        class ConditionalInstanceNorm2d(nn.Module):\n",
    "            def __init__(self, num_features, num_params):\n",
    "                super().__init__()\n",
    "                self.num_features = num_features\n",
    "                self.InstNorm = nn.InstanceNorm2d(num_features, affine=False)\n",
    "                self.affine = nn.Linear(num_params, num_features * 2)\n",
    "                self.affine.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n",
    "                self.affine.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n",
    "\n",
    "            def forward(self, x, y):\n",
    "                out = self.InstNorm(x)\n",
    "                gamma, beta = self.affine(y).chunk(2, 1)\n",
    "                out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n",
    "                return out\n",
    "\n",
    "        class ConditionalSequential(nn.Sequential):\n",
    "            def __init__(self,*args):\n",
    "                super(ConditionalSequential, self).__init__(*args)\n",
    "\n",
    "            def forward(self, inputs, labels):\n",
    "                for module in self:\n",
    "                    if module.__class__ is ConditionalInstanceNorm2d:\n",
    "                        inputs = module(inputs, labels.float())\n",
    "                    else:\n",
    "                        inputs = module(inputs)\n",
    "\n",
    "                return inputs\n",
    "\n",
    "        class Generator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Generator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ngf','kernel_size','stride','g_padding']\n",
    "                ngpu, nz,nc,ngf,kernel_size,stride,g_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "\n",
    "                self.main = ConditionalSequential(\n",
    "                    # nn.ConvTranspose3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Linear(nz,nc*ngf*8**3),# 262144\n",
    "                    nn.BatchNorm3d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    View(shape=[-1,ngf*8,4,4,4]),\n",
    "                    nn.ConvTranspose3d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "                    ConditionalInstanceNorm2d(ngf*4,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*4) x 8 x 8\n",
    "                    nn.ConvTranspose3d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    ConditionalInstanceNorm2d(ngf*2,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf*2) x 16 x 16\n",
    "                    nn.ConvTranspose3d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "                    ConditionalInstanceNorm2d(ngf,1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # state size. (ngf) x 32 x 32\n",
    "                    nn.ConvTranspose3d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "                    nn.Tanh()\n",
    "                )\n",
    "                \n",
    "                \n",
    "            def forward(self, noise,labels):\n",
    "                img=self.main(noise,labels)\n",
    "\n",
    "                return img\n",
    "\n",
    "        class Discriminator(nn.Module):\n",
    "            def __init__(self, gdict):\n",
    "                super(Discriminator, self).__init__()\n",
    "\n",
    "                ## Define new variables from dict\n",
    "                keys=['ngpu','nz','nc','ndf','kernel_size','stride','d_padding']\n",
    "                ngpu, nz,nc,ndf,kernel_size,stride,d_padding=list(collections.OrderedDict({key:gdict[key] for key in keys}).values())\n",
    "                \n",
    "                self.main = nn.Sequential(\n",
    "                    # input is (nc) x 64 x 64\n",
    "                    # nn.Conv3d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "                    nn.Conv3d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "                    ConditionalInstanceNorm2d(ndf,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf) x 32 x 32\n",
    "                    nn.Conv3d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*2,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*2) x 16 x 16\n",
    "                    nn.Conv3d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*4,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*4) x 8 x 8\n",
    "                    nn.Conv3d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "                    ConditionalInstanceNorm2d(ndf*8,1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    # state size. (ndf*8) x 4 x 4\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        #             nn.Sigmoid()\n",
    "                )\n",
    "        \n",
    "            def forward(self, ip,labels):   \n",
    "                results=[ip]\n",
    "                lst_idx=[]\n",
    "                for i,submodel in enumerate(self.main.children()):\n",
    "                    mid_output=submodel(results[-1])\n",
    "                    results.append(mid_output)\n",
    "                    ## Select indices in list corresponding to output of Conv layers\n",
    "                    if submodel.__class__.__name__.startswith('Conv'):\n",
    "        #                 print(submodel.__class__.__name__)\n",
    "        #                 print(mid_output.shape)\n",
    "                        lst_idx.append(i)\n",
    "\n",
    "                FMloss=True\n",
    "                if FMloss:\n",
    "                    ans=[results[1:][i] for i in lst_idx + [-1]]\n",
    "                else :\n",
    "                    ans=results[-1]\n",
    "                return ans\n",
    "\n",
    "    return Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/3d_data/dataset4_smoothing_4univ_cgan_varying_sigma_128cube', 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/', 'image_size': 128, 'num_imgs': 5000, 'ip_fldr': '', 'chkpt_file': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210722_200025_cgan_128_nodes1_lr0.00002_finetune/models/checkpoint_last.tar', 'workers': 2, 'nc': 1, 'nz': 64, 'ngf': 64, 'ndf': 64, 'beta1': 0.5, 'kernel_size': 5, 'stride': 2, 'g_padding': 2, 'd_padding': 2, 'flip_prob': 0.01, 'bns': 50, 'checkpoint_size': 10, 'sigma_list': [0.5, 0.8, 1.1], 'model': 2, 'batch_size': 8, 'epochs': 300, 'op_size': 8, 'learn_rate_d': 2e-06, 'learn_rate_g': 2e-06, 'lr_d_epochs': [40, 60], 'lr_d_gamma': 1.0, 'lr_g_epochs': [40, 60], 'lr_g_gamma': 1.0, 'deterministic': False, 'seed': 234373, 'lambda_spec_mean': 0.1, 'lambda_spec_var': 0.1, 'lambda_fm': 50.0, 'lambda_gp': 0.0, 'grad_clip': 1.0, 'save_steps_list': 'all', 'run_suffix': 'cgan_128_lr0.000002_vary_fm50', 'description': '3d conditional GAN 128cube', 'config': '1_main_code/config_3d_cgan_128_cori.yaml', 'mode': 'fresh', 'facility': 'cori', 'distributed': False, 'ngpu': 1}\n",
      "Building GAN networks\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=65, out_features=262144, bias=True)\n",
      "    (1): BatchNorm3d(1, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): View()\n",
      "    (4): ConvTranspose3d(512, 256, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(2, 2, 2), output_padding=(1, 1, 1), bias=False)\n",
      "    (5): BatchNorm3d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): ConvTranspose3d(256, 128, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(2, 2, 2), output_padding=(1, 1, 1), bias=False)\n",
      "    (8): BatchNorm3d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ConvTranspose3d(128, 64, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(2, 2, 2), output_padding=(1, 1, 1), bias=False)\n",
      "    (11): BatchNorm3d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): ConvTranspose3d(64, 1, kernel_size=(5, 5, 5), stride=(2, 2, 2), padding=(2, 2, 2), output_padding=(1, 1, 1), bias=False)\n",
      "    (14): Tanh()\n",
      "  )\n",
      ")\n",
      "Number of GPUs used 1\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    t0=time.time()\n",
    "    #################################\n",
    "    args=f_manual_add_argparse()\n",
    "    ### Set up ###\n",
    "\n",
    "    # Initilize variables    \n",
    "    gdict={}\n",
    "    gdict=f_init_gdict(args,gdict)\n",
    "    print(gdict)\n",
    "    ## Add args variables to gdict\n",
    "#     for key in ['ngpu']:\n",
    "#         gdict[key]=vars(args)[key]\n",
    "    gdict['device']=torch.device(\"cuda\" if (torch.cuda.is_available() and gdict['ngpu'] > 0) else \"cpu\")\n",
    "    gdict['ngpu']=torch.cuda.device_count()\n",
    "    gdict['multi-gpu']=True if (gdict['device'].type == 'cuda') and (gdict['ngpu'] > 1) else False \n",
    "\n",
    "    Generator, Discriminator=f_get_model(gdict['model'],gdict)\n",
    "    print(\"Building GAN networks\")\n",
    "    # Create Generator\n",
    "    netG = Generator(gdict).to(gdict['device'])\n",
    "    netG.apply(weights_init)\n",
    "    print(netG)\n",
    "    # summary(netG,(1,1,64))\n",
    "    \n",
    "    print(\"Number of GPUs used %s\"%(gdict['ngpu']))\n",
    "    if (gdict['multi-gpu']):\n",
    "        netG = nn.DataParallel(netG, list(range(gdict['ngpu'])))\n",
    "\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=gdict['learn_rate_g'], betas=(gdict['beta1'], 0.999),eps=1e-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/3d_data/dataset4_smoothing_4univ_cgan_varying_sigma_128cube',\n",
       " 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/',\n",
       " 'image_size': 128,\n",
       " 'num_imgs': 5000,\n",
       " 'ip_fldr': '',\n",
       " 'chkpt_file': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210722_200025_cgan_128_nodes1_lr0.00002_finetune/models/checkpoint_last.tar',\n",
       " 'workers': 2,\n",
       " 'nc': 1,\n",
       " 'nz': 64,\n",
       " 'ngf': 64,\n",
       " 'ndf': 64,\n",
       " 'beta1': 0.5,\n",
       " 'kernel_size': 5,\n",
       " 'stride': 2,\n",
       " 'g_padding': 2,\n",
       " 'd_padding': 2,\n",
       " 'flip_prob': 0.01,\n",
       " 'bns': 50,\n",
       " 'checkpoint_size': 10,\n",
       " 'sigma_list': [0.5, 0.8, 1.1],\n",
       " 'model': 2,\n",
       " 'batch_size': 8,\n",
       " 'epochs': 300,\n",
       " 'op_size': 8,\n",
       " 'learn_rate_d': 2e-06,\n",
       " 'learn_rate_g': 2e-06,\n",
       " 'lr_d_epochs': [40, 60],\n",
       " 'lr_d_gamma': 1.0,\n",
       " 'lr_g_epochs': [40, 60],\n",
       " 'lr_g_gamma': 1.0,\n",
       " 'deterministic': False,\n",
       " 'seed': 234373,\n",
       " 'lambda_spec_mean': 0.1,\n",
       " 'lambda_spec_var': 0.1,\n",
       " 'lambda_fm': 50.0,\n",
       " 'lambda_gp': 0.0,\n",
       " 'grad_clip': 1.0,\n",
       " 'save_steps_list': 'all',\n",
       " 'run_suffix': 'cgan_128_lr0.000002_vary_fm50',\n",
       " 'description': '3d conditional GAN 128cube',\n",
       " 'config': '1_main_code/config_3d_cgan_128_cori.yaml',\n",
       " 'mode': 'fresh',\n",
       " 'facility': 'cori',\n",
       " 'distributed': False,\n",
       " 'ngpu': 1,\n",
       " 'device': device(type='cuda'),\n",
       " 'multi-gpu': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m20210615_72613_cgan_64_bs32_nodes8_lr0.0001_good_cgan_run\u001b[0m/\n",
      "\u001b[01;34m20210617_204752_cgan-64_bs32_nodes1_lr0.0001-vary_fm50_goodrun\u001b[0m/\n",
      "\u001b[01;34m20210619_224213_cgan_bs32_lr0.0001-vary_fm50_spec0.1\u001b[0m/\n",
      "\u001b[01;34m20210620_113852_cgan_bs32_lr0.0001-vary_fm50_spec0.01\u001b[0m/\n",
      "\u001b[01;34m20210620_63445_cgan_bs32_lr0.0001-vary_fm50_spec0.05\u001b[0m/\n",
      "\u001b[01;34m20210626_100420_cgan-128_bs8_nodes1_lr0.0002_fm0_goodrun\u001b[0m/\n",
      "\u001b[01;34m20210626_50127_cgan_bs32_lr0.0001-vary_fm50_spec0.5\u001b[0m/\n",
      "\u001b[01;34m20210628_73858_cgan_128_nodes1_lr0.0002_fm0_goodrun\u001b[0m/\n",
      "\u001b[01;34m20210703_50233_cgan_128_nodes1_lr0.0002_vary_fm0_goodrun\u001b[0m/\n",
      "\u001b[01;34m20210707_65905_cgan_128_lr0.0002_vary_fm50_bad\u001b[0m/\n",
      "\u001b[01;34m20210707_95421_cgan_128_bs8_nodes4_lr0.0004_gclip1.0_fm0_summit\u001b[0m/\n",
      "\u001b[01;34m20210719_153427_cgan_128_bs8_nodes4_lr0.0004_gclip1.0_spec10\u001b[0m/\n",
      "\u001b[01;34m20210719_153444_cgan_128_bs8_nodes4_lr0.0002_gclip1.0_fm0_spec50\u001b[0m/\n",
      "\u001b[01;34m20210720_63509_cgan_128_bs8_nodes4_lr0.0004_gclip1.0_spec10_noadv\u001b[0m/\n",
      "\u001b[01;34m20210720_81213_cgan_128_bs8_nodes4_lr0.0002_gclip1.0_spec50_noadv\u001b[0m/\n",
      "\u001b[01;34m20210721_71301_cgan_128_bs8_nodes4_lr0.0004_gclip1.0_specfullsize\u001b[0m/\n",
      "\u001b[01;34m20210721_71318_cgan_128_bs8_nodes4_lr0.0002_gclip1.0_specfullsize\u001b[0m/\n",
      "\u001b[01;34m20210723_72050_cgan_128_nodes1_lr0.00002_finetune\u001b[0m/\n",
      "\u001b[01;34m20210723_74424_cgan_128_nodes1_lr0.000002_finetune\u001b[0m/\n",
      "\u001b[01;34m20210726_173009_cgan_128_nodes1_lr0.000002_finetune\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## For single checkpoint\n",
    "# main_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/'\n",
    "# fldr='20210119_134802_cgan_predict_0.65_m2'\n",
    "# param_label=1.1\n",
    "# op_loc=main_dir+fldr+'/images/'\n",
    "# ip_fname=main_dir+fldr+'/models/checkpoint_best_spec.tar'\n",
    "# f_gen_images(gdict,netG,optimizerG,param_label,ip_fname,op_loc,op_strg='inference_spec',op_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1740.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-7_step-1740.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1740.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-7_step-1740.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1740.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-7_step-1740.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1740.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-7_step-1740.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1950.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-8_step-1950.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1950.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-8_step-1950.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1950.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-8_step-1950.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_1950.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-8_step-1950.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_2580.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-11_step-2580.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_2580.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-11_step-2580.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_2580.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-11_step-2580.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_2580.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-11_step-2580.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_3010.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-12_step-3010.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_3010.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-12_step-3010.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_3010.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-12_step-3010.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_3010.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-12_step-3010.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_5590.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-23_step-5590.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_5590.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-23_step-5590.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_5590.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-23_step-5590.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_5590.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-23_step-5590.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_6800.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.5_epoch-29_step-6800.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_6800.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.65_epoch-29_step-6800.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_6800.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-0.8_epoch-29_step-6800.npy\n",
      "/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/20210726_173009_cgan_128_nodes1_lr0.000002_finetune/models/checkpoint_6800.tar\n",
      "(32, 1, 128, 128, 128)\n",
      "Image saved in  inference_label-1.1_epoch-29_step-6800.npy\n"
     ]
    }
   ],
   "source": [
    "## For multiple checkpoints \n",
    "main_dir='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/3d_cGAN/'\n",
    "fldr='20210726_173009_cgan_128_nodes1_lr0.000002_finetune'\n",
    "\n",
    "param_list=[0.5,0.65,0.8,1.1]\n",
    "op_loc=main_dir+fldr+'/images/'\n",
    "step_list=[1740, 1950, 2580, 3010, 5590, 6800]\n",
    "\n",
    "for step in step_list:\n",
    "    for param_label in param_list:\n",
    "        try:\n",
    "    #     ip_fname=main_dir+fldr+'/models/checkpoint_{0}.tar'.format(step)\n",
    "    #     f_gen_images(gdict,netG,optimizerG,param_label,ip_fname,op_loc,op_strg='inference_spec',op_size=1000)\n",
    "            ip_fname=glob.glob(main_dir+fldr+'/models/checkpoint_*{0}.tar'.format(step))[0]\n",
    "            print(ip_fname)\n",
    "            f_gen_images(gdict,netG,optimizerG,param_label,ip_fname,op_loc,op_strg='inference',op_size=32)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"skipping \",step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname=op_loc+'inference_spec_epoch-11_step-37040.npy'\n",
    "# a1=np.load(fname)\n",
    "# print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
