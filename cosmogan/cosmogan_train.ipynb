{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cosmogan\n",
    "Aug 25, 2020\n",
    "\n",
    "Borrowing pieces of code from : \n",
    "\n",
    "- https://github.com/pytorch/tutorials/blob/11569e0db3599ac214b03e01956c2971b02c64ce/beginner_source/dcgan_faces_tutorial.py\n",
    "- https://github.com/exalearn/epiCorvid/tree/master/cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchsummary import summary\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_load_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "### Transformation functions for image pixel values\n",
    "def f_transform(x):\n",
    "    return 2.*x/(x + 4.) - 1.\n",
    "\n",
    "def f_invtransform(s):\n",
    "    return 4.*(1. + s)/(1. - s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Generator Code\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu,nz,nc,ngf,kernel_size,stride,g_padding):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.nz,self.nc,self.ngf=nz,nc,ngf\n",
    "#         self.kernel_size,self.g_padding=kernel_size,g_padding\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Linear(nz,nc*ngf*8*8*8),# 32768\n",
    "            nn.BatchNorm2d(nc,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            View(shape=[-1,ngf*8,8,8]),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size, stride, g_padding, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, kernel_size, stride, g_padding, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, kernel_size, stride,g_padding, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, nz,nc,ndf,kernel_size,stride,d_padding):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # nn.Conv2d(in_channels, out_channels, kernel_size,stride,padding,output_padding,groups,bias, Dilation,padding_mode)\n",
    "            nn.Conv2d(nc, ndf,kernel_size, stride, d_padding,  bias=True),\n",
    "            nn.BatchNorm2d(ndf,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 2,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 4,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size, stride, d_padding, bias=True),\n",
    "            nn.BatchNorm2d(ndf * 8,eps=1e-05, momentum=0.9, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc*ndf*8*8*8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gen_images(netG,optimizerG,nz,device,ip_fname,strg,save_dir,op_size=500):\n",
    "    '''Generate images for best saved models\n",
    "     Arguments: ip_fname: name of input file\n",
    "                strg: ['hist' or 'spec']\n",
    "                op_size: Number of images to generate\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        return\n",
    "    \n",
    "    netG.load_state_dict(checkpoint['G_state'])\n",
    "#    netD.load_state_dict(checkpoint['D_state'])\n",
    "\n",
    "    ## Load other stuff\n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    # Generate batch of latent vectors\n",
    "    noise = torch.randn(op_size, 1, 1, nz, device=device)\n",
    "    # Generate fake image batch with G\n",
    "    netG.eval() ## This is required before running inference\n",
    "    gen = netG(noise)\n",
    "    gen_images=gen.detach().cpu().numpy()[:,0,:,:]\n",
    "    print(gen_images.shape)\n",
    "\n",
    "    op_fname='best-%s_gen_img_epoch-%s_step-%s.npy'%(strg,epoch,iters)\n",
    "\n",
    "    np.save(save_dir+'/images/'+op_fname,gen_images)\n",
    "\n",
    "    print(\"Image saved in \",op_fname)\n",
    "    \n",
    "    \n",
    "def f_save_checkpoint(epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc):\n",
    "    ''' Checkpoint model '''\n",
    "    torch.save({'epoch':epoch,'iters':iters,'best_chi1':best_chi1,'best_chi2':best_chi2,\n",
    "                'G_state':netG.state_dict(),'D_state':netD.state_dict(),'optimizerG_state_dict':optimizerG.state_dict(),\n",
    "                'optimizerD_state_dict':optimizerD.state_dict()}, save_loc) \n",
    "    \n",
    "    \n",
    "def f_load_checkpoint(ip_fname,netG,netD,optimizerG,optimizerD):\n",
    "    ''' Load saved checkpoint\n",
    "    Also loads step, epoch, best_chi1, best_chi2'''\n",
    "    \n",
    "    try:\n",
    "        checkpoint=torch.load(ip_fname)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"skipping generation of images for \",ip_fname)\n",
    "        raise SystemError\n",
    "    \n",
    "    ## Load checkpoint\n",
    "    netG.load_state_dict(checkpoint['G_state'])\n",
    "    netD.load_state_dict(checkpoint['D_state'])\n",
    "\n",
    "    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])\n",
    "    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])\n",
    "    \n",
    "    iters=checkpoint['iters']\n",
    "    epoch=checkpoint['epoch']\n",
    "    best_chi1=checkpoint['best_chi1']\n",
    "    best_chi2=checkpoint['best_chi2']\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    return iters,epoch,best_chi1,best_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spectrum and histogram codes\n",
    "### Pytorch code ###\n",
    "####################\n",
    "\n",
    "def f_torch_radial_profile(img, center=(None,None)):\n",
    "    ''' Module to compute radial profile of a 2D image \n",
    "    Bincount causes issues with backprop, so not using this code\n",
    "    '''\n",
    "    \n",
    "    y,x=torch.meshgrid(torch.arange(0,img.shape[0]),torch.arange(0,img.shape[1])) # Get a grid of x and y values\n",
    "    if center[0]==None and center[1]==None:\n",
    "        center = torch.Tensor([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0]) # compute centers\n",
    "\n",
    "    # get radial values of every pair of points\n",
    "    r = torch.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    r= r.int()\n",
    "    \n",
    "#     print(r.shape,img.shape)\n",
    "    # Compute histogram of r values\n",
    "    tbin=torch.bincount(torch.reshape(r,(-1,)),weights=torch.reshape(img,(-1,)).type(torch.DoubleTensor))\n",
    "    nr = torch.bincount(torch.reshape(r,(-1,)))\n",
    "    radialprofile = tbin / nr\n",
    "    \n",
    "    return radialprofile[1:-1]\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage_with_batch(image, center=None): ### Not used in this code.\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile. Only use if you need to combine batches\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, channel, height, width = image.shape\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (batch, channel,-1)))\n",
    "    r_sorted = torch.gather(torch.reshape(r, (batch, channel, -1,)),2, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, (batch, channel, -1,)),2, ind)\n",
    "\n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[:,:,1:] - r_int[:,:,:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[2], (batch, -1))    # location of changes in radius\n",
    "    rind=torch.unsqueeze(rind,1)\n",
    "    nr = (rind[:,:,1:] - rind[:,:,:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "\n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    print(csum.shape,rind.shape,nr.shape)\n",
    "\n",
    "    tbin = torch.gather(csum, 2, rind[:,:,1:]) - torch.gather(csum, 2, rind[:,:,:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "\n",
    "def f_torch_get_azimuthalAverage(image, center=None):\n",
    "    \"\"\"\n",
    "    Calculate the azimuthally averaged radial profile.\n",
    "\n",
    "    image - The 2D image\n",
    "    center - The [x,y] pixel coordinates used as the center. The default is \n",
    "             None, which then uses the center of the image (including \n",
    "             fracitonal pixels).\n",
    "    source: https://www.astrobetter.com/blog/2010/03/03/fourier-transforms-of-images-in-python/\n",
    "    \"\"\"\n",
    "    \n",
    "    height, width = image.shape\n",
    "    # Create a grid of points with x and y coordinates\n",
    "    y, x = np.indices([height,width])\n",
    "\n",
    "    if not center:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "\n",
    "    # Get the radial coordinate for every grid point. Array has the shape of image\n",
    "    r = torch.tensor(np.hypot(x - center[0], y - center[1]))\n",
    "\n",
    "    # Get sorted radii\n",
    "    ind = torch.argsort(torch.reshape(r, (-1,)))\n",
    "    print(type(ind),ind.get_device())\n",
    "    r_sorted = torch.gather(torch.reshape(r, ( -1,)),0, ind)\n",
    "    i_sorted = torch.gather(torch.reshape(image, ( -1,)),0, ind)\n",
    "\n",
    "    \n",
    "    # Get the integer part of the radii (bin size = 1)\n",
    "    r_int=r_sorted.to(torch.int32)\n",
    "\n",
    "    # Find all pixels that fall within each radial bin.\n",
    "    deltar = r_int[1:] - r_int[:-1]  # Assumes all radii represented\n",
    "    rind = torch.reshape(torch.where(deltar)[0], (-1,))    # location of changes in radius\n",
    "    nr = (rind[1:] - rind[:-1]).type(torch.float)       # number of radius bin\n",
    "\n",
    "    # Cumulative sum to figure out sums for each radius bin\n",
    "    \n",
    "    csum = torch.cumsum(i_sorted, axis=-1)\n",
    "    tbin = torch.gather(csum, 0, rind[1:]) - torch.gather(csum, 0, rind[:-1])\n",
    "    radial_prof = tbin / nr\n",
    "\n",
    "    return radial_prof\n",
    "\n",
    "def f_torch_fftshift(real, imag):\n",
    "    for dim in range(0, len(real.size())):\n",
    "        real = torch.roll(real, dims=dim, shifts=real.size(dim)//2)\n",
    "        imag = torch.roll(imag, dims=dim, shifts=imag.size(dim)//2)\n",
    "    return real, imag\n",
    "\n",
    "def f_torch_compute_spectrum(arr):\n",
    "    \n",
    "    GLOBAL_MEAN=1.0\n",
    "    arr=(arr-GLOBAL_MEAN)/(GLOBAL_MEAN)\n",
    "    y1=torch.rfft(arr,signal_ndim=2,onesided=False)\n",
    "    real,imag=f_torch_fftshift(y1[:,:,0],y1[:,:,1])    ## last index is real/imag part\n",
    "    y2=real**2+imag**2     ## Absolute value of each complex number\n",
    "    \n",
    "#     print(y2.shape)\n",
    "    z1=f_torch_get_azimuthalAverage(y2)     ## Compute radial profile\n",
    "#     z1=f_torch_radial_profile(y2)     ## Compute radial profile\n",
    "    \n",
    "    return(z1)\n",
    "\n",
    "def f_torch_compute_batch_spectrum(arr):\n",
    "    \n",
    "    batch_pk=torch.stack([f_torch_compute_spectrum(i) for i in arr])\n",
    "    \n",
    "    return batch_pk\n",
    "\n",
    "def f_torch_image_spectrum(x,num_channels):\n",
    "    '''\n",
    "    Data has to be in the form (batch,channel,x,y)\n",
    "    '''\n",
    "    mean=[[] for i in range(num_channels)]    \n",
    "    sdev=[[] for i in range(num_channels)]    \n",
    "\n",
    "    for i in range(num_channels):\n",
    "        arr=x[:,i,:,:]\n",
    "#         print(i,arr.shape)\n",
    "        batch_pk=f_torch_compute_batch_spectrum(arr)\n",
    "#         print(batch_pk.shape)\n",
    "        mean[i]=torch.mean(batch_pk,axis=0)\n",
    "        sdev[i]=torch.std(batch_pk,axis=0)/np.sqrt(batch_pk.shape[0])\n",
    "        \n",
    "    mean=torch.stack(mean)\n",
    "    sdev=torch.stack(sdev)\n",
    "    return mean,sdev\n",
    "\n",
    "def f_compute_hist(data,bins):\n",
    "    hist_data=torch.histc(data,bins=bins)\n",
    "    ## A kind of normalization of histograms: divide by total sum\n",
    "    hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "\n",
    "    return hist_data\n",
    "\n",
    "### Losses \n",
    "def loss_spectrum(spec_mean,spec_mean_ref,spec_std,spec_std_ref,image_size):\n",
    "    ''' Loss function for the spectrum : mean + variance '''\n",
    "    \n",
    "    # Log ( sum( batch value - expect value) ^ 2 ))\n",
    "    \n",
    "    idx=int(image_size/2) ### For the spectrum, use only N/2 indices for loss calc.\n",
    "    \n",
    "    ### Warning: the first index is the channel number.For multiple channels, you are averaging over them, which is fine.\n",
    "    spec_mean=torch.log(torch.mean(torch.pow(spec_mean[:,:idx]-spec_mean_ref[:,:idx],2)))\n",
    "    spec_sdev=torch.log(torch.mean(torch.pow(spec_std[:,:idx]-spec_std_ref[:,:idx],2)))\n",
    "\n",
    "     \n",
    "    lambda1=1.0;lambda2=1.0;\n",
    "    ans=lambda1*spec_mean+lambda2*spec_sdev\n",
    "    return ans\n",
    "\n",
    "def loss_hist(hist_sample,hist_ref):\n",
    "    \n",
    "    lambda1=1.0\n",
    "    return torch.log(torch.mean(torch.pow(hist_sample-hist_ref,2)))\n",
    "#     return lambda1*torch.mean(torch.pow(hist_sample-hist_ref,2)).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'GAN', 'data': {'ip_fname': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/norm_1_train_val.npy', 'op_loc': '/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/'}, 'training': {'workers': 2, 'nc': 1, 'nz': 64, 'ngf': 64, 'ndf': 64, 'lr': 0.0002, 'beta1': 0.5, 'kernel_size': 5, 'stride': 2, 'g_padding': 2, 'd_padding': 2, 'image_size': 128, 'flip_prob': 0.01}}\n"
     ]
    }
   ],
   "source": [
    "config_file='1_main_code/config_128.yaml'\n",
    "config_dict=f_load_config(config_file)\n",
    "print(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workers=config_dict['training']['workers']\n",
    "nc=config_dict['training']['nc']\n",
    "nc,nz,ngf,ndf=config_dict['training']['nc'],config_dict['training']['nz'],config_dict['training']['ngf'],config_dict['training']['ndf']\n",
    "lr,beta1=config_dict['training']['lr'],config_dict['training']['beta1']\n",
    "kernel_size,stride=config_dict['training']['kernel_size'],config_dict['training']['stride']\n",
    "g_padding,d_padding=config_dict['training']['g_padding'],config_dict['training']['d_padding']\n",
    "image_size=config_dict['training']['image_size']\n",
    "ip_fname=config_dict['data']['ip_fname']\n",
    "op_loc=config_dict['data']['op_loc']\n",
    "flip_prob=config_dict['training']['flip_prob']\n",
    "\n",
    "\n",
    "ngpu=1\n",
    "batch_size=128\n",
    "spec_loss_flag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  21245\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "manualSeed=21245\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1, 128, 128) torch.Size([2000, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# ip_fname='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/norm_1_train_val.npy'\n",
    "img=np.load(ip_fname)[:2000].transpose(0,1,2,3)\n",
    "t_img=torch.from_numpy(img)\n",
    "print(img.shape,t_img.shape)\n",
    "del(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, torch.Size([1, 128, 128]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=TensorDataset(t_img)\n",
    "dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=1,drop_last=True)\n",
    "\n",
    "len(dataset),(dataset[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32768, bias=True)\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): View()\n",
      "    (4): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
      "    (14): Tanh()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1          [-1, 1, 1, 32768]       2,129,920\n",
      "       BatchNorm2d-2          [-1, 1, 1, 32768]               2\n",
      "              ReLU-3          [-1, 1, 1, 32768]               0\n",
      "              View-4            [-1, 512, 8, 8]               0\n",
      "   ConvTranspose2d-5          [-1, 256, 16, 16]       3,276,800\n",
      "       BatchNorm2d-6          [-1, 256, 16, 16]             512\n",
      "              ReLU-7          [-1, 256, 16, 16]               0\n",
      "   ConvTranspose2d-8          [-1, 128, 32, 32]         819,200\n",
      "       BatchNorm2d-9          [-1, 128, 32, 32]             256\n",
      "             ReLU-10          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-11           [-1, 64, 64, 64]         204,800\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-14          [-1, 1, 128, 128]           1,600\n",
      "             Tanh-15          [-1, 1, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 6,433,218\n",
      "Trainable params: 6,433,218\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 11.75\n",
      "Params size (MB): 24.54\n",
      "Estimated Total Size (MB): 36.29\n",
      "----------------------------------------------------------------\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): Flatten()\n",
      "    (13): Linear(in_features=32768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,664\n",
      "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
      "         LeakyReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]         204,928\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "         LeakyReLU-6          [-1, 128, 32, 32]               0\n",
      "            Conv2d-7          [-1, 256, 16, 16]         819,456\n",
      "       BatchNorm2d-8          [-1, 256, 16, 16]             512\n",
      "         LeakyReLU-9          [-1, 256, 16, 16]               0\n",
      "           Conv2d-10            [-1, 512, 8, 8]       3,277,312\n",
      "      BatchNorm2d-11            [-1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-12            [-1, 512, 8, 8]               0\n",
      "          Flatten-13                [-1, 32768]               0\n",
      "           Linear-14                    [-1, 1]          32,769\n",
      "================================================================\n",
      "Total params: 4,338,049\n",
      "Trainable params: 4,338,049\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 11.50\n",
      "Params size (MB): 16.55\n",
      "Estimated Total Size (MB): 28.11\n",
      "----------------------------------------------------------------\n",
      "Number of GPUs used 0\n"
     ]
    }
   ],
   "source": [
    "### Build Models ###\n",
    "# Create generator\n",
    "netG = Generator(ngpu,nz,nc,ngf,kernel_size,stride,g_padding).to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "summary(netG,(1,1,64))\n",
    "\n",
    "# Create Discriminator\n",
    "netD = Discriminator(ngpu, nz,nc,ndf,kernel_size,stride,g_padding).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "summary(netD,(1,128,128))\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "ngpu=torch.cuda.device_count()\n",
    "\n",
    "print(\"Number of GPUs used\",ngpu)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f_size(ip):\n",
    "#     p=2;s=2\n",
    "# #     return (ip + 2 * 0 - 1 * (p-1) -1 )/ s + 1\n",
    "\n",
    "#     return (ip-1)*s - 2 * p + 1 *(5-1)+ 1 + 1\n",
    "\n",
    "# f_size(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, 1, 1, nz, device=device)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999),eps=1e-7)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999),eps=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precompute metrics with validation data for computing losses\n",
    "val_img=f_invtransform(np.load(ip_fname)[210000:213000]).transpose(0,1,2,3)\n",
    "t_val_img=torch.from_numpy(val_img)\n",
    "## Stored mean and std of spectrum for full input data once\n",
    "mean_spec_val,sdev_spec_val=f_torch_image_spectrum(t_val_img,1)\n",
    "bns=50\n",
    "hist_val=f_compute_hist(t_val_img,bins=bns)\n",
    "del(val_img)\n",
    "del(t_val_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 88])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_spec_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_suffix='_nb_test'\n",
    "### Create prefix for foldername \n",
    "now=datetime.now()\n",
    "fldr_name=now.strftime('%Y%m%d_%H%M%S') ## time format\n",
    "# print(fldr_name)\n",
    "save_dir=op_loc+fldr_name+run_suffix\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir+'/models')\n",
    "    os.makedirs(save_dir+'/images')\n",
    "\n",
    "num_epochs=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=save_dir+'/log.log',filemode='w',format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize variables\n",
    "keys=['Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss','spec_chi','hist_chi']\n",
    "size=int(len(dataloader) * num_epochs)+1\n",
    "metric_dict=dict(zip(keys,[np.empty(size)*np.nan for i in range(len(keys))]))\n",
    "\n",
    "iters = 0; start_epoch=0\n",
    "best_chi1,best_chi2=1e10,1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "[0/4][0/15]\tLoss_D: 0.1732\tLoss_adv: 4.8921\tLoss_G: 53.6308\tD(x): 5.1619\tD(G(z)): -5.4940 / -4.8835\n",
      "Spec loss: tensor(48.7386, dtype=torch.float64, grad_fn=<AddBackward0>),\t hist loss: tensor(3.9308, grad_fn=<LogBackward>)\n",
      "Time taken for step 0 : 26.673779249191284\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Time taken for epoch 0: 425.2840690612793\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "[1/4][0/15]\tLoss_D: 4.9272\tLoss_adv: 8.2614\tLoss_G: 57.0077\tD(x): 3.2634\tD(G(z)): 4.9004 / -8.2611\n",
      "Spec loss: tensor(48.7463, dtype=torch.float64, grad_fn=<AddBackward0>),\t hist loss: tensor(3.9433, grad_fn=<LogBackward>)\n",
      "Time taken for step 15 : 25.654173374176025\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Time taken for epoch 1: 422.0609893798828\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "[2/4][0/15]\tLoss_D: 0.4460\tLoss_adv: 12.8087\tLoss_G: 61.4814\tD(x): 1.2754\tD(G(z)): -9.3768 / -12.8087\n",
      "Spec loss: tensor(48.6727, dtype=torch.float64, grad_fn=<AddBackward0>),\t hist loss: tensor(3.6082, grad_fn=<LogBackward>)\n",
      "Time taken for step 30 : 25.447478532791138\n",
      "Saving best hist model at epoch 2, step 31.\n",
      "Saving best spec model at epoch 2, step 31\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best hist model at epoch 2, step 33.\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 2, step 41\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 2, step 42\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 2, step 43\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 2, step 44\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 2, step 45\n",
      "Time taken for epoch 2: 421.0359606742859\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "[3/4][0/15]\tLoss_D: 0.6786\tLoss_adv: 11.0112\tLoss_G: 59.5370\tD(x): 0.8090\tD(G(z)): -8.4430 / -11.0112\n",
      "Spec loss: tensor(48.5257, dtype=torch.float64, grad_fn=<AddBackward0>),\t hist loss: tensor(3.9292, grad_fn=<LogBackward>)\n",
      "Time taken for step 45 : 26.624633073806763\n",
      "Saving best spec model at epoch 3, step 46\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 3, step 47\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 3, step 48\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 3, step 57\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Saving best spec model at epoch 3, step 58\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Time taken for epoch 3: 422.1815481185913\n",
      "Total time 1690.5644297599792\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    t_epoch_start=time.time()\n",
    "    for count, data in enumerate(dataloader, 0):\n",
    "        netG.train(); netD.train();  ### Need to add these after inference and before training\n",
    "\n",
    "        tme1=time.time()\n",
    "        ### Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        netD.zero_grad()\n",
    "\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        real_label = torch.full((b_size,), 1, device=device)\n",
    "        fake_label = torch.full((b_size,), 0, device=device)\n",
    "        g_label = torch.full((b_size,), 1, device=device) ## No flipping for Generator labels\n",
    "\n",
    "        ## Flip labels with probability flip_prob\n",
    "        for idx in np.random.choice(np.arange(b_size),size=int(np.ceil(b_size*flip_prob))):\n",
    "            real_label[idx]=0; fake_label[idx]=1\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        noise = torch.randn(b_size, 1, 1, nz, device=device)\n",
    "        fake = netG(noise)            \n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, real_label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, fake_label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###Update G network: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "\n",
    "        output = netD(fake).view(-1)\n",
    "        errG_adv = criterion(output, g_label)\n",
    "\n",
    "        # Histogram pixel intensity loss\n",
    "        hist_gen=f_compute_hist(f_invtransform(fake),bins=bns)\n",
    "        hist_loss=loss_hist(hist_gen,hist_val.to(device))\n",
    "\n",
    "        # Add spectral loss\n",
    "        mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1)  ### compute spectral mean,std for fake images for batch\n",
    "        spec_loss=loss_spectrum(mean,mean_spec_val,sdev,sdev_spec_val,image_size)   \n",
    "        print(type(spec_loss),type(errG_adv))\n",
    "#             errG=errG_adv\n",
    "        errG=errG_adv+spec_loss.detach()\n",
    "#         errG=errG_adv+torch.Tensor(np.array([spec_loss]))\n",
    "#             if spec_loss_flag: errG+=spec_loss\n",
    "#                 errG+=hist_loss\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        tme2=time.time()\n",
    "        # Output training stats\n",
    "        if count % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_adv: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, count, len(dataloader), errD.item(), errG_adv.item(),errG.item(), D_x, D_G_z1, D_G_z2)),\n",
    "            print(\"Spec loss: %s,\\t hist loss: %s\"%(spec_loss,hist_loss)),\n",
    "            print(\"Time taken for step %s : %s\"%(iters, tme2-tme1))\n",
    "\n",
    "        # Save metrics\n",
    "\n",
    "        for key,val in zip(['Dreal','Dfake','Dfull','G_adv','G_full','spec_loss','hist_loss'],[errD_real.item(),errD_fake.item(),errD.item(),errG_adv.item(),errG.item(),spec_loss,hist_loss]):\n",
    "            metric_dict[key][iters]=val\n",
    "\n",
    "        ### Checkpoint the best model\n",
    "        checkpoint=True\n",
    "\n",
    "        iters += 1  ### Model has been updated, so update iters before saving metrics and model.\n",
    "\n",
    "        ### Compute validation metrics for updated model\n",
    "        netG.eval()\n",
    "        with torch.no_grad():\n",
    "            #fake = netG(fixed_noise).detach().cpu()\n",
    "            fake = netG(fixed_noise)\n",
    "#                 print('size of fake image array',fake.shape)\n",
    "\n",
    "            hist_gen=f_compute_hist(f_invtransform(fake),bins=bns)\n",
    "#                 hist_chi=loss_hist(hist_gen,hist_val)\n",
    "            hist_chi=loss_hist(hist_gen,hist_val.to(device))\n",
    "            mean,sdev=f_torch_image_spectrum(f_invtransform(fake),1)\n",
    "            spec_chi=loss_spectrum(mean,mean_spec_val,sdev,sdev_spec_val,image_size)            \n",
    "# \n",
    "#             hist_chi=0.1\n",
    "        for key,val in zip(['spec_chi','hist_chi'],[spec_chi,hist_chi]):  metric_dict[key][iters]=val            \n",
    "        if count == len(dataloader)-1: ## Check point at last step of epoch\n",
    "            # Checkpoint model for continuing run\n",
    "            f_save_checkpoint(epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_last.tar')  \n",
    "\n",
    "        if (checkpoint and (epoch > 1)):\n",
    "            # Choose best models by metric\n",
    "            if hist_chi< best_chi1:\n",
    "                f_save_checkpoint(epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_hist.tar')\n",
    "                best_chi1=hist_chi\n",
    "                print(\"Saving best hist model at epoch %s, step %s.\"%(epoch,iters))\n",
    "\n",
    "            if  spec_chi< best_chi2:\n",
    "                f_save_checkpoint(epoch,iters,best_chi1,best_chi2,netG,netD,optimizerG,optimizerD,save_loc=save_dir+'/models/checkpoint_best_spec.tar')\n",
    "                best_chi2=spec_chi\n",
    "                print(\"Saving best spec model at epoch %s, step %s\"%(epoch,iters))\n",
    "\n",
    "        # Save G's output on fixed_noise\n",
    "\n",
    "        if (iters % 50 == 0) or ((epoch == num_epochs-1) and (count == len(dataloader)-1)):\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "                img_arr=np.array(fake[:,0,:,:])\n",
    "                fname='gen_img_epoch-%s_step-%s'%(epoch,iters)\n",
    "                np.save(save_dir+'/images/'+fname,img_arr)\n",
    "\n",
    "\n",
    "    t_epoch_end=time.time()\n",
    "    print(\"Time taken for epoch %s: %s\"%(epoch,t_epoch_end-t_epoch_start))\n",
    "\n",
    "tf=time.time()\n",
    "print(\"Total time\",tf-t0)\n",
    "\n",
    "### Save Losses to files\n",
    "with open (save_dir+'/metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(metric_dict,f)\n",
    "\n",
    "# ### Generate images for best saved models    \n",
    "# model_fname=save_dir+'/models/checkpoint_best_spec.tar'\n",
    "# f_gen_images(netG,optimizerG,nz,device,model_fname,'spec',save_dir,2000)\n",
    "\n",
    "# model_fname=save_dir+'/models/checkpoint_best_hist.tar'\n",
    "# f_gen_images(netG,optimizerG,nz,device,model_fname,'hist',save_dir,2000)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_chi1,best_chi2,hist_chi, spec_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spec_val.is_cuda,mean.is_cuda,sdev.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hist_gen),type(hist_gen_loss),type(hist_val)\n",
    "\n",
    "hist_gen.is_cuda,hist_val.is_cuda, hist_gen_loss.is_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_gen.is_cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_gen_loss-hist_val.to(device)\n",
    "hist_gen_loss-hist_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jupyter nbconvert --to script cosmogan_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_dict.keys())\n",
    "metric_dict['spec_chi']\n",
    "# metric_dict['hist_chi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the loss functions on results and keras results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.17*3125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
