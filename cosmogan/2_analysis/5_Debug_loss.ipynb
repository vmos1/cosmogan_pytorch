{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the issue with losses not matching in 2 codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pickle \n",
    "\n",
    "from matplotlib.colors import LogNorm, PowerNorm, Normalize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchsummary import summary\n",
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "from scipy import fftpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/global/u1/v/vpa/project/jpt_notebooks/Cosmology/Cosmo_GAN/repositories/cosmogan_pytorch/cosmogan/1_main_code/')\n",
    "import spec_loss as spc\n",
    "import post_analysis_pandas as post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1, 128, 128), torch.Size([1000, 1, 128, 128]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_fname='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/raw_data/128_square/dataset_2_smoothing_200k/norm_1_train_val.npy'\n",
    "img=np.load(ip_fname)[:1000].transpose(0,1,2,3)\n",
    "t_img=torch.from_numpy(img)\n",
    "img.shape,t_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrum check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to use fftpack for pytorch : https://github.com/locuslab/pytorch_fft/issues/9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spec_data,sdev_spec_data=spc.f_torch_image_spectrum(t_img,1)\n",
    "dict_sample=post.f_compute_hist_spect(img[:,0,:,:],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 37.52268981933594 37.49547916246857\n"
     ]
    }
   ],
   "source": [
    "### Test mean \n",
    "ans1=mean_spec_data[0,:]\n",
    "ans2=dict_sample['spec_val']\n",
    "\n",
    "for i in range(len(ans2)):\n",
    "    a,b=ans1[i].item(),ans2[i]\n",
    "    if ((a-b)>1e-2):\n",
    "        print(i,a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1463.673095703125 1463.3635259438863\n",
      "1 729.9336547851562 729.7808725265966\n",
      "2 492.84881591796875 492.7423083249079\n",
      "3 341.07000732421875 340.9979875539751\n",
      "4 251.4705047607422 251.4172896418705\n",
      "5 190.66429138183594 190.62336548128832\n",
      "6 146.30580139160156 146.27525772497586\n",
      "7 123.51824951171875 123.49126561776418\n",
      "8 97.71236419677734 97.69218995694958\n",
      "9 81.3443832397461 81.32674792307743\n",
      "10 65.38652038574219 65.37335760784842\n",
      "11 53.44884490966797 53.43744381703413\n"
     ]
    }
   ],
   "source": [
    "### Test stderr\n",
    "ans1=sdev_spec_data[0,:]\n",
    "ans2=dict_sample['spec_err']\n",
    "\n",
    "for i in range(len(ans2)):\n",
    "    a,b=ans1[i].item(),ans2[i]\n",
    "    if ((a-b)>1e-2):\n",
    "        print(i,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: \n",
    "Oct 22, 2020 (Bday!) \\\n",
    "The spectra match. \\\n",
    "The stderr matches less, but it's still in the ballpark. deviation probably due to rounding off errors.\n",
    "\n",
    "Oct 29, 2022. \\\n",
    "Modified pytorch radial profile code to remove bincount to help with backprop. code taken from tensorflow. \\\n",
    "Results agree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_batch_histogram(img_arr,bins,norm,hist_range):\n",
    "    ''' Compute histogram statistics for a batch of images'''\n",
    "\n",
    "    ## Extracting the range. This is important to ensure that the different histograms are compared correctly\n",
    "    if hist_range==None : ulim,llim=np.max(img_arr),np.min(img_arr)\n",
    "    else: ulim,llim=hist_range[1],hist_range[0]\n",
    "#         print(ulim,llim)\n",
    "    ### array of histogram of each image\n",
    "    hist_arr=np.array([np.histogram(arr.flatten(), bins=bins, range=(llim,ulim), density=norm) for arr in img_arr]) ## range is important\n",
    "    hist=np.stack(hist_arr[:,0]) # First element is histogram array\n",
    "#         print(hist.shape)\n",
    "    bin_list=np.stack(hist_arr[:,1]) # Second element is bin value \n",
    "    ### Compute statistics over histograms of individual images\n",
    "    mean,err=np.mean(hist,axis=0),np.std(hist,axis=0)/np.sqrt(hist.shape[0])\n",
    "    bin_edges=bin_list[0]\n",
    "    centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    return mean,err,centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_compute_hist(data,bins):\n",
    "    hist_data=torch.histc(data,bins=bins)\n",
    "    hist_data=(hist_data*bins)/torch.sum(hist_data)\n",
    "    \n",
    "    return hist_data\n",
    "\n",
    "a=f_compute_hist(torch.from_numpy(img),10)\n",
    "b=f_batch_histogram(img,10,True,None)[0]\n",
    "\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(a,linestyle='',marker='*')\n",
    "plt.plot(b,linestyle='',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the loss functions on results and keras results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/exagan1/run5_fixed_cosmology/models/gen_imgs.npy'\n",
    "f1='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201020_130552_spec_loss_/images/gen_img_epoch-23_step-37450.npy'\n",
    "f1='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201020_130552_spec_loss_/images/gen_img_epoch-19_step-30450.npy'\n",
    "img_1=np.expand_dims(np.load(f1)[:500],axis=1)\n",
    "\n",
    "f2='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201015_072548/images/gen_img_epoch-2_step-3150.npy'\n",
    "f2='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201015_072548/images/best_-hist_gen_img_epoch-4_step-7002.npy'\n",
    "f2='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201015_072548/images/best_-spec_gen_img_epoch-5_step-8012.npy'\n",
    "f2='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201020_130552_spec_loss_/images/best-spec_gen_img_epoch-14_step-23290.npy'\n",
    "f2='/global/cfs/cdirs/m3363/vayyar/cosmogan_data/results_from_other_code/pytorch/results/128sq/20201020_130552_spec_loss_/images/best-hist_gen_img_epoch-2_step-3473.npy'\n",
    "img_2=np.expand_dims(np.load(f2)[:1000],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spec_data,sdev_spec_data=spc.f_torch_image_spectrum(t_img[:1000],1)\n",
    "m1,s1=spc.f_torch_image_spectrum(torch.from_numpy(img_1),1)\n",
    "m2,s2=spc.f_torch_image_spectrum(torch.from_numpy(img_2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spc.loss_spectrum(mean_spec_data,m1,sdev_spec_data,s1,128))\n",
    "print(spc.loss_spectrum(mean_spec_data,m2,sdev_spec_data,s2,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data=torch.histc(t_img[:1000],bins=50)\n",
    "print(spc.loss_hist(t_img[5000:7000],hist_data))\n",
    "print(spc.loss_hist(torch.from_numpy(img_1),hist_data),spc.loss_hist(torch.from_numpy(img_2),hist_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Oct 20, 2020\n",
    "There are variations in the spectrum functions in pytorch and numpy.\n",
    "1. fftshit\n",
    "2. abs()**2\n",
    "\n",
    "However, the loss is computed with the same function for input and generated images, so it should not matter.\n",
    "The comparison with pytorch and keras data shows that keras is doing better (for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
